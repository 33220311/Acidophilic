{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Nw0r859ss4qA",
        "outputId": "5a3f4c31-e1a4-4dd4-f612-d5254f57a3c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==2.12\n",
            "  Downloading tensorflow-2.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (585.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m585.9/585.9 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12) (23.5.26)\n",
            "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow==2.12)\n",
            "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12) (1.60.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12) (3.9.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12) (0.4.23)\n",
            "Collecting keras<2.13,>=2.12.0 (from tensorflow==2.12)\n",
            "  Downloading keras-2.12.0-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m74.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12) (16.0.6)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12) (1.16.0)\n",
            "Collecting tensorboard<2.13,>=2.12 (from tensorflow==2.12)\n",
            "  Downloading tensorboard-2.12.3-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m75.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-estimator<2.13,>=2.12.0 (from tensorflow==2.12)\n",
            "  Downloading tensorflow_estimator-2.12.0-py2.py3-none-any.whl (440 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.7/440.7 kB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12) (0.34.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.12) (0.42.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow==2.12) (0.2.0)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow==2.12) (1.11.4)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (2.17.3)\n",
            "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.13,>=2.12->tensorflow==2.12)\n",
            "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (3.5.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow==2.12) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12) (3.2.2)\n",
            "Installing collected packages: tensorflow-estimator, keras, gast, google-auth-oauthlib, tensorboard, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.15.0\n",
            "    Uninstalling tensorflow-estimator-2.15.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.15.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.15.0\n",
            "    Uninstalling keras-2.15.0:\n",
            "      Successfully uninstalled keras-2.15.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.4\n",
            "    Uninstalling gast-0.5.4:\n",
            "      Successfully uninstalled gast-0.5.4\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.2.0\n",
            "    Uninstalling google-auth-oauthlib-1.2.0:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.2.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.15.1\n",
            "    Uninstalling tensorboard-2.15.1:\n",
            "      Successfully uninstalled tensorboard-2.15.1\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.15.0\n",
            "    Uninstalling tensorflow-2.15.0:\n",
            "      Successfully uninstalled tensorflow-2.15.0\n",
            "Successfully installed gast-0.4.0 google-auth-oauthlib-1.0.0 keras-2.12.0 tensorboard-2.12.3 tensorflow-2.12.0 tensorflow-estimator-2.12.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gast",
                  "keras",
                  "tensorboard",
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "pip install tensorflow==2.12"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qcx8q2B2wMhZ",
        "outputId": "fa2866e7-4036-4ea9-9ad1-bc4806f89581"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras==2.12.0 in /usr/local/lib/python3.10/dist-packages (2.12.0)\n"
          ]
        }
      ],
      "source": [
        "pip install keras==2.12.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDdkaA0fD4cQ"
      },
      "source": [
        "## Pre-requisite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqvolxWQx8uC",
        "outputId": "2bbac0a0-041a-4763-a71e-583811efa41a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting xlsxwriter\n",
            "  Downloading XlsxWriter-3.1.9-py3-none-any.whl (154 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/154.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m153.6/154.8 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xlsxwriter\n",
            "Successfully installed xlsxwriter-3.1.9\n"
          ]
        }
      ],
      "source": [
        "pip install xlsxwriter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71NgultbyHY3",
        "outputId": "1251c34d-c49e-47a6-bf0a-0b7c6f6abbbf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting sklearn-evaluation\n",
            "  Downloading sklearn_evaluation-0.12.0-py3-none-any.whl (111 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/111.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.7/111.1 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.1/111.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ploomber-core>=0.2.6 (from sklearn-evaluation)\n",
            "  Downloading ploomber_core-0.2.15-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sklearn-evaluation) (1.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from sklearn-evaluation) (3.7.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from sklearn-evaluation) (4.4.2)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from sklearn-evaluation) (0.9.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from sklearn-evaluation) (3.1.2)\n",
            "Requirement already satisfied: mistune in /usr/local/lib/python3.10/dist-packages (from sklearn-evaluation) (0.8.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from sklearn-evaluation) (1.5.3)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from sklearn-evaluation) (5.9.2)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from sklearn-evaluation) (7.34.0)\n",
            "Collecting black (from sklearn-evaluation)\n",
            "  Downloading black-23.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: parso in /usr/local/lib/python3.10/dist-packages (from sklearn-evaluation) (0.8.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from ploomber-core>=0.2.6->sklearn-evaluation) (8.1.7)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from ploomber-core>=0.2.6->sklearn-evaluation) (6.0.1)\n",
            "Collecting posthog (from ploomber-core>=0.2.6->sklearn-evaluation)\n",
            "  Downloading posthog-3.0.2-py2.py3-none-any.whl (37 kB)\n",
            "Collecting mypy-extensions>=0.4.3 (from black->sklearn-evaluation)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Requirement already satisfied: packaging>=22.0 in /usr/local/lib/python3.10/dist-packages (from black->sklearn-evaluation) (23.2)\n",
            "Collecting pathspec>=0.9.0 (from black->sklearn-evaluation)\n",
            "  Downloading pathspec-0.11.2-py3-none-any.whl (29 kB)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.10/dist-packages (from black->sklearn-evaluation) (3.11.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from black->sklearn-evaluation) (2.0.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from black->sklearn-evaluation) (4.5.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython->sklearn-evaluation) (67.7.2)\n",
            "Collecting jedi>=0.16 (from ipython->sklearn-evaluation)\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->sklearn-evaluation) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython->sklearn-evaluation) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->sklearn-evaluation) (3.0.39)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->sklearn-evaluation) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->sklearn-evaluation) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->sklearn-evaluation) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->sklearn-evaluation) (4.8.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->sklearn-evaluation) (2.1.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->sklearn-evaluation) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->sklearn-evaluation) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->sklearn-evaluation) (4.43.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->sklearn-evaluation) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib->sklearn-evaluation) (1.23.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->sklearn-evaluation) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->sklearn-evaluation) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->sklearn-evaluation) (2.8.2)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.10/dist-packages (from nbformat->sklearn-evaluation) (2.18.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat->sklearn-evaluation) (4.19.1)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.10/dist-packages (from nbformat->sklearn-evaluation) (5.4.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->sklearn-evaluation) (2023.3.post1)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sklearn-evaluation) (1.11.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sklearn-evaluation) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sklearn-evaluation) (3.2.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->sklearn-evaluation) (23.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->sklearn-evaluation) (2023.7.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->sklearn-evaluation) (0.30.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->sklearn-evaluation) (0.10.6)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->sklearn-evaluation) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->sklearn-evaluation) (0.2.8)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->sklearn-evaluation) (1.16.0)\n",
            "Requirement already satisfied: requests<3.0,>=2.7 in /usr/local/lib/python3.10/dist-packages (from posthog->ploomber-core>=0.2.6->sklearn-evaluation) (2.31.0)\n",
            "Collecting monotonic>=1.5 (from posthog->ploomber-core>=0.2.6->sklearn-evaluation)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog->ploomber-core>=0.2.6->sklearn-evaluation)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.7->posthog->ploomber-core>=0.2.6->sklearn-evaluation) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.7->posthog->ploomber-core>=0.2.6->sklearn-evaluation) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.7->posthog->ploomber-core>=0.2.6->sklearn-evaluation) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.7->posthog->ploomber-core>=0.2.6->sklearn-evaluation) (2023.7.22)\n",
            "Installing collected packages: monotonic, pathspec, mypy-extensions, jedi, backoff, posthog, black, ploomber-core, sklearn-evaluation\n",
            "Successfully installed backoff-2.2.1 black-23.10.1 jedi-0.19.1 monotonic-1.6 mypy-extensions-1.0.0 pathspec-0.11.2 ploomber-core-0.2.15 posthog-3.0.2 sklearn-evaluation-0.12.0\n"
          ]
        }
      ],
      "source": [
        "pip install sklearn-evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CoTuyH3VsxAI"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "# Data utilities\n",
        "import pandas as pd\n",
        "from pandas import read_csv\n",
        "import matplotlib.pyplot as plt\n",
        "import os.path\n",
        "import torch\n",
        "import h5py\n",
        "\n",
        "# Machine learning\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import matthews_corrcoef, make_scorer\n",
        "from sklearn.model_selection import cross_val_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XrsCx0xf0E8z"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_curve, roc_auc_score, auc, precision_recall_curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cOHIvuD4ZMKw"
      },
      "outputs": [],
      "source": [
        "#!wget https://raw.githubusercontent.com/33220311/Extremophiles/main/Embeddings/Sannotations.csv\n",
        "#!wget https://raw.githubusercontent.com/33220311/Extremophiles/main/Embeddings/SuniqueAnnotations.csv\n",
        "#!wget https://raw.githubusercontent.com/33220311/Extremophiles/main/Embeddings/Sannotations1024.csv\n",
        "#!wget https://raw.githubusercontent.com/33220311/Extremophiles/main/Embeddings/SuniqueAnnotations1024.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P966jTOzoA-W",
        "outputId": "d969b6d0-342e-4546-b3cb-2ef2f2079e24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "2UvpiEKmvyi2",
        "outputId": "25c0681a-e47e-464a-f0f8-1b834ba76a7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sending incremental file list\n",
            "./\n",
            "17KD_RICPR.pt\n",
            "35NBP_CHRVO.pt\n",
            "3HAO_PSEFL.pt\n",
            "3MDO_PSEAE.pt\n",
            "3PASE_HUNT2.pt\n",
            "5C1CA.pt\n",
            "6H82D.pt\n",
            "6OJ0_Z.pt\n",
            "6OJ0_f.pt\n",
            "6PGL_CHLTR.pt\n",
            "6PGL_THEMA.pt\n",
            "AAC82870.pt\n",
            "AAC82872.pt\n",
            "AACC1_PSEAI.pt\n",
            "AACP_AGRFC.pt\n",
            "AADR_RHOPA.pt\n",
            "AAEB_SHIFL.pt\n",
            "AAEP_CYTH3.pt\n",
            "AAK14975.pt\n",
            "AAP41498.pt\n",
            "AAS13419.pt\n",
            "AAS13421.pt\n",
            "AAV44265.pt\n",
            "AAV44354.pt\n",
            "AAV44368.pt\n",
            "AAV44371.pt\n",
            "AAV44449.pt\n",
            "AAV44493.pt\n",
            "AAV44566.pt\n",
            "AAV44731.pt\n",
            "AAV45349.pt\n",
            "AAV45378.pt\n",
            "AAV45540.pt\n",
            "AAV45606.pt\n",
            "AAV45915.pt\n",
            "AAV46261.pt\n",
            "AAV46365.pt\n",
            "AAV46711.pt\n",
            "AAV46844.pt\n",
            "AAV47010.pt\n",
            "AAV47111.pt\n",
            "AAV47281.pt\n",
            "AAV47499.pt\n",
            "AAY24960.pt\n",
            "ABE57473.pt\n",
            "ABE57481.pt\n",
            "ABE57489.pt\n",
            "ABE57494.pt\n",
            "ABE57495.pt\n",
            "ABE57564.pt\n",
            "ABE57667.pt\n",
            "ABE57687.pt\n",
            "ABE57960.pt\n",
            "ABE58224.pt\n",
            "ABE58278.pt\n",
            "ABE58472.pt\n",
            "ABE58488.pt\n",
            "ABE58558.pt\n",
            "ABE58621.pt\n",
            "ABE58729.pt\n",
            "ABE58758.pt\n",
            "ABE58760.pt\n",
            "ABE58779.pt\n",
            "ABE58781.pt\n",
            "ABE58849.pt\n",
            "ABE58926.pt\n",
            "ABE59016.pt\n",
            "ABE59157.pt\n",
            "ABE59170.pt\n",
            "ABE59203.pt\n",
            "ABE59204.pt\n",
            "ABE59276.pt\n",
            "ABE59344.pt\n",
            "ABE59379.pt\n",
            "ABE59385.pt\n",
            "ABE59426.pt\n",
            "ABE59475.pt\n",
            "ABE59487.pt\n",
            "ABE59504.pt\n",
            "ABE59548.pt\n",
            "ABE59647.pt\n",
            "ABE59672.pt\n",
            "ABE59713.pt\n",
            "ABE59716.pt\n",
            "ABE59840.pt\n",
            "ABE59851.pt\n",
            "ABE59852.pt\n",
            "ABE59866.pt\n",
            "ABE59917.pt\n",
            "ABE59927.pt\n",
            "ABE59987.pt\n",
            "ABE60006.pt\n",
            "ABE60009.pt\n",
            "ABE60115.pt\n",
            "ABE60349.pt\n"
          ]
        }
      ],
      "source": [
        "!rsync -av /content/drive/MyDrive/Halophilic/ESM2_3B_HaloAdd/ /content/drive/MyDrive/Multiclass/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhkJoiulXECn",
        "outputId": "88f29bb2-fe8e-48ec-8344-db5f8d2d1cbe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-11-06 02:50:04--  https://raw.githubusercontent.com/33220311/Acidophilic/main/acidoSplit.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1293036 (1.2M) [text/plain]\n",
            "Saving to: ‘acidoSplit.csv’\n",
            "\n",
            "acidoSplit.csv      100%[===================>]   1.23M  --.-KB/s    in 0.009s  \n",
            "\n",
            "2023-11-06 02:50:04 (138 MB/s) - ‘acidoSplit.csv’ saved [1293036/1293036]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/33220311/Acidophilic/main/acidoSplit.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4nVECYciQze",
        "outputId": "034ba42d-638e-4f71-eec3-fdb188195e2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-11-06 02:50:04--  https://raw.githubusercontent.com/33220311/Acidophilic/main/acido100.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1022160 (998K) [text/plain]\n",
            "Saving to: ‘acido100.csv’\n",
            "\n",
            "acido100.csv        100%[===================>] 998.20K  --.-KB/s    in 0.005s  \n",
            "\n",
            "2023-11-06 02:50:05 (186 MB/s) - ‘acido100.csv’ saved [1022160/1022160]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/33220311/Acidophilic/main/acido100.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGX1NEE1iT5j",
        "outputId": "ebb4b154-d8d6-45ec-93ba-2951d58158fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-11-06 02:50:05--  https://raw.githubusercontent.com/33220311/Acidophilic/main/acido80.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 872332 (852K) [text/plain]\n",
            "Saving to: ‘acido80.csv’\n",
            "\n",
            "acido80.csv         100%[===================>] 851.89K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2023-11-06 02:50:05 (75.6 MB/s) - ‘acido80.csv’ saved [872332/872332]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/33220311/Acidophilic/main/acido80.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6VVn3LeiUIH",
        "outputId": "556b0871-3a5c-4627-dac5-4bdc206eb369"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-11-06 02:50:05--  https://raw.githubusercontent.com/33220311/Acidophilic/main/acido60.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 705992 (689K) [text/plain]\n",
            "Saving to: ‘acido60.csv’\n",
            "\n",
            "acido60.csv         100%[===================>] 689.45K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2023-11-06 02:50:05 (67.9 MB/s) - ‘acido60.csv’ saved [705992/705992]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/33220311/Acidophilic/main/acido60.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZqZ0wgSQiUYH",
        "outputId": "56f65909-dc2f-4b5f-b997-50458b53f3f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-11-06 02:50:05--  https://raw.githubusercontent.com/33220311/Acidophilic/main/acido40.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 560477 (547K) [text/plain]\n",
            "Saving to: ‘acido40.csv’\n",
            "\n",
            "acido40.csv         100%[===================>] 547.34K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2023-11-06 02:50:05 (55.6 MB/s) - ‘acido40.csv’ saved [560477/560477]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/33220311/Acidophilic/main/acido40.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JbBXYMOCiUmz",
        "outputId": "cae00af9-e796-4f3d-8e11-77c63e8929c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-11-06 02:50:05--  https://raw.githubusercontent.com/33220311/Acidophilic/main/acido20.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 408394 (399K) [text/plain]\n",
            "Saving to: ‘acido20.csv’\n",
            "\n",
            "acido20.csv         100%[===================>] 398.82K  --.-KB/s    in 0.007s  \n",
            "\n",
            "2023-11-06 02:50:05 (56.6 MB/s) - ‘acido20.csv’ saved [408394/408394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/33220311/Acidophilic/main/acido20.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydyh-KnUWTNV"
      },
      "source": [
        "## Utility Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-MIcQL7TV7AE"
      },
      "outputs": [],
      "source": [
        "# Utility function: plot model's accuracy and loss\n",
        "\n",
        "# https://realpython.com/python-keras-text-classification/\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "def plot_history(history):\n",
        "  acc = history.history['accuracy']\n",
        "  val_acc = history.history['val_accuracy']\n",
        "  loss = history.history['loss']\n",
        "  val_loss = history.history['val_loss']\n",
        "  x = range(1, len(acc) + 1)\n",
        "\n",
        "  plt.figure(figsize=(12, 5))\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plt.plot(x, acc, 'b', label='Training acc')\n",
        "  plt.plot(x, val_acc, 'r', label='Validation acc')\n",
        "  plt.title('Training and validation accuracy')\n",
        "  plt.legend()\n",
        "\n",
        "  plt.subplot(1, 2, 2)\n",
        "  plt.plot(x, loss, 'b', label='Training loss')\n",
        "  plt.plot(x, val_loss, 'r', label='Validation loss')\n",
        "  plt.title('Training and validation loss')\n",
        "  plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1miZbCjfWcl3"
      },
      "outputs": [],
      "source": [
        "# Utility function: Display model score(Loss & Accuracy) across all sets.\n",
        "\n",
        "def display_model_score(model, train, val, test, batch_size):\n",
        "\n",
        "  train_score = model.evaluate(train[0], train[1], batch_size=batch_size, verbose=1)\n",
        "  print('Train loss: ', train_score[0])\n",
        "  print('Train accuracy: ', train_score[1])\n",
        "  print('-'*70)\n",
        "\n",
        "  val_score = model.evaluate(val[0], val[1], batch_size=batch_size, verbose=1)\n",
        "  print('Val loss: ', val_score[0])\n",
        "  print('Val accuracy: ', val_score[1])\n",
        "  print('-'*70)\n",
        "\n",
        "  test_score = model.evaluate(test[0], test[1], batch_size=batch_size, verbose=1)\n",
        "  print('Test loss: ', test_score[0])\n",
        "  print('Test accuracy: ', test_score[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1We_4nA3VGCl"
      },
      "outputs": [],
      "source": [
        "def equal_error_rate(y_true, y_pred):\n",
        "    n_imp = tf.count_nonzero(tf.equal(y_true, 0), dtype=tf.float32) + tf.constant(K.epsilon())\n",
        "    n_gen = tf.count_nonzero(tf.equal(y_true, 1), dtype=tf.float32) + tf.constant(K.epsilon())\n",
        "\n",
        "    scores_imp = tf.boolean_mask(y_pred, tf.equal(y_true, 0))\n",
        "    scores_gen = tf.boolean_mask(y_pred, tf.equal(y_true, 1))\n",
        "\n",
        "    loop_vars = (tf.constant(0.0), tf.constant(1.0), tf.constant(0.0))\n",
        "    cond = lambda t, fpr, fnr: tf.greater_equal(fpr, fnr)\n",
        "    body = lambda t, fpr, fnr: (\n",
        "        t + 0.001,\n",
        "        tf.divide(tf.count_nonzero(tf.greater_equal(scores_imp, t), dtype=tf.float32), n_imp),\n",
        "        tf.divide(tf.count_nonzero(tf.less(scores_gen, t), dtype=tf.float32), n_gen)\n",
        "    )\n",
        "    t, fpr, fnr = tf.while_loop(cond, body, loop_vars, back_prop=False)\n",
        "    eer = (fpr + fnr) / 2\n",
        "\n",
        "    return eer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KsbWeA0qjBvb"
      },
      "outputs": [],
      "source": [
        "def error_rate(testing_labels, predicted_testing_labels):\n",
        "  from sklearn.metrics import f1_score, matthews_corrcoef, balanced_accuracy_score\n",
        "  from sklearn.metrics import accuracy_score\n",
        "  from sklearn.metrics import classification_report\n",
        "  from sklearn.metrics import recall_score\n",
        "  import numpy as np\n",
        "\n",
        "  bootstrap_performances = list()\n",
        "  performances = list()\n",
        "  f1_performances = list()\n",
        "  sn = list()\n",
        "  sp = list()\n",
        "  Y = np.array(testing_labels) # convert list of groundtruths to numpy\n",
        "  Yhat = np.array(predicted_testing_labels) # same same for predictions\n",
        "  n_samples = len(Y) # get number of samples\n",
        "  n_bootstrap = 1000 # number of bootstrap iterations\n",
        "  metric = matthews_corrcoef # the metric you want to compute\n",
        "  for i in range(n_bootstrap): # for each bootstrap draw\n",
        "    subset = np.random.choice(n_samples, n_samples, replace=True)\n",
        "    # create a random subset of your predictions/targets with replacement (this line will only generate the indices for list elements and the line below will grab the random subset with replacement\n",
        "    bootstrap_performances.append( matthews_corrcoef(testing_labels[subset], predicted_testing_labels[subset]) )\n",
        "    performances.append(accuracy_score(testing_labels[subset], predicted_testing_labels[subset]))\n",
        "    f1_performances.append(f1_score(testing_labels[subset], predicted_testing_labels[subset]))\n",
        "    sn.append(recall_score(testing_labels[subset], predicted_testing_labels[subset],labels=[1],average='macro'))\n",
        "    sp.append(recall_score(testing_labels[subset], predicted_testing_labels[subset],labels=[1],average='macro'))\n",
        "  sd_mcc = np.std(bootstrap_performances) # compute std deviation over the bootstrapped performances\n",
        "  sd_acc = np.std(performances)\n",
        "  sd_f1 = np.std(f1_performances)\n",
        "  sd_sn = np.std(sn)\n",
        "  sd_sp = np.std(sp)\n",
        "\n",
        "  print('acc:',accuracy_score(testing_labels, predicted_testing_labels))\n",
        "  print('f1:',f1_score(testing_labels, predicted_testing_labels))\n",
        "  print('mcc:',matthews_corrcoef(testing_labels, predicted_testing_labels))\n",
        "  print('sn:',sn)\n",
        "  print('sp:',sp)\n",
        "  print('sd_acc:',sd_acc)\n",
        "  print('sd_f1:',sd_f1)\n",
        "  print('sd_mcc:',sd_mcc)\n",
        "  print('sd_sn:',sd_sn)\n",
        "  print('sd_sp:',sd_sp)\n",
        "  print(classification_report(testing_labels, predicted_testing_labels))\n",
        "\n",
        "  return (sd_acc, sd_mcc, sd_f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqLP-ppCjBzo"
      },
      "outputs": [],
      "source": [
        "def conf_matrix(confusion_matrix_data):\n",
        "  from mlxtend.plotting import plot_confusion_matrix\n",
        "  fig, ax = plot_confusion_matrix(conf_mat =confusion_matrix_data,\n",
        "                                show_absolute=True,\n",
        "                                show_normed=True,\n",
        "                                #display_labels=class_dict.values(),\n",
        "                                colorbar=True)\n",
        "  labels = ['Non-halophilic', 'Halophilic']\n",
        "  ax.set_xticklabels([''] + labels)\n",
        "  ax.set_yticklabels([''] + labels)\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-kBBuZ19gfQZ"
      },
      "outputs": [],
      "source": [
        "def mcc(clf,X,y):\n",
        "  y_pred = clf.predict(X)\n",
        "  mcc = matthews_corrcoef(y, y_pred)\n",
        "  return mcc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31MQmkhjgfRb"
      },
      "outputs": [],
      "source": [
        "def std_acc(clf,X,y):\n",
        "  from sklearn.metrics import accuracy_score\n",
        "  import numpy as np\n",
        "\n",
        "  bootstrap_performances = list()\n",
        "  performances = list()\n",
        "  y_pred = clf.predict(X)\n",
        "  Y = np.array(y) # convert list of groundtruths to numpy\n",
        "  Yhat = np.array(y_pred) # same same for predictions\n",
        "  n_samples = len(Y) # get number of samples\n",
        "  n_bootstrap = 1000 # number of bootstrap iterations\n",
        "  for i in range(n_bootstrap): # for each bootstrap draw\n",
        "    subset = np.random.choice(n_samples, n_samples, replace=True)\n",
        "    # create a random subset of your predictions/targets with replacement (this line will only generate the indices for list elements and the line below will grab the random subset with replacement\n",
        "    bootstrap_performances.append( accuracy_score(y[subset], y_pred[subset]) )\n",
        "  sd_acc = np.std(bootstrap_performances)*1.96\n",
        "  return sd_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GGWcbIxEgfV4"
      },
      "outputs": [],
      "source": [
        "def std_f1(clf,X,y):\n",
        "  from sklearn.metrics import f1_score\n",
        "  import numpy as np\n",
        "\n",
        "  bootstrap_performances = list()\n",
        "  performances = list()\n",
        "  y_pred = clf.predict(X)\n",
        "  Y = np.array(y) # convert list of groundtruths to numpy\n",
        "  Yhat = np.array(y_pred) # same same for predictions\n",
        "  n_samples = len(Y) # get number of samples\n",
        "  n_bootstrap = 1000 # number of bootstrap iterations\n",
        "  for i in range(n_bootstrap): # for each bootstrap draw\n",
        "    subset = np.random.choice(n_samples, n_samples, replace=True)\n",
        "    # create a random subset of your predictions/targets with replacement (this line will only generate the indices for list elements and the line below will grab the random subset with replacement\n",
        "    bootstrap_performances.append(f1_score(y[subset], y_pred[subset]) )\n",
        "  sd_f1 = np.std(bootstrap_performances)*1.96\n",
        "  return sd_f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GUmsh2g8gfhf"
      },
      "outputs": [],
      "source": [
        "def std_mcc(clf,X,y):\n",
        "  from sklearn.metrics import matthews_corrcoef\n",
        "  import numpy as np\n",
        "\n",
        "  bootstrap_performances = list()\n",
        "  performances = list()\n",
        "  y_pred = clf.predict(X)\n",
        "  Y = np.array(y) # convert list of groundtruths to numpy\n",
        "  Yhat = np.array(y_pred) # same same for predictions\n",
        "  n_samples = len(Y) # get number of samples\n",
        "  n_bootstrap = 1000 # number of bootstrap iterations\n",
        "  for i in range(n_bootstrap): # for each bootstrap draw\n",
        "    subset = np.random.choice(n_samples, n_samples, replace=True)\n",
        "    # create a random subset of your predictions/targets with replacement (this line will only generate the indices for list elements and the line below will grab the random subset with replacement\n",
        "    bootstrap_performances.append(matthews_corrcoef(y[subset], y_pred[subset]) )\n",
        "  sd_mcc = np.std(bootstrap_performances)*1.96\n",
        "  return sd_mcc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "biqm5i_3B-hv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwrEQOWRAufE"
      },
      "source": [
        "## Open embedding file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wn6qNd85C6bx"
      },
      "outputs": [],
      "source": [
        "path = '/content/drive/MyDrive/Acidophilic/ESM2_3B_AcidoAdd' #'/content/drive/MyDrive/Halophilic/ESM2_3B/' #'/content/drive/MyDrive/HalophilicESM2'\n",
        "suffix = '.pt'\n",
        "delete = []\n",
        "embeddings = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8A4aZOotEMQ"
      },
      "outputs": [],
      "source": [
        "proteins = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfNnO9qltX6v"
      },
      "outputs": [],
      "source": [
        "# Basic Protocol 3 — Step 5\n",
        "annotations = read_csv('acido20.csv')\n",
        "#annotations = read_csv('annotationsUnique.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "t36iDvOQti9m",
        "outputId": "8ec2a65d-bf15-49ff-963f-841b749abf04"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-b4ceaaf9-1eaf-4cdc-93c1-c0daa9ae83a6\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>identifier</th>\n",
              "      <th>sequence</th>\n",
              "      <th>label</th>\n",
              "      <th>set</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>OIQ67012</td>\n",
              "      <td>MSMSRLKDEVELRQAIEATGYHPALVADAVASALGRETCLAYVVHQ...</td>\n",
              "      <td>1</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GAX85530</td>\n",
              "      <td>MGVKMESTALAYRLLMGKDMIDIGLIDSSEEEEDDEEREEFSAGET...</td>\n",
              "      <td>1</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>GAX80298</td>\n",
              "      <td>MFYTSHTRKKLNEDGKKRASTSEHLIYEDSSLFGLNAQAAFQTYFF...</td>\n",
              "      <td>1</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b4ceaaf9-1eaf-4cdc-93c1-c0daa9ae83a6')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b4ceaaf9-1eaf-4cdc-93c1-c0daa9ae83a6 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b4ceaaf9-1eaf-4cdc-93c1-c0daa9ae83a6');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a09f7f46-5d74-454c-84b0-90227e864356\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a09f7f46-5d74-454c-84b0-90227e864356')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a09f7f46-5d74-454c-84b0-90227e864356 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "  identifier                                           sequence  label    set\n",
              "0   OIQ67012  MSMSRLKDEVELRQAIEATGYHPALVADAVASALGRETCLAYVVHQ...      1  train\n",
              "1   GAX85530  MGVKMESTALAYRLLMGKDMIDIGLIDSSEEEEDDEEREEFSAGET...      1  train\n",
              "2   GAX80298  MFYTSHTRKKLNEDGKKRASTSEHLIYEDSSLFGLNAQAAFQTYFF...      1  train"
            ]
          },
          "execution_count": 108,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "annotations[:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eScyOlMFtGjH"
      },
      "outputs": [],
      "source": [
        "for filename in annotations.identifier:\n",
        "    file = os.path.join(path, filename + suffix)\n",
        "    if (os.path.exists(file)):\n",
        "      result = torch.load(file)\n",
        "      rep = result.get('mean_representations')\n",
        "      val = rep.values()\n",
        "      val = list(val)\n",
        "      val = np.array(val[0])\n",
        "      val = np.reshape(val,(-1,1280))\n",
        "      proteins.append((filename,val))\n",
        "    else:\n",
        "      delete.append(filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZdiQLZSDs8A"
      },
      "outputs": [],
      "source": [
        "#proteins"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zdwva6fUtkxW"
      },
      "outputs": [],
      "source": [
        "# Basic Protocol 3 — Step 6\n",
        "train_set = annotations[annotations.set == \"train\"]\n",
        "test_set = annotations[annotations.set == \"test\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AsPlLP3utmt0",
        "outputId": "097c90a9-295f-47a0-a1d2-8c3feec04ce9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The train set contains 530 samples, and we will test on 1149 samples.\n"
          ]
        }
      ],
      "source": [
        "print(f\"The train set contains {len(train_set)} samples, and we will test on {len(test_set)} samples.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nN1PpTc3tw-o",
        "outputId": "be2cfc3e-b67e-4c4d-da79-548dc40d929d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "530\n",
            "530\n"
          ]
        }
      ],
      "source": [
        "# Basic Protocol 3 — Step 7\n",
        "\n",
        "training_identifiers = train_set.identifier.values\n",
        "training_labels = train_set.label.values\n",
        "print(len(training_identifiers))\n",
        "print(len(training_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SfK4RZisuE63",
        "outputId": "03e4d648-ded6-42fb-ea68-99d057f509f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1149\n",
            "1149\n"
          ]
        }
      ],
      "source": [
        "testing_identifiers = test_set.identifier.values\n",
        "testing_labels = test_set.label.values\n",
        "print(len(testing_identifiers))\n",
        "print(len(testing_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yeo0RsDewH6e"
      },
      "outputs": [],
      "source": [
        "seq = dict(proteins)\n",
        "delete = list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kw7Gs-Ovu1gR"
      },
      "outputs": [],
      "source": [
        "training_embeddings = list()\n",
        "for identifier in training_identifiers:\n",
        "        if identifier in seq:\n",
        "            embedding = seq[identifier]\n",
        "            training_embeddings.append(embedding)\n",
        "        else:\n",
        "          delete.append(identifier)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hu95EwOVxceu"
      },
      "outputs": [],
      "source": [
        "testing_embeddings = list()\n",
        "for identifier in testing_identifiers:\n",
        "        if identifier in seq:\n",
        "            embedding = seq[identifier]\n",
        "            testing_embeddings.append(embedding)\n",
        "        else:\n",
        "          delete.append(identifier)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBgbhAESPht-",
        "outputId": "2e518807-4b39-47e8-83c6-0418a514b28e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([], dtype=int64),)"
            ]
          },
          "execution_count": 118,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.where(training_identifiers=='AIA54169')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TdfBRxkLQytb"
      },
      "outputs": [],
      "source": [
        "#training_identifiers = np.delete(training_identifiers,0)\n",
        "#training_labels = np.delete(training_labels,0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVpB7hTIPlG8",
        "outputId": "2d339cd8-f20e-41eb-f49d-26776a1d8915"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "530"
            ]
          },
          "execution_count": 120,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(training_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLHzaARbS5wx",
        "outputId": "0a36d922-02cb-4668-dbb5-b1fe98c72b3e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "530"
            ]
          },
          "execution_count": 121,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(training_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cq2HsFdtxkpE"
      },
      "outputs": [],
      "source": [
        "# A sanity check: make sure that the numbers are equal!\n",
        "assert(len(training_identifiers) == len(training_embeddings))\n",
        "assert(len(testing_identifiers) == len(testing_embeddings))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJsKdxddnhMO",
        "outputId": "f94bf5e1-e39c-4b73-8a3a-a22ca568f1c3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "530"
            ]
          },
          "execution_count": 123,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(training_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1K3zQYOh3xiC",
        "outputId": "fc5e66a6-6028-45c8-e94c-ee6aca6a1c3c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 124,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "delete"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZOHJFqNvNsJ"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RjStFxt_YIq",
        "outputId": "bad376d9-5620-482a-ce1e-01f252de4ca8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(530, 2560)"
            ]
          },
          "execution_count": 125,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "arr_train = np.array(training_embeddings)\n",
        "nsample, nx, ny = arr_train.shape\n",
        "train_dataset = arr_train.reshape((nsample, nx*ny))\n",
        "train_dataset.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwtSoASGAPK8",
        "outputId": "f6397688-058f-4609-88c3-0b18e844d616"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1149, 2560)"
            ]
          },
          "execution_count": 126,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "arr_test = np.array(testing_embeddings)\n",
        "nsample, nx, ny = arr_test.shape\n",
        "test_dataset = arr_test.reshape((nsample, nx*ny))\n",
        "test_dataset.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eEXVSpeI8Un"
      },
      "source": [
        "### LR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n6VLSupfI-mW"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import plotly.express as px\n",
        "from sklearn.metrics import accuracy_score, make_scorer, f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IPuwO3lT6h93"
      },
      "outputs": [],
      "source": [
        "from numpy.random import seed\n",
        "seed(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7cjQl3Y6h9_"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import set_random_seed\n",
        "set_random_seed(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVSgV_nzJAGf",
        "outputId": "165e44a5-b4c3-499b-cda7-c7c0b6e22833"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.9007832898172323"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lr = LogisticRegression()\n",
        "lr_history = lr.fit(train_dataset, training_labels)\n",
        "lr.score(test_dataset,testing_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "837CqDItNnwt"
      },
      "outputs": [],
      "source": [
        "scoring ={'accuracy':make_scorer(accuracy_score,greater_is_better=True),'f1':make_scorer(f1_score,greater_is_better=True),'mcc':make_scorer(matthews_corrcoef,greater_is_better=True)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1QyODNLQAPaz",
        "outputId": "8808ab97-a015-4dee-b630-8078a158d1ea"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'fit_time': array([1.06840277, 1.01214218, 1.14148068, 0.91625738, 0.97118187,\n",
              "        1.0761559 , 1.02693963, 0.99290609, 0.87978292, 0.78595924]),\n",
              " 'score_time': array([0.01525569, 0.01154733, 0.00829697, 0.00849295, 0.00887275,\n",
              "        0.00831318, 0.00863814, 0.00811386, 0.00906849, 0.00831103]),\n",
              " 'test_accuracy': array([0.76304348, 0.87391304, 0.94347826, 0.90849673, 0.91503268,\n",
              "        0.90849673, 0.78431373, 0.85185185, 0.90413943, 0.94553377]),\n",
              " 'test_f1': array([0.8466948 , 0.91666667, 0.96142433, 0.93948127, 0.94187779,\n",
              "        0.93841642, 0.83849918, 0.88925081, 0.93037975, 0.9614792 ]),\n",
              " 'test_mcc': array([0.36186634, 0.68237648, 0.86137531, 0.77601581, 0.78784731,\n",
              "        0.77141578, 0.52887381, 0.6814988 , 0.78200087, 0.86886148])}"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "scores = cross_validate(lr, train_dataset, training_labels, scoring=scoring, cv=10)\n",
        "scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQ54HJvRSYxi"
      },
      "source": [
        "source https://www.kaggle.com/code/jnikhilsai/cross-validation-with-linear-regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3NXeE_jgw_1",
        "outputId": "c93e411e-a09e-4a8f-e80a-b1f3d3e4fa4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "acc: 0.935554104071413\n",
            "f1: 0.9550015202189115\n",
            "mcc: 0.8416921531263278\n",
            "sn: [0.9614906832298137, 0.9615503875968993, 0.9567600487210719, 0.960582171012735, 0.9603779335568424, 0.9530630905211825, 0.95856524427953, 0.9599510852950168, 0.9613601236476044, 0.9577167019027484, 0.9657348963029756, 0.9586016559337627, 0.9578660200060625, 0.9588452088452089, 0.961864406779661, 0.9604779411764706, 0.9623380536306116, 0.9547936893203883, 0.9575831553249924, 0.96163215590743, 0.9593345656192237, 0.9600720504353047, 0.9591710485617074, 0.9602810876871372, 0.9581818181818181, 0.9570121951219512, 0.9604449938195303, 0.9614689945815773, 0.9644083671557915, 0.9591008429597253, 0.9567484662576687, 0.9589646464646465, 0.9586903304773562, 0.9620330679730558, 0.9606707317073171, 0.964, 0.9556500607533415, 0.9617486338797814, 0.9610823958650045, 0.9560472870566838, 0.9607722954336501, 0.9614067849361967, 0.9628942486085343, 0.9650218613366646, 0.9553846153846154, 0.9600374648766781, 0.9609423434593924, 0.9580398162327718, 0.9625192012288787, 0.9595990279465371, 0.9660912453760789, 0.9620367580596565, 0.9613615455381784, 0.9579598145285935, 0.9593644974029942, 0.9622641509433962, 0.9576427255985267, 0.9570087798970632, 0.9556513704958424, 0.9606418407508326, 0.9640552995391705, 0.9582052858020897, 0.9621394230769231, 0.9583333333333334, 0.9556303549571603, 0.9593147751605996, 0.9616087751371115, 0.9577039274924471, 0.9651376146788991, 0.9613970588235294, 0.9561186650185414, 0.959830866807611, 0.9585253456221198, 0.9531966962373815, 0.9578331794398276, 0.9593984962406015, 0.9640773718145532, 0.9634296250768285, 0.9639474488237091, 0.9614684466019418, 0.9628942486085343, 0.9641772920461446, 0.965527950310559, 0.958756540473992, 0.9627611262488647, 0.9568832768709578, 0.9593520782396088, 0.9653543307086614, 0.9624961573931755, 0.957542908762421, 0.9619366626065774, 0.963088280529068, 0.9569526169092598, 0.9538985148514851, 0.9512946979038225, 0.9591078066914498, 0.9575628978478327, 0.9580911593759559, 0.9615853658536585, 0.9606517061174301, 0.9621441550575409, 0.9632942628007403, 0.9629398941139832, 0.9635193133047211, 0.9592952612393681, 0.9613733905579399, 0.9662265888854774, 0.9611679711017459, 0.9617469879518072, 0.9553409776704889, 0.9621072088724584, 0.9577812018489985, 0.9625117150890347, 0.9636913767019667, 0.9638407778790641, 0.9582560296846011, 0.9601959583588487, 0.960171568627451, 0.9585010839269124, 0.9560945655511207, 0.9594959495949595, 0.9637257915770059, 0.962224236929586, 0.9565881993274228, 0.9583077866339669, 0.9553708833487227, 0.9597335755373902, 0.9580655035200489, 0.9645412130637636, 0.9629629629629629, 0.9645171243443381, 0.9621720561317877, 0.9615733736762481, 0.9638777400432232, 0.9615970740627857, 0.9599149711509262, 0.9591146633876422, 0.9641133896260555, 0.9619945272119185, 0.9527027027027027, 0.9666666666666667, 0.9639721465334544, 0.9636923076923077, 0.9603603603603603, 0.956428352255293, 0.9626197096076614, 0.9681432038834952, 0.9595141700404858, 0.9633168927250308, 0.9621489621489622, 0.9602186456119041, 0.9564949089787103, 0.9652226967663209, 0.9609756097560975, 0.955837870538415, 0.9596847529554411, 0.9578555934304307, 0.9572910311165345, 0.9604349139232861, 0.9647455548743102, 0.9605386783589102, 0.96179706601467, 0.964329268292683, 0.9632690541781451, 0.964438994481913, 0.9614109134760326, 0.9611197511664075, 0.9622181596587447, 0.9627272727272728, 0.9547250075964753, 0.9647887323943662, 0.9592882991556092, 0.9587724736515809, 0.9622013909888116, 0.9621705426356589, 0.9649230769230769, 0.9701177180802898, 0.9588999697793895, 0.9653054958550814, 0.9585744745659458, 0.9602832512315271, 0.9566831683168316, 0.9623392529087569, 0.9640917320458661, 0.9623161764705882, 0.9583207490184235, 0.9598173515981735, 0.9608853850818678, 0.9577635976906715, 0.9596507076181873, 0.9595896798259248, 0.9621353196772191, 0.9629297458893872, 0.9592024539877301, 0.9629175605271223, 0.962457337883959, 0.9595990279465371, 0.9613394216133943, 0.9654750992972808, 0.9646914338348174, 0.9608682360134515, 0.9663505659223004, 0.9639111659469464, 0.9568590937306021, 0.9660762533773641, 0.9528475199020208, 0.9650668286755771, 0.9592274678111588, 0.9611976779712802, 0.9594843462246777, 0.9577379142596534, 0.9571341090018372, 0.9585125998770744, 0.9616087751371115, 0.9623443668387489, 0.9660446619761395, 0.9569174757281553, 0.9577931866144106, 0.9615026389320087, 0.95817843866171, 0.9666767646167828, 0.9605303731113167, 0.9529875644525326, 0.9603537663921927, 0.959291485394655, 0.9592209373097992, 0.9589296826384568, 0.9654754694124773, 0.958256599140577, 0.96220036877689, 0.9635862913096696, 0.964451313755796, 0.9580012262415696, 0.9583970633221168, 0.9597701149425287, 0.9610148514851485, 0.9606879606879607, 0.9630190966959685, 0.9658640658335873, 0.9595288432497735, 0.9652278177458034, 0.9605745721271394, 0.965947096381879, 0.9612883245586868, 0.959866220735786, 0.9607664233576643, 0.9666151468315302, 0.9613122872175797, 0.9695084007467331, 0.9601208459214502, 0.9629629629629629, 0.9665863937387116, 0.955255899479007, 0.9611532625189682, 0.9615384615384616, 0.9613140927233651, 0.9587786259541985, 0.9665323830182833, 0.9566020313942751, 0.9548248309772588, 0.9656133828996283, 0.9671724992257665, 0.9588875453446191, 0.9587628865979382, 0.9606396063960639, 0.9573750383318, 0.9655066214967662, 0.96541169268442, 0.9583718778908418, 0.9595679012345679, 0.9571516646115906, 0.9538226299694189, 0.9566797940018177, 0.9599383667180277, 0.9618553555080867, 0.9611231101511879, 0.9614457831325302, 0.9647377938517179, 0.9614206981016534, 0.9592969472710453, 0.9585365853658536, 0.9641228336880511, 0.9624312767257178, 0.9572160289243749, 0.9649014778325123, 0.96, 0.962188748847218, 0.9557792009759073, 0.9655592807071015, 0.9613899613899614, 0.9610507246376812, 0.9665336198956095, 0.958499846295727, 0.9532233883058471, 0.9579625652040503, 0.9601471489883507, 0.9583718778908418, 0.9604415823367065, 0.9600491702519975, 0.9626618488407106, 0.9581325301204819, 0.9587912087912088, 0.9628942486085343, 0.956959706959707, 0.9621072088724584, 0.9582577132486388, 0.954990814451929, 0.9585635359116023, 0.9614791987673343, 0.9621305418719212, 0.9561956195619562, 0.957820197044335, 0.9642190416925949, 0.9617151607963247, 0.9510684422421802, 0.9570457354758962, 0.9586270303401777, 0.9560108466405544, 0.9606811145510836, 0.9615985014049329, 0.9641214351425943, 0.958179581795818, 0.9571037420139945, 0.9658356417359187, 0.9538790470372633, 0.962098241358399, 0.9591961023142509, 0.9574977551631249, 0.9635322483423749, 0.9626110940851976, 0.9651342178340019, 0.9670667897814712, 0.958307597282273, 0.9667277167277167, 0.957047791893527, 0.9563625267012511, 0.9657513113236655, 0.9577764277035237, 0.9622415343122566, 0.9603900060938452, 0.9560975609756097, 0.9612426945555214, 0.9596302003081664, 0.9561243144424132, 0.9546553808948005, 0.9636363636363636, 0.9567517112632234, 0.9648962148962149, 0.963339494762785, 0.9565481616529931, 0.9603469640644362, 0.9648640391078521, 0.9631788892298251, 0.9604141291108405, 0.9611145131659522, 0.9632888349514563, 0.9572570725707257, 0.9587912087912088, 0.9671779141104294, 0.9587408312958435, 0.9666872301048736, 0.9566676838571865, 0.9603053435114504, 0.9600123992560446, 0.9597772277227723, 0.9640791476407915, 0.9628378378378378, 0.9592209373097992, 0.9632330598602249, 0.9602832512315271, 0.9590839694656489, 0.9585852478839177, 0.9609110495537089, 0.955219271155034, 0.9580655035200489, 0.9671432917554, 0.9599876885195445, 0.9580911593759559, 0.9564417177914111, 0.9635590646826602, 0.9593693147362038, 0.9524844720496894, 0.9659969088098919, 0.9596302003081664, 0.9622524752475248, 0.9575628978478327, 0.9588617383235385, 0.9649068322981367, 0.9674011469966798, 0.9580060422960726, 0.9594553706505295, 0.9663966396639664, 0.9647168059424327, 0.9569392812887236, 0.9582431178472007, 0.9634969325153374, 0.963552833078101, 0.9571384520505705, 0.9569728410131217, 0.9567073170731707, 0.9646125686394142, 0.9527027027027027, 0.9624356255680097, 0.9624888359630842, 0.9622985709942232, 0.9612546125461254, 0.9614443084455324, 0.9540924192086983, 0.9613394216133943, 0.9559093692590325, 0.9565351418002466, 0.9631753697555085, 0.9565084226646248, 0.9661538461538461, 0.9590062111801242, 0.9590214067278288, 0.9581703280168522, 0.957716049382716, 0.9662818955042527, 0.9639830508474576, 0.9644792293798916, 0.9621771217712177, 0.9634221946683199, 0.9643944004869142, 0.9574209245742092, 0.9602203182374541, 0.9557059366348816, 0.9602050045221586, 0.9582309582309583, 0.958256599140577, 0.9618181818181818, 0.965422276621787, 0.962670713201821, 0.9611768274188657, 0.958152510849349, 0.9647995102540557, 0.9602669093114953, 0.9565351418002466, 0.9630428087465353, 0.9654645476772616, 0.9614678899082569, 0.9593769089798412, 0.9577635976906715, 0.9630075187969925, 0.9570906877662811, 0.9565081351689612, 0.9587408312958435, 0.9642638973732437, 0.9586751747189304, 0.9593272171253823, 0.9520295202952029, 0.9609492340042055, 0.956989247311828, 0.9578331794398276, 0.9639065817409767, 0.9627851140456183, 0.96475635917867, 0.9632716049382716, 0.9575253924284395, 0.9604694255713403, 0.9608749229821318, 0.9655382738639829, 0.9622411693057247, 0.9573300822919841, 0.9656523049111178, 0.9617991373998768, 0.9641434262948207, 0.9598282735357252, 0.9574402939375383, 0.9624499846106495, 0.9618813403012604, 0.9625798600547611, 0.958102279728897, 0.9606775559588627, 0.96, 0.9628834355828221, 0.9602360981671326, 0.9587786259541985, 0.9546008531383303, 0.9611145131659522, 0.9665856622114216, 0.952513966480447, 0.9603321033210332, 0.9608442948914041, 0.9588790740176668, 0.9585211902614968, 0.9660757946210269, 0.9650707618187293, 0.9595054095826894, 0.9613362202288895, 0.9642190416925949, 0.9609471094710947, 0.9639222941720629, 0.965569774527727, 0.9573095823095823, 0.9629286376274329, 0.9626658438753471, 0.9627367135003054, 0.9582431178472007, 0.9566031845145176, 0.9576142900393582, 0.9548229548229549, 0.9655066214967662, 0.9621951219512195, 0.961763798951588, 0.9611296689948375, 0.9592898683807775, 0.9553948161543099, 0.9563106796116505, 0.9572257867399939, 0.9537065052950076, 0.9666460778258185, 0.9605421686746988, 0.9593147751605996, 0.9602465331278891, 0.9631109744850906, 0.9641982864137086, 0.9613827468078481, 0.9587253414264036, 0.9623443668387489, 0.9636583888552392, 0.9622641509433962, 0.9591587930508991, 0.9617873651771957, 0.9595776772247361, 0.9651162790697675, 0.9560025141420491, 0.9573283858998145, 0.9586500456065673, 0.9605060166615242, 0.9585601935874168, 0.9608869725900832, 0.9613839975285758, 0.9597069597069597, 0.9598641556035814, 0.9617537313432836, 0.9603537663921927, 0.9566553950199815, 0.9518879415347138, 0.9595129375951293, 0.9521457239888854, 0.9607054455445545, 0.9580929243850592, 0.9619783616692427, 0.9614316568960197, 0.9596412556053812, 0.9555281037677579, 0.9611414693381907, 0.961939840392879, 0.9645585090131378, 0.9580615097856477, 0.9593623007189747, 0.9625114294422432, 0.960880195599022, 0.9614435533621222, 0.961890243902439, 0.9616929255483473, 0.9584352078239609, 0.9611976779712802, 0.9593846153846154, 0.9684466019417476, 0.9662058371735791, 0.9617718446601942, 0.9604141291108405, 0.9598173515981735, 0.9653679653679653, 0.9662408759124088, 0.9548918012800975, 0.9595375722543352, 0.9581564584596726, 0.9654856444715944, 0.9630415025749772, 0.9624286143672979, 0.9562958435207825, 0.9649817295980512, 0.9591523341523341, 0.9601959583588487, 0.9598405885959534, 0.9603779335568424, 0.9561752988047809, 0.9629856225145305, 0.960453709380748, 0.9627213420316869, 0.9596117720726361, 0.9579425113464448, 0.9594220719335997, 0.9578368469294225, 0.9585492227979274, 0.9621374045801526, 0.9601837672281777, 0.968465736810188, 0.9649342517219787, 0.9558425953739862, 0.9614325068870524, 0.9610112701797137, 0.9617402036408516, 0.9642638973732437, 0.9546279491833031, 0.9557331693821088, 0.9570121951219512, 0.9627253284448518, 0.9595566207309766, 0.961670302274852, 0.9521765787860208, 0.957619477006312, 0.9618696186961869, 0.956865127582017, 0.9622411693057247, 0.9608140947752126, 0.9630082543564659, 0.9575720037163209, 0.9577804583835947, 0.9549192811452939, 0.951471034273582, 0.9632690541781451, 0.9635577516985794, 0.9625386996904025, 0.9644403215831787, 0.965296803652968, 0.9608735213830755, 0.9634857318195765, 0.9562974203338391, 0.9566279780085523, 0.9602065613608749, 0.9664202094886013, 0.9606707317073171, 0.9546423135464231, 0.9597047062442325, 0.9585365853658536, 0.9611532625189682, 0.9578816971198514, 0.9598540145985401, 0.959778937672705, 0.9555284800487359, 0.9583458195984417, 0.9629297458893872, 0.9581691421642922, 0.9556513704958424, 0.9648622981956315, 0.9584605986560782, 0.9633555420956996, 0.9582052858020897, 0.9609665427509294, 0.963180693069307, 0.9609471094710947, 0.9594635781773849, 0.9622119815668203, 0.9644808743169399, 0.9583461894477013, 0.9596823457544288, 0.9620065301276343, 0.9600244125724748, 0.95578231292517, 0.9583581201665675, 0.9581183611532625, 0.9593470896211888, 0.9593323216995447, 0.9624810892586989, 0.9651771956856703, 0.9602097470697101, 0.9587253414264036, 0.9583590376310919, 0.9618786215309546, 0.9552329098608591, 0.9588452088452089, 0.95998805613616, 0.9655702428527513, 0.9606827186833282, 0.9656441717791411, 0.9633587786259542, 0.9606106870229008, 0.9646840148698885, 0.9606106870229008, 0.9622700342999688, 0.9624924379915305, 0.9602768582606079, 0.9582952815829529, 0.9530242554498004, 0.9620330679730558, 0.9583077866339669, 0.9592705167173252, 0.9585762503835532, 0.9587029672682778, 0.9656430525995744, 0.9599388379204893, 0.9572078907435508, 0.9571561626823968, 0.9608140947752126, 0.958433734939759, 0.9671476819158735, 0.9566953316953317, 0.9646153846153847, 0.9631197097944377, 0.9587473652514303, 0.9523223623500462, 0.9634369287020109, 0.964438994481913, 0.9580820265379976, 0.9653143388045835, 0.9584732824427481, 0.9596453683888719, 0.9626339969372129, 0.9538461538461539, 0.9553571428571429, 0.960956416464891, 0.9644052327350168, 0.9575313168347083, 0.9556313993174061, 0.9657282741738066, 0.9630415025749772, 0.9644716692189893, 0.9649122807017544, 0.9627463054187192, 0.9584479071188512, 0.9650562139167427, 0.966859227728793, 0.960392999692969, 0.9679193400549955, 0.95817843866171, 0.9576993304930006, 0.9617021276595744, 0.9562065841135609, 0.9633846153846154, 0.9658914728682171, 0.9646511627906976, 0.9637321760694358, 0.9606445615122404, 0.9580567290283645, 0.9610548911376878, 0.962717979555021, 0.9594928880643167, 0.9540614542135686, 0.9598005609224057, 0.9653916211293261, 0.9606060606060606, 0.9579396525449558, 0.9651269501376568, 0.9590643274853801, 0.9622296679865976, 0.9597091790366555, 0.9589540893888719, 0.9617402036408516, 0.9595744680851064, 0.9587878787878787, 0.9601482854494903, 0.9583847102342786, 0.9643835616438357, 0.9630314232902033, 0.9594553706505295, 0.9564817652467055, 0.9629171817058096, 0.963339494762785, 0.9543656829936112, 0.9610271903323263, 0.9588325652841782, 0.9650521152667075, 0.957859120270686, 0.9616093366093366, 0.959349593495935, 0.9594759293113955, 0.9668914776210914, 0.9592466810744057, 0.9610468654899574, 0.9686530105524519, 0.959592843923504, 0.9615970740627857, 0.9657701711491442, 0.9630996309963099, 0.9616904834296138, 0.9613852283174993, 0.9641133896260555, 0.9554868624420402, 0.9602307225258045, 0.9622066443157574, 0.9600609756097561, 0.9612003637465899, 0.9576946658491723, 0.9571603427172583, 0.9630973986690865, 0.9569044006069802, 0.9605425400739828, 0.9656652360515021, 0.9480243161094225, 0.9555416405760802, 0.959214964734744, 0.9582689335394127, 0.9523219814241486, 0.959051724137931, 0.9624173180998196, 0.9604294478527607, 0.9559322033898305, 0.9594635781773849, 0.9637703408044213, 0.9646233607807259, 0.9609160305343512, 0.9595990279465371, 0.9569990850869168, 0.958435960591133, 0.9577981651376147, 0.9575572519083969, 0.9630638813200121, 0.9622235872235873, 0.9600363306085377, 0.9623161764705882, 0.9678800856531049, 0.96179706601467, 0.9631223110018439, 0.9597091790366555, 0.9604051565377533, 0.9583847102342786, 0.9588830929733047, 0.9653250773993808, 0.9612426945555214, 0.9634857318195765, 0.9629518072289157, 0.9592581331711767, 0.9623161764705882, 0.9647455548743102, 0.9591089411046689, 0.9595410628019324, 0.9591263650546021, 0.9567749846719804, 0.9610548911376878, 0.960530280204881, 0.9572727272727273, 0.9648640391078521, 0.9591457753017641, 0.9578720787207872, 0.9611768274188657, 0.9605712511642347, 0.9598418010343779, 0.9626284646527562, 0.9651019147621989, 0.9549330085261876, 0.9644171779141104, 0.9608259945338598, 0.9613970588235294, 0.9643183897529735, 0.9627067669172933, 0.9638157894736842, 0.9637480798771121, 0.9633333333333334, 0.962596599690881, 0.9599507085643869, 0.9606894466283641, 0.9658328248932275, 0.9611414693381907, 0.96, 0.9606879606879607, 0.9611620795107033, 0.958941605839416, 0.9541892706449668, 0.9623224212476837, 0.9612426945555214, 0.9568567026194145, 0.9623507805325987, 0.9642524090767796, 0.9583718778908418, 0.9648148148148148, 0.9609350968932636, 0.9632197769068436, 0.958667489204195, 0.9605745721271394, 0.9637941352483543, 0.9623046276432731, 0.9622930717351318, 0.9594220719335997, 0.9600495509445649, 0.9602932193036041, 0.963020030816641, 0.960617994547107, 0.9588957055214724, 0.9568832768709578, 0.9606811145510836, 0.9644934190388735, 0.960438222763238, 0.9564164648910412, 0.9593798449612403, 0.9612922889362999, 0.9614673242909988, 0.9698630136986301, 0.96179706601467, 0.9624456859093731, 0.9645585090131378, 0.9611921763427507, 0.964741641337386, 0.960995085995086, 0.9625570776255707, 0.9559672031582144, 0.9573095823095823, 0.9659969088098919, 0.958756540473992, 0.9649069270674397, 0.9635914841098426, 0.9613409845967985, 0.9643719806763285, 0.9585365853658536, 0.9610468654899574, 0.9622075584883023, 0.9607782581840643, 0.9593596059113301, 0.9629854368932039, 0.9625270981728089, 0.9612403100775194, 0.963302752293578, 0.9614443084455324, 0.9578754578754579, 0.9602977667493796, 0.9599518217404396, 0.9665551839464883, 0.9541820418204182, 0.9595410628019324, 0.9627118644067797, 0.9640157242213486, 0.9642529789184234, 0.9555284800487359, 0.9562403697996918, 0.9593924364538128, 0.9606758832565284, 0.9570383912248629, 0.9535452322738386, 0.9584097859327217, 0.9649652672908486, 0.955629338967703, 0.9618344105878732, 0.9545454545454546, 0.9611532625189682, 0.9612332112332113, 0.9559093692590325, 0.9630086313193588, 0.9580655035200489, 0.9591100420926038, 0.9610307982401005, 0.9587282176704371, 0.9621489621489622, 0.9557173187746436, 0.9655702428527513, 0.9621026894865525, 0.9569070904645477, 0.9626749611197511, 0.9672279013830427, 0.9576993304930006, 0.9623942337825133, 0.9586751747189304, 0.9629292008508052, 0.9580764488286067, 0.9541555752735877, 0.9551681195516812, 0.9539634146341464, 0.9649176327028676, 0.9567205120390125, 0.9640791476407915, 0.9580801944106926, 0.9595092024539877, 0.9573283858998145, 0.9615384615384616, 0.9613992762364294, 0.9594678217821783, 0.9616093366093366, 0.9617385981022345, 0.9609875038098141, 0.9596480582524272, 0.9644268774703557, 0.9624128523795089, 0.9584097859327217, 0.9585889570552147, 0.9567351948450445, 0.965158924205379, 0.9616916947594238, 0.960254854368932, 0.9560339599757429, 0.9533273110508883, 0.9579367516119128, 0.9578915480157528, 0.9576014536644458, 0.9651849549269506, 0.9591648756524409, 0.9593041317179248, 0.9605745721271394, 0.9666972196761381, 0.9541256157635468, 0.9580615097856477, 0.9595393713040772, 0.9635464936669756, 0.9572468162522741, 0.9578915480157528, 0.9650455927051672, 0.955837870538415, 0.9559050262102992, 0.9576606762107828, 0.9579962663347853, 0.9622524752475248, 0.9613733905579399, 0.9657056145675266, 0.9647995102540557, 0.9663636363636363, 0.9650735294117647, 0.9637703408044213, 0.9648102815177478, 0.9620098039215687]\n",
            "sp: [0.9614906832298137, 0.9615503875968993, 0.9567600487210719, 0.960582171012735, 0.9603779335568424, 0.9530630905211825, 0.95856524427953, 0.9599510852950168, 0.9613601236476044, 0.9577167019027484, 0.9657348963029756, 0.9586016559337627, 0.9578660200060625, 0.9588452088452089, 0.961864406779661, 0.9604779411764706, 0.9623380536306116, 0.9547936893203883, 0.9575831553249924, 0.96163215590743, 0.9593345656192237, 0.9600720504353047, 0.9591710485617074, 0.9602810876871372, 0.9581818181818181, 0.9570121951219512, 0.9604449938195303, 0.9614689945815773, 0.9644083671557915, 0.9591008429597253, 0.9567484662576687, 0.9589646464646465, 0.9586903304773562, 0.9620330679730558, 0.9606707317073171, 0.964, 0.9556500607533415, 0.9617486338797814, 0.9610823958650045, 0.9560472870566838, 0.9607722954336501, 0.9614067849361967, 0.9628942486085343, 0.9650218613366646, 0.9553846153846154, 0.9600374648766781, 0.9609423434593924, 0.9580398162327718, 0.9625192012288787, 0.9595990279465371, 0.9660912453760789, 0.9620367580596565, 0.9613615455381784, 0.9579598145285935, 0.9593644974029942, 0.9622641509433962, 0.9576427255985267, 0.9570087798970632, 0.9556513704958424, 0.9606418407508326, 0.9640552995391705, 0.9582052858020897, 0.9621394230769231, 0.9583333333333334, 0.9556303549571603, 0.9593147751605996, 0.9616087751371115, 0.9577039274924471, 0.9651376146788991, 0.9613970588235294, 0.9561186650185414, 0.959830866807611, 0.9585253456221198, 0.9531966962373815, 0.9578331794398276, 0.9593984962406015, 0.9640773718145532, 0.9634296250768285, 0.9639474488237091, 0.9614684466019418, 0.9628942486085343, 0.9641772920461446, 0.965527950310559, 0.958756540473992, 0.9627611262488647, 0.9568832768709578, 0.9593520782396088, 0.9653543307086614, 0.9624961573931755, 0.957542908762421, 0.9619366626065774, 0.963088280529068, 0.9569526169092598, 0.9538985148514851, 0.9512946979038225, 0.9591078066914498, 0.9575628978478327, 0.9580911593759559, 0.9615853658536585, 0.9606517061174301, 0.9621441550575409, 0.9632942628007403, 0.9629398941139832, 0.9635193133047211, 0.9592952612393681, 0.9613733905579399, 0.9662265888854774, 0.9611679711017459, 0.9617469879518072, 0.9553409776704889, 0.9621072088724584, 0.9577812018489985, 0.9625117150890347, 0.9636913767019667, 0.9638407778790641, 0.9582560296846011, 0.9601959583588487, 0.960171568627451, 0.9585010839269124, 0.9560945655511207, 0.9594959495949595, 0.9637257915770059, 0.962224236929586, 0.9565881993274228, 0.9583077866339669, 0.9553708833487227, 0.9597335755373902, 0.9580655035200489, 0.9645412130637636, 0.9629629629629629, 0.9645171243443381, 0.9621720561317877, 0.9615733736762481, 0.9638777400432232, 0.9615970740627857, 0.9599149711509262, 0.9591146633876422, 0.9641133896260555, 0.9619945272119185, 0.9527027027027027, 0.9666666666666667, 0.9639721465334544, 0.9636923076923077, 0.9603603603603603, 0.956428352255293, 0.9626197096076614, 0.9681432038834952, 0.9595141700404858, 0.9633168927250308, 0.9621489621489622, 0.9602186456119041, 0.9564949089787103, 0.9652226967663209, 0.9609756097560975, 0.955837870538415, 0.9596847529554411, 0.9578555934304307, 0.9572910311165345, 0.9604349139232861, 0.9647455548743102, 0.9605386783589102, 0.96179706601467, 0.964329268292683, 0.9632690541781451, 0.964438994481913, 0.9614109134760326, 0.9611197511664075, 0.9622181596587447, 0.9627272727272728, 0.9547250075964753, 0.9647887323943662, 0.9592882991556092, 0.9587724736515809, 0.9622013909888116, 0.9621705426356589, 0.9649230769230769, 0.9701177180802898, 0.9588999697793895, 0.9653054958550814, 0.9585744745659458, 0.9602832512315271, 0.9566831683168316, 0.9623392529087569, 0.9640917320458661, 0.9623161764705882, 0.9583207490184235, 0.9598173515981735, 0.9608853850818678, 0.9577635976906715, 0.9596507076181873, 0.9595896798259248, 0.9621353196772191, 0.9629297458893872, 0.9592024539877301, 0.9629175605271223, 0.962457337883959, 0.9595990279465371, 0.9613394216133943, 0.9654750992972808, 0.9646914338348174, 0.9608682360134515, 0.9663505659223004, 0.9639111659469464, 0.9568590937306021, 0.9660762533773641, 0.9528475199020208, 0.9650668286755771, 0.9592274678111588, 0.9611976779712802, 0.9594843462246777, 0.9577379142596534, 0.9571341090018372, 0.9585125998770744, 0.9616087751371115, 0.9623443668387489, 0.9660446619761395, 0.9569174757281553, 0.9577931866144106, 0.9615026389320087, 0.95817843866171, 0.9666767646167828, 0.9605303731113167, 0.9529875644525326, 0.9603537663921927, 0.959291485394655, 0.9592209373097992, 0.9589296826384568, 0.9654754694124773, 0.958256599140577, 0.96220036877689, 0.9635862913096696, 0.964451313755796, 0.9580012262415696, 0.9583970633221168, 0.9597701149425287, 0.9610148514851485, 0.9606879606879607, 0.9630190966959685, 0.9658640658335873, 0.9595288432497735, 0.9652278177458034, 0.9605745721271394, 0.965947096381879, 0.9612883245586868, 0.959866220735786, 0.9607664233576643, 0.9666151468315302, 0.9613122872175797, 0.9695084007467331, 0.9601208459214502, 0.9629629629629629, 0.9665863937387116, 0.955255899479007, 0.9611532625189682, 0.9615384615384616, 0.9613140927233651, 0.9587786259541985, 0.9665323830182833, 0.9566020313942751, 0.9548248309772588, 0.9656133828996283, 0.9671724992257665, 0.9588875453446191, 0.9587628865979382, 0.9606396063960639, 0.9573750383318, 0.9655066214967662, 0.96541169268442, 0.9583718778908418, 0.9595679012345679, 0.9571516646115906, 0.9538226299694189, 0.9566797940018177, 0.9599383667180277, 0.9618553555080867, 0.9611231101511879, 0.9614457831325302, 0.9647377938517179, 0.9614206981016534, 0.9592969472710453, 0.9585365853658536, 0.9641228336880511, 0.9624312767257178, 0.9572160289243749, 0.9649014778325123, 0.96, 0.962188748847218, 0.9557792009759073, 0.9655592807071015, 0.9613899613899614, 0.9610507246376812, 0.9665336198956095, 0.958499846295727, 0.9532233883058471, 0.9579625652040503, 0.9601471489883507, 0.9583718778908418, 0.9604415823367065, 0.9600491702519975, 0.9626618488407106, 0.9581325301204819, 0.9587912087912088, 0.9628942486085343, 0.956959706959707, 0.9621072088724584, 0.9582577132486388, 0.954990814451929, 0.9585635359116023, 0.9614791987673343, 0.9621305418719212, 0.9561956195619562, 0.957820197044335, 0.9642190416925949, 0.9617151607963247, 0.9510684422421802, 0.9570457354758962, 0.9586270303401777, 0.9560108466405544, 0.9606811145510836, 0.9615985014049329, 0.9641214351425943, 0.958179581795818, 0.9571037420139945, 0.9658356417359187, 0.9538790470372633, 0.962098241358399, 0.9591961023142509, 0.9574977551631249, 0.9635322483423749, 0.9626110940851976, 0.9651342178340019, 0.9670667897814712, 0.958307597282273, 0.9667277167277167, 0.957047791893527, 0.9563625267012511, 0.9657513113236655, 0.9577764277035237, 0.9622415343122566, 0.9603900060938452, 0.9560975609756097, 0.9612426945555214, 0.9596302003081664, 0.9561243144424132, 0.9546553808948005, 0.9636363636363636, 0.9567517112632234, 0.9648962148962149, 0.963339494762785, 0.9565481616529931, 0.9603469640644362, 0.9648640391078521, 0.9631788892298251, 0.9604141291108405, 0.9611145131659522, 0.9632888349514563, 0.9572570725707257, 0.9587912087912088, 0.9671779141104294, 0.9587408312958435, 0.9666872301048736, 0.9566676838571865, 0.9603053435114504, 0.9600123992560446, 0.9597772277227723, 0.9640791476407915, 0.9628378378378378, 0.9592209373097992, 0.9632330598602249, 0.9602832512315271, 0.9590839694656489, 0.9585852478839177, 0.9609110495537089, 0.955219271155034, 0.9580655035200489, 0.9671432917554, 0.9599876885195445, 0.9580911593759559, 0.9564417177914111, 0.9635590646826602, 0.9593693147362038, 0.9524844720496894, 0.9659969088098919, 0.9596302003081664, 0.9622524752475248, 0.9575628978478327, 0.9588617383235385, 0.9649068322981367, 0.9674011469966798, 0.9580060422960726, 0.9594553706505295, 0.9663966396639664, 0.9647168059424327, 0.9569392812887236, 0.9582431178472007, 0.9634969325153374, 0.963552833078101, 0.9571384520505705, 0.9569728410131217, 0.9567073170731707, 0.9646125686394142, 0.9527027027027027, 0.9624356255680097, 0.9624888359630842, 0.9622985709942232, 0.9612546125461254, 0.9614443084455324, 0.9540924192086983, 0.9613394216133943, 0.9559093692590325, 0.9565351418002466, 0.9631753697555085, 0.9565084226646248, 0.9661538461538461, 0.9590062111801242, 0.9590214067278288, 0.9581703280168522, 0.957716049382716, 0.9662818955042527, 0.9639830508474576, 0.9644792293798916, 0.9621771217712177, 0.9634221946683199, 0.9643944004869142, 0.9574209245742092, 0.9602203182374541, 0.9557059366348816, 0.9602050045221586, 0.9582309582309583, 0.958256599140577, 0.9618181818181818, 0.965422276621787, 0.962670713201821, 0.9611768274188657, 0.958152510849349, 0.9647995102540557, 0.9602669093114953, 0.9565351418002466, 0.9630428087465353, 0.9654645476772616, 0.9614678899082569, 0.9593769089798412, 0.9577635976906715, 0.9630075187969925, 0.9570906877662811, 0.9565081351689612, 0.9587408312958435, 0.9642638973732437, 0.9586751747189304, 0.9593272171253823, 0.9520295202952029, 0.9609492340042055, 0.956989247311828, 0.9578331794398276, 0.9639065817409767, 0.9627851140456183, 0.96475635917867, 0.9632716049382716, 0.9575253924284395, 0.9604694255713403, 0.9608749229821318, 0.9655382738639829, 0.9622411693057247, 0.9573300822919841, 0.9656523049111178, 0.9617991373998768, 0.9641434262948207, 0.9598282735357252, 0.9574402939375383, 0.9624499846106495, 0.9618813403012604, 0.9625798600547611, 0.958102279728897, 0.9606775559588627, 0.96, 0.9628834355828221, 0.9602360981671326, 0.9587786259541985, 0.9546008531383303, 0.9611145131659522, 0.9665856622114216, 0.952513966480447, 0.9603321033210332, 0.9608442948914041, 0.9588790740176668, 0.9585211902614968, 0.9660757946210269, 0.9650707618187293, 0.9595054095826894, 0.9613362202288895, 0.9642190416925949, 0.9609471094710947, 0.9639222941720629, 0.965569774527727, 0.9573095823095823, 0.9629286376274329, 0.9626658438753471, 0.9627367135003054, 0.9582431178472007, 0.9566031845145176, 0.9576142900393582, 0.9548229548229549, 0.9655066214967662, 0.9621951219512195, 0.961763798951588, 0.9611296689948375, 0.9592898683807775, 0.9553948161543099, 0.9563106796116505, 0.9572257867399939, 0.9537065052950076, 0.9666460778258185, 0.9605421686746988, 0.9593147751605996, 0.9602465331278891, 0.9631109744850906, 0.9641982864137086, 0.9613827468078481, 0.9587253414264036, 0.9623443668387489, 0.9636583888552392, 0.9622641509433962, 0.9591587930508991, 0.9617873651771957, 0.9595776772247361, 0.9651162790697675, 0.9560025141420491, 0.9573283858998145, 0.9586500456065673, 0.9605060166615242, 0.9585601935874168, 0.9608869725900832, 0.9613839975285758, 0.9597069597069597, 0.9598641556035814, 0.9617537313432836, 0.9603537663921927, 0.9566553950199815, 0.9518879415347138, 0.9595129375951293, 0.9521457239888854, 0.9607054455445545, 0.9580929243850592, 0.9619783616692427, 0.9614316568960197, 0.9596412556053812, 0.9555281037677579, 0.9611414693381907, 0.961939840392879, 0.9645585090131378, 0.9580615097856477, 0.9593623007189747, 0.9625114294422432, 0.960880195599022, 0.9614435533621222, 0.961890243902439, 0.9616929255483473, 0.9584352078239609, 0.9611976779712802, 0.9593846153846154, 0.9684466019417476, 0.9662058371735791, 0.9617718446601942, 0.9604141291108405, 0.9598173515981735, 0.9653679653679653, 0.9662408759124088, 0.9548918012800975, 0.9595375722543352, 0.9581564584596726, 0.9654856444715944, 0.9630415025749772, 0.9624286143672979, 0.9562958435207825, 0.9649817295980512, 0.9591523341523341, 0.9601959583588487, 0.9598405885959534, 0.9603779335568424, 0.9561752988047809, 0.9629856225145305, 0.960453709380748, 0.9627213420316869, 0.9596117720726361, 0.9579425113464448, 0.9594220719335997, 0.9578368469294225, 0.9585492227979274, 0.9621374045801526, 0.9601837672281777, 0.968465736810188, 0.9649342517219787, 0.9558425953739862, 0.9614325068870524, 0.9610112701797137, 0.9617402036408516, 0.9642638973732437, 0.9546279491833031, 0.9557331693821088, 0.9570121951219512, 0.9627253284448518, 0.9595566207309766, 0.961670302274852, 0.9521765787860208, 0.957619477006312, 0.9618696186961869, 0.956865127582017, 0.9622411693057247, 0.9608140947752126, 0.9630082543564659, 0.9575720037163209, 0.9577804583835947, 0.9549192811452939, 0.951471034273582, 0.9632690541781451, 0.9635577516985794, 0.9625386996904025, 0.9644403215831787, 0.965296803652968, 0.9608735213830755, 0.9634857318195765, 0.9562974203338391, 0.9566279780085523, 0.9602065613608749, 0.9664202094886013, 0.9606707317073171, 0.9546423135464231, 0.9597047062442325, 0.9585365853658536, 0.9611532625189682, 0.9578816971198514, 0.9598540145985401, 0.959778937672705, 0.9555284800487359, 0.9583458195984417, 0.9629297458893872, 0.9581691421642922, 0.9556513704958424, 0.9648622981956315, 0.9584605986560782, 0.9633555420956996, 0.9582052858020897, 0.9609665427509294, 0.963180693069307, 0.9609471094710947, 0.9594635781773849, 0.9622119815668203, 0.9644808743169399, 0.9583461894477013, 0.9596823457544288, 0.9620065301276343, 0.9600244125724748, 0.95578231292517, 0.9583581201665675, 0.9581183611532625, 0.9593470896211888, 0.9593323216995447, 0.9624810892586989, 0.9651771956856703, 0.9602097470697101, 0.9587253414264036, 0.9583590376310919, 0.9618786215309546, 0.9552329098608591, 0.9588452088452089, 0.95998805613616, 0.9655702428527513, 0.9606827186833282, 0.9656441717791411, 0.9633587786259542, 0.9606106870229008, 0.9646840148698885, 0.9606106870229008, 0.9622700342999688, 0.9624924379915305, 0.9602768582606079, 0.9582952815829529, 0.9530242554498004, 0.9620330679730558, 0.9583077866339669, 0.9592705167173252, 0.9585762503835532, 0.9587029672682778, 0.9656430525995744, 0.9599388379204893, 0.9572078907435508, 0.9571561626823968, 0.9608140947752126, 0.958433734939759, 0.9671476819158735, 0.9566953316953317, 0.9646153846153847, 0.9631197097944377, 0.9587473652514303, 0.9523223623500462, 0.9634369287020109, 0.964438994481913, 0.9580820265379976, 0.9653143388045835, 0.9584732824427481, 0.9596453683888719, 0.9626339969372129, 0.9538461538461539, 0.9553571428571429, 0.960956416464891, 0.9644052327350168, 0.9575313168347083, 0.9556313993174061, 0.9657282741738066, 0.9630415025749772, 0.9644716692189893, 0.9649122807017544, 0.9627463054187192, 0.9584479071188512, 0.9650562139167427, 0.966859227728793, 0.960392999692969, 0.9679193400549955, 0.95817843866171, 0.9576993304930006, 0.9617021276595744, 0.9562065841135609, 0.9633846153846154, 0.9658914728682171, 0.9646511627906976, 0.9637321760694358, 0.9606445615122404, 0.9580567290283645, 0.9610548911376878, 0.962717979555021, 0.9594928880643167, 0.9540614542135686, 0.9598005609224057, 0.9653916211293261, 0.9606060606060606, 0.9579396525449558, 0.9651269501376568, 0.9590643274853801, 0.9622296679865976, 0.9597091790366555, 0.9589540893888719, 0.9617402036408516, 0.9595744680851064, 0.9587878787878787, 0.9601482854494903, 0.9583847102342786, 0.9643835616438357, 0.9630314232902033, 0.9594553706505295, 0.9564817652467055, 0.9629171817058096, 0.963339494762785, 0.9543656829936112, 0.9610271903323263, 0.9588325652841782, 0.9650521152667075, 0.957859120270686, 0.9616093366093366, 0.959349593495935, 0.9594759293113955, 0.9668914776210914, 0.9592466810744057, 0.9610468654899574, 0.9686530105524519, 0.959592843923504, 0.9615970740627857, 0.9657701711491442, 0.9630996309963099, 0.9616904834296138, 0.9613852283174993, 0.9641133896260555, 0.9554868624420402, 0.9602307225258045, 0.9622066443157574, 0.9600609756097561, 0.9612003637465899, 0.9576946658491723, 0.9571603427172583, 0.9630973986690865, 0.9569044006069802, 0.9605425400739828, 0.9656652360515021, 0.9480243161094225, 0.9555416405760802, 0.959214964734744, 0.9582689335394127, 0.9523219814241486, 0.959051724137931, 0.9624173180998196, 0.9604294478527607, 0.9559322033898305, 0.9594635781773849, 0.9637703408044213, 0.9646233607807259, 0.9609160305343512, 0.9595990279465371, 0.9569990850869168, 0.958435960591133, 0.9577981651376147, 0.9575572519083969, 0.9630638813200121, 0.9622235872235873, 0.9600363306085377, 0.9623161764705882, 0.9678800856531049, 0.96179706601467, 0.9631223110018439, 0.9597091790366555, 0.9604051565377533, 0.9583847102342786, 0.9588830929733047, 0.9653250773993808, 0.9612426945555214, 0.9634857318195765, 0.9629518072289157, 0.9592581331711767, 0.9623161764705882, 0.9647455548743102, 0.9591089411046689, 0.9595410628019324, 0.9591263650546021, 0.9567749846719804, 0.9610548911376878, 0.960530280204881, 0.9572727272727273, 0.9648640391078521, 0.9591457753017641, 0.9578720787207872, 0.9611768274188657, 0.9605712511642347, 0.9598418010343779, 0.9626284646527562, 0.9651019147621989, 0.9549330085261876, 0.9644171779141104, 0.9608259945338598, 0.9613970588235294, 0.9643183897529735, 0.9627067669172933, 0.9638157894736842, 0.9637480798771121, 0.9633333333333334, 0.962596599690881, 0.9599507085643869, 0.9606894466283641, 0.9658328248932275, 0.9611414693381907, 0.96, 0.9606879606879607, 0.9611620795107033, 0.958941605839416, 0.9541892706449668, 0.9623224212476837, 0.9612426945555214, 0.9568567026194145, 0.9623507805325987, 0.9642524090767796, 0.9583718778908418, 0.9648148148148148, 0.9609350968932636, 0.9632197769068436, 0.958667489204195, 0.9605745721271394, 0.9637941352483543, 0.9623046276432731, 0.9622930717351318, 0.9594220719335997, 0.9600495509445649, 0.9602932193036041, 0.963020030816641, 0.960617994547107, 0.9588957055214724, 0.9568832768709578, 0.9606811145510836, 0.9644934190388735, 0.960438222763238, 0.9564164648910412, 0.9593798449612403, 0.9612922889362999, 0.9614673242909988, 0.9698630136986301, 0.96179706601467, 0.9624456859093731, 0.9645585090131378, 0.9611921763427507, 0.964741641337386, 0.960995085995086, 0.9625570776255707, 0.9559672031582144, 0.9573095823095823, 0.9659969088098919, 0.958756540473992, 0.9649069270674397, 0.9635914841098426, 0.9613409845967985, 0.9643719806763285, 0.9585365853658536, 0.9610468654899574, 0.9622075584883023, 0.9607782581840643, 0.9593596059113301, 0.9629854368932039, 0.9625270981728089, 0.9612403100775194, 0.963302752293578, 0.9614443084455324, 0.9578754578754579, 0.9602977667493796, 0.9599518217404396, 0.9665551839464883, 0.9541820418204182, 0.9595410628019324, 0.9627118644067797, 0.9640157242213486, 0.9642529789184234, 0.9555284800487359, 0.9562403697996918, 0.9593924364538128, 0.9606758832565284, 0.9570383912248629, 0.9535452322738386, 0.9584097859327217, 0.9649652672908486, 0.955629338967703, 0.9618344105878732, 0.9545454545454546, 0.9611532625189682, 0.9612332112332113, 0.9559093692590325, 0.9630086313193588, 0.9580655035200489, 0.9591100420926038, 0.9610307982401005, 0.9587282176704371, 0.9621489621489622, 0.9557173187746436, 0.9655702428527513, 0.9621026894865525, 0.9569070904645477, 0.9626749611197511, 0.9672279013830427, 0.9576993304930006, 0.9623942337825133, 0.9586751747189304, 0.9629292008508052, 0.9580764488286067, 0.9541555752735877, 0.9551681195516812, 0.9539634146341464, 0.9649176327028676, 0.9567205120390125, 0.9640791476407915, 0.9580801944106926, 0.9595092024539877, 0.9573283858998145, 0.9615384615384616, 0.9613992762364294, 0.9594678217821783, 0.9616093366093366, 0.9617385981022345, 0.9609875038098141, 0.9596480582524272, 0.9644268774703557, 0.9624128523795089, 0.9584097859327217, 0.9585889570552147, 0.9567351948450445, 0.965158924205379, 0.9616916947594238, 0.960254854368932, 0.9560339599757429, 0.9533273110508883, 0.9579367516119128, 0.9578915480157528, 0.9576014536644458, 0.9651849549269506, 0.9591648756524409, 0.9593041317179248, 0.9605745721271394, 0.9666972196761381, 0.9541256157635468, 0.9580615097856477, 0.9595393713040772, 0.9635464936669756, 0.9572468162522741, 0.9578915480157528, 0.9650455927051672, 0.955837870538415, 0.9559050262102992, 0.9576606762107828, 0.9579962663347853, 0.9622524752475248, 0.9613733905579399, 0.9657056145675266, 0.9647995102540557, 0.9663636363636363, 0.9650735294117647, 0.9637703408044213, 0.9648102815177478, 0.9620098039215687]\n",
            "sd_acc: 0.0035351913142641113\n",
            "sd_f1: 0.0025523710145743963\n",
            "sd_mcc: 0.008688682317113545\n",
            "sd_sn: 0.003302546898508732\n",
            "sd_sp: 0.003302546898508732\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.87      0.89      1323\n",
            "           1       0.95      0.96      0.96      3270\n",
            "\n",
            "    accuracy                           0.94      4593\n",
            "   macro avg       0.92      0.92      0.92      4593\n",
            "weighted avg       0.94      0.94      0.94      4593\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.0035351913142641113, 0.008688682317113545, 0.0025523710145743963)"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predicted_training_labels = lr.predict(train_dataset)\n",
        "error_rate(training_labels, predicted_training_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjQLJMtyJNtD",
        "outputId": "1a742bcb-0de3-452d-bbf4-bdf215a09928"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Our model has an accuracy of 0.9\n"
          ]
        }
      ],
      "source": [
        "predicted_lr = lr.predict(test_dataset)\n",
        "accuracy = accuracy_score(testing_labels, predicted_lr)\n",
        "\n",
        "print(f\"Our model has an accuracy of {accuracy:.2}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxjhmdcwJXsD",
        "outputId": "7f3fe002-8249-46ee-b176-5fa30ac294cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "acc: 0.9007832898172323\n",
            "f1: 0.9313253012048193\n",
            "mcc: 0.7537980462711416\n",
            "sn: [0.9439707673568819, 0.9519807923169268, 0.9459459459459459, 0.933253873659118, 0.9392466585662211, 0.9644607843137255, 0.9507803121248499, 0.9378806333739342, 0.9537712895377128, 0.9481302774427021, 0.9527363184079602, 0.9517326732673267, 0.964329643296433, 0.9409681227863046, 0.9444444444444444, 0.9602409638554217, 0.9445812807881774, 0.95125, 0.9427527405602923, 0.9421686746987952, 0.9408866995073891, 0.9548229548229549, 0.9601990049751243, 0.9548693586698337, 0.9337423312883436, 0.951530612244898, 0.9419191919191919, 0.9475032010243278, 0.9550970873786407, 0.944792973651192, 0.95, 0.921760391198044, 0.9419588875453446, 0.9398315282791817, 0.9505562422744128, 0.9423312883435583, 0.9493201483312732, 0.958128078817734, 0.9645476772616137, 0.9375764993880049, 0.9457459926017263, 0.9459459459459459, 0.9532374100719424, 0.9379474940334129, 0.9445129469790382, 0.9471744471744472, 0.9304029304029304, 0.9462102689486552, 0.9538834951456311, 0.9408926417370326, 0.9433734939759036, 0.944920440636475, 0.9551122194513716, 0.9476961394769614, 0.9471788715486195, 0.9322459222082811, 0.9500594530321046, 0.9359430604982206, 0.9502427184466019, 0.9463414634146341, 0.9494584837545126, 0.9571938168846611, 0.9411042944785276, 0.9341317365269461, 0.9483793517406963, 0.9526123936816525, 0.9356376638855781, 0.957972805933251, 0.9516908212560387, 0.9464068209500609, 0.9583333333333334, 0.9307411907654921, 0.931077694235589, 0.9458483754512635, 0.9544334975369458, 0.951073985680191, 0.9300361881785284, 0.9346793349168646, 0.9566294919454771, 0.9229813664596274, 0.9405204460966543, 0.94625, 0.9573170731707317, 0.94026284348865, 0.947560975609756, 0.9527272727272728, 0.9302325581395349, 0.9484029484029484, 0.9408866995073891, 0.9536019536019537, 0.9333333333333333, 0.9347290640394089, 0.9333333333333333, 0.9471744471744472, 0.9458794587945879, 0.96375, 0.9478787878787879, 0.9445114595898673, 0.9390386869871044, 0.9582836710369488, 0.9520958083832335, 0.9588528678304239, 0.9500609013398295, 0.9443772672309553, 0.9472361809045227, 0.9547123623011016, 0.9464922711058263, 0.9411042944785276, 0.948905109489051, 0.9441141498216409, 0.944920440636475, 0.9554753309265944, 0.9311594202898551, 0.9286592865928659, 0.9539007092198581, 0.9523212045169385, 0.928136419001218, 0.9428223844282239, 0.9552795031055901, 0.9476248477466505, 0.9358024691358025, 0.9476885644768857, 0.9243697478991597, 0.9389788293897883, 0.9598445595854922, 0.9368029739776952, 0.9419035846724351, 0.940389294403893, 0.9271844660194175, 0.9385749385749386, 0.9377431906614786, 0.9508982035928144, 0.9538274605103281, 0.9434194341943419, 0.9447174447174447, 0.940677966101695, 0.9524940617577197, 0.9459134615384616, 0.947560975609756, 0.9418457648546145, 0.9526011560693641, 0.9466666666666667, 0.9422604422604423, 0.9475, 0.925700365408039, 0.9517326732673267, 0.9481132075471698, 0.9525593008739076, 0.9315403422982885, 0.9526699029126213, 0.9478260869565217, 0.9482120838471023, 0.9376498800959233, 0.9482758620689655, 0.946047678795483, 0.9319041614123581, 0.9392812887236679, 0.939540507859734, 0.9409638554216867, 0.9521410579345088, 0.9415971394517283, 0.9461444308445532, 0.9537712895377128, 0.9396135265700483, 0.9427527405602923, 0.9411764705882353, 0.936969696969697, 0.9330900243309003, 0.9442379182156134, 0.9356060606060606, 0.9528415961305925, 0.9483173076923077, 0.9232643118148599, 0.9444444444444444, 0.9520807061790668, 0.9493365500603136, 0.938949938949939, 0.957983193277311, 0.9431680773881499, 0.9504830917874396, 0.943609022556391, 0.947560975609756, 0.939622641509434, 0.9518518518518518, 0.9464720194647201, 0.9583333333333334, 0.9416873449131513, 0.9482758620689655, 0.9443742098609356, 0.9398034398034398, 0.9396135265700483, 0.950363196125908, 0.9622641509433962, 0.950530035335689, 0.9411042944785276, 0.9504232164449818, 0.9443786982248521, 0.9532374100719424, 0.943558282208589, 0.9416058394160584, 0.9393203883495146, 0.9425837320574163, 0.9423312883435583, 0.94377990430622, 0.9433734939759036, 0.9534883720930233, 0.9432098765432099, 0.9504232164449818, 0.9467455621301775, 0.9476813317479191, 0.9333333333333333, 0.9447236180904522, 0.9555555555555556, 0.9503030303030303, 0.9476885644768857, 0.9470365699873896, 0.9436274509803921, 0.9451728247914184, 0.9410288582183187, 0.9488139825218477, 0.9380315917375456, 0.9532828282828283, 0.9511599511599511, 0.9501246882793017, 0.9331713244228432, 0.932061978545888, 0.9372647427854455, 0.9423769507803121, 0.9397163120567376, 0.9529553679131484, 0.9466327827191868, 0.9469135802469136, 0.9476885644768857, 0.9395061728395062, 0.9544334975369458, 0.9568965517241379, 0.9497549019607843, 0.943069306930693, 0.9389788293897883, 0.9416342412451362, 0.9434889434889435, 0.9512195121951219, 0.9505428226779252, 0.9442379182156134, 0.9418181818181818, 0.9364303178484108, 0.948626045400239, 0.9438202247191011, 0.9371221281741233, 0.9452054794520548, 0.934040047114252, 0.9494451294697904, 0.9434416365824309, 0.9329268292682927, 0.9507389162561576, 0.94375, 0.9404052443384983, 0.9364303178484108, 0.9399038461538461, 0.9452554744525548, 0.9382566585956417, 0.9393203883495146, 0.9546568627450981, 0.9348093480934809, 0.9338327091136079, 0.9517923362175525, 0.9288343558282208, 0.9415971394517283, 0.9400479616306955, 0.9445129469790382, 0.9502427184466019, 0.9504232164449818, 0.9444444444444444, 0.9496402877697842, 0.945054945054945, 0.956949569495695, 0.9351740696278511, 0.9563106796116505, 0.9349881796690307, 0.959409594095941, 0.9427148194271482, 0.9405339805825242, 0.9598051157125457, 0.9544364508393285, 0.9456118665018541, 0.9556650246305419, 0.9430604982206405, 0.9427480916030534, 0.9442424242424242, 0.9419588875453446, 0.9430604982206405, 0.9448529411764706, 0.9553140096618358, 0.9479905437352246, 0.9504232164449818, 0.9515738498789347, 0.9415274463007159, 0.95, 0.9495192307692307, 0.9496932515337423, 0.9382566585956417, 0.9478908188585607, 0.9436964504283966, 0.9317904993909866, 0.941320293398533, 0.9365269461077844, 0.943127962085308, 0.9459119496855346, 0.9246658566221142, 0.9407313997477932, 0.9343434343434344, 0.9480840543881335, 0.9426829268292682, 0.9493975903614458, 0.9342723004694836, 0.9443037974683545, 0.9400244798041616, 0.944640753828033, 0.9455006337135615, 0.9477521263669502, 0.9502617801047121, 0.9395807644882861, 0.9418742586002372, 0.9372693726937269, 0.9338942307692307, 0.9534883720930233, 0.9413173652694611, 0.9381818181818182, 0.9474969474969475, 0.9433734939759036, 0.9556354916067147, 0.9490291262135923, 0.9452554744525548, 0.9526699029126213, 0.9331742243436754, 0.9579207920792079, 0.95079950799508, 0.960431654676259, 0.9505562422744128, 0.9461444308445532, 0.9344660194174758, 0.9540636042402827, 0.9432367149758454, 0.946078431372549, 0.9449760765550239, 0.9459459459459459, 0.930778739184178, 0.9445129469790382, 0.9408926417370326, 0.9469026548672567, 0.954653937947494, 0.9363525091799265, 0.941320293398533, 0.9559902200488998, 0.9499404052443385, 0.9473039215686274, 0.9425149700598803, 0.9419035846724351, 0.9543269230769231, 0.9477521263669502, 0.9453681710213777, 0.9426829268292682, 0.951397326852977, 0.9458438287153652, 0.9464720194647201, 0.9412484700122399, 0.9418181818181818, 0.950530035335689, 0.946236559139785, 0.9390862944162437, 0.9564691656590084, 0.9385342789598109, 0.940959409594096, 0.9468354430379747, 0.9375764993880049, 0.9525, 0.9609467455621302, 0.9438202247191011, 0.9431680773881499, 0.9465174129353234, 0.9505562422744128, 0.939877300613497, 0.93375, 0.9405204460966543, 0.9460154241645244, 0.9519343493552169, 0.9461538461538461, 0.9582822085889571, 0.9459783913565426, 0.941320293398533, 0.937192118226601, 0.9422110552763819, 0.9297163995067818, 0.9335748792270532, 0.9405339805825242, 0.9486552567237164, 0.9495192307692307, 0.9369592088998764, 0.9506781750924784, 0.944233206590621, 0.9464922711058263, 0.9502427184466019, 0.9418181818181818, 0.9495192307692307, 0.9504716981132075, 0.9500609013398295, 0.9577464788732394, 0.9509433962264151, 0.9467956469165659, 0.9459459459459459, 0.9466327827191868, 0.9405940594059405, 0.9281210592686002, 0.9526184538653366, 0.9552238805970149, 0.950363196125908, 0.930622009569378, 0.9479418886198547, 0.9562043795620438, 0.9514792899408284, 0.954653937947494, 0.9515738498789347, 0.9498806682577565, 0.959902794653706, 0.9435294117647058, 0.9457177322074789, 0.9519807923169268, 0.9428223844282239, 0.9326923076923077, 0.9204406364749081, 0.9436964504283966, 0.93681652490887, 0.95, 0.9435096153846154, 0.9517326732673267, 0.9486552567237164, 0.947109471094711, 0.950363196125908, 0.9348612786489746, 0.949569495694957, 0.931980906921241, 0.9462759462759462, 0.9445812807881774, 0.9508599508599509, 0.9466666666666667, 0.9445812807881774, 0.9446540880503145, 0.9464068209500609, 0.9454094292803971, 0.9500624219725343, 0.9408284023668639, 0.944801026957638, 0.9571788413098237, 0.9345454545454546, 0.9399249061326659, 0.9503030303030303, 0.9481481481481482, 0.9495798319327731, 0.9438902743142145, 0.9412484700122399, 0.939540507859734, 0.953714981729598, 0.9469135802469136, 0.9476885644768857, 0.9414519906323185, 0.925700365408039, 0.9523809523809523, 0.9465174129353234, 0.9373433583959899, 0.9343065693430657, 0.9350811485642946, 0.921968787515006, 0.9381953028430161, 0.9582822085889571, 0.9519704433497537, 0.957002457002457, 0.9432799013563502, 0.9534313725490197, 0.943728018757327, 0.9463171036204744, 0.9502427184466019, 0.938347718865598, 0.9511323003575686, 0.9438470728793309, 0.9353293413173652, 0.9514218009478673, 0.9493827160493827, 0.9416767922235723, 0.9470443349753694, 0.9536340852130326, 0.9481302774427021, 0.9510050251256281, 0.9428223844282239, 0.93125, 0.9373433583959899, 0.9453416149068323, 0.9417360285374554, 0.9586877278250304, 0.9463414634146341, 0.951073985680191, 0.9266272189349113, 0.9442424242424242, 0.9372738238841978, 0.9413919413919414, 0.9493975903614458, 0.9515938606847698, 0.9516324062877872, 0.9406565656565656, 0.9520295202952029, 0.9425149700598803, 0.949874686716792, 0.9416243654822335, 0.9429611650485437, 0.9621489621489622, 0.9506024096385542, 0.9352078239608802, 0.9411764705882353, 0.9300873907615481, 0.948019801980198, 0.9431396786155748, 0.939622641509434, 0.9536585365853658, 0.9483173076923077, 0.9562575941676792, 0.9411764705882353, 0.9313358302122348, 0.9539951573849879, 0.9451887941534713, 0.9492753623188406, 0.9381313131313131, 0.9404617253948967, 0.9507389162561576, 0.9511599511599511, 0.9513715710723192, 0.9424280350438048, 0.9464508094645081, 0.9563543003851092, 0.9517326732673267, 0.942643391521197, 0.9433734939759036, 0.9390243902439024, 0.9520958083832335, 0.9462102689486552, 0.9522012578616352, 0.959409594095941, 0.9525547445255474, 0.9553140096618358, 0.9416666666666667, 0.9412484700122399, 0.9485924112607099, 0.9429611650485437, 0.9373493975903614, 0.9452054794520548, 0.9350490196078431, 0.9344660194174758, 0.9442467378410438, 0.9444444444444444, 0.9475566150178785, 0.9474940334128878, 0.9510664993726474, 0.9443037974683545, 0.9390386869871044, 0.9486552567237164, 0.9498207885304659, 0.925, 0.940389294403893, 0.9448441247002398, 0.9432098765432099, 0.9308641975308642, 0.942652329749104, 0.9463414634146341, 0.948051948051948, 0.9457831325301205, 0.9390243902439024, 0.9645476772616137, 0.9463414634146341, 0.9532019704433498, 0.9418181818181818, 0.939540507859734, 0.9442424242424242, 0.9586466165413534, 0.9440389294403893, 0.9454329774614472, 0.945, 0.9426129426129426, 0.933997509339975, 0.9293544457978076, 0.9546012269938651, 0.9345679012345679, 0.9556354916067147, 0.9318181818181818, 0.9299287410926366, 0.9459459459459459, 0.9527272727272728, 0.9415422885572139, 0.9514925373134329, 0.9602977667493796, 0.9504950495049505, 0.9424724602203183, 0.9517923362175525, 0.9447852760736196, 0.9463840399002493, 0.9417596034696406, 0.9473684210526315, 0.933253873659118, 0.9425837320574163, 0.9459783913565426, 0.9527272727272728, 0.9473039215686274, 0.9452554744525548, 0.930622009569378, 0.943558282208589, 0.9512195121951219, 0.9417596034696406, 0.9400244798041616, 0.9500624219725343, 0.9405940594059405, 0.9464720194647201, 0.9439809296781884, 0.9473039215686274, 0.9416873449131513, 0.9224030037546934, 0.9360393603936039, 0.9524390243902439, 0.9447779111644657, 0.9492574257425742, 0.9325153374233128, 0.9422835633626098, 0.9482551143200962, 0.9443069306930693, 0.9405339805825242, 0.9426129426129426, 0.9454545454545454, 0.950661853188929, 0.9476248477466505, 0.9186785260482846, 0.9465558194774347, 0.9492140266021766, 0.951188986232791, 0.943577430972389, 0.946987951807229, 0.9467821782178217, 0.9425, 0.9357142857142857, 0.9357575757575758, 0.9455696202531646, 0.9357142857142857, 0.9570552147239264, 0.93625, 0.9466824644549763, 0.9635220125786164, 0.93173198482933, 0.9466666666666667, 0.9481132075471698, 0.9498164014687882, 0.9368686868686869, 0.9437652811735942, 0.936969696969697, 0.9468354430379747, 0.9577804583835947, 0.9419753086419753, 0.9620563035495716, 0.942189421894219, 0.9507389162561576, 0.9277708592777086, 0.9542168674698795, 0.941320293398533, 0.9252450980392157, 0.9319562575941677, 0.9420289855072463, 0.9395807644882861, 0.957124842370744, 0.947109471094711, 0.9528415961305925, 0.941320293398533, 0.9479289940828403, 0.9393564356435643, 0.9502427184466019, 0.9580764488286067, 0.9437939110070258, 0.9383033419023136, 0.9453416149068323, 0.9420468557336621, 0.94026284348865, 0.9503105590062112, 0.9504337050805453, 0.9455864570737605, 0.9478155339805825, 0.9436274509803921, 0.957286432160804, 0.9394673123486683, 0.9454770755885997, 0.9368029739776952, 0.941747572815534, 0.9392857142857143, 0.9496314496314496, 0.9311594202898551, 0.9379562043795621, 0.93681652490887, 0.9463840399002493, 0.9467312348668281, 0.9385194479297365, 0.9402439024390243, 0.9389788293897883, 0.9519906323185011, 0.9477521263669502, 0.941031941031941, 0.9425149700598803, 0.9479553903345725, 0.9457177322074789, 0.9391727493917275, 0.9389952153110048, 0.9379562043795621, 0.9495073891625616, 0.9525, 0.9468599033816425, 0.9523809523809523, 0.937046004842615, 0.9420654911838791, 0.9523809523809523, 0.9566294919454771, 0.9408212560386473, 0.951188986232791, 0.9448441247002398, 0.945679012345679, 0.9394673123486683, 0.9441069258809235, 0.9390986601705238, 0.9368811881188119, 0.9451530612244898, 0.9413833528722158, 0.9511599511599511, 0.9348612786489746, 0.9545454545454546, 0.947560975609756, 0.9359605911330049, 0.9463647199046484, 0.925609756097561, 0.9515366430260047, 0.9502487562189055, 0.9329479768786128, 0.9468354430379747, 0.9454094292803971, 0.9415422885572139, 0.9433497536945813, 0.942080378250591, 0.9517490952955368, 0.9540816326530612, 0.9552599758162031, 0.9408805031446541, 0.9488859764089121, 0.9433734939759036, 0.9462759462759462, 0.9473039215686274, 0.9443069306930693, 0.9371221281741233, 0.9416767922235723, 0.9526123936816525, 0.9330143540669856, 0.9436964504283966, 0.9470517448856799, 0.9503030303030303, 0.9411764705882353, 0.9528985507246377, 0.9479418886198547, 0.946236559139785, 0.9503030303030303, 0.9406060606060606, 0.9533742331288344, 0.9526699029126213, 0.9399509803921569, 0.9359129383313181, 0.9496932515337423, 0.9207317073170732, 0.9280487804878049, 0.9522643818849449, 0.9546599496221663, 0.9485924112607099, 0.9401709401709402, 0.95, 0.9354838709677419, 0.9509803921568627, 0.9521472392638037, 0.9523241954707986, 0.9446494464944649, 0.9588377723970944, 0.9364508393285371, 0.9476248477466505, 0.9488428745432399, 0.9472329472329473, 0.932349323493235, 0.9405204460966543, 0.9500609013398295, 0.944640753828033, 0.9445843828715366, 0.9563164108618654, 0.9452054794520548, 0.9583333333333334, 0.9527363184079602, 0.9426129426129426, 0.9425427872860636, 0.9352869352869353, 0.948780487804878, 0.939622641509434, 0.9446450060168472, 0.9456118665018541, 0.9480048367593712, 0.9346485819975339, 0.9495586380832283, 0.950661853188929, 0.955, 0.9373493975903614, 0.944920440636475, 0.9496932515337423, 0.948905109489051, 0.9476961394769614, 0.9286563614744352, 0.9331683168316832, 0.9359605911330049, 0.9533742331288344, 0.9482758620689655, 0.9474313022700119, 0.9346485819975339, 0.9392059553349876, 0.9399249061326659, 0.9477521263669502, 0.9559748427672956, 0.9465838509316771, 0.9547123623011016, 0.9480676328502415, 0.9511599511599511, 0.9395807644882861, 0.9526123936816525, 0.9230769230769231, 0.941034897713598, 0.9491525423728814, 0.9538077403245943, 0.9582309582309583, 0.9470517448856799, 0.9359605911330049, 0.9376558603491272, 0.9515738498789347, 0.9447174447174447, 0.9541616405307599, 0.9431818181818182, 0.9563636363636364, 0.9490049751243781, 0.95, 0.9479418886198547, 0.9565217391304348, 0.9344660194174758, 0.9481302774427021, 0.9396863691194209, 0.9426129426129426, 0.9447174447174447, 0.9390547263681592, 0.951188986232791, 0.9451371571072319, 0.9349693251533743, 0.9504337050805453, 0.9499374217772215, 0.9458128078817734, 0.9365269461077844, 0.9387254901960784, 0.9498806682577565, 0.9519112207151664, 0.9372693726937269, 0.9474969474969475, 0.9543269230769231, 0.9469135802469136, 0.9540372670807453, 0.9444444444444444, 0.9375, 0.937888198757764, 0.9443099273607748, 0.9487179487179487, 0.9478787878787879, 0.9452054794520548, 0.9283065512978986, 0.9152334152334153, 0.9481481481481482, 0.9532374100719424, 0.942189421894219, 0.9265944645006017, 0.9502427184466019, 0.9497549019607843, 0.9386503067484663, 0.9210526315789473, 0.9451073985680191, 0.9648484848484848, 0.9535452322738386, 0.9420468557336621, 0.9538274605103281, 0.9416873449131513, 0.9451219512195121, 0.9367396593673966, 0.947109471094711, 0.94026284348865, 0.9449877750611247, 0.9339045287637698, 0.9418886198547215, 0.9618696186961869, 0.9306431273644389, 0.9512195121951219, 0.9466019417475728, 0.9426129426129426, 0.9398496240601504, 0.945321992709599, 0.9566294919454771, 0.9544334975369458, 0.9347290640394089, 0.9612121212121212, 0.9556354916067147, 0.9429250891795482, 0.932285368802902, 0.9641975308641976, 0.9424724602203183, 0.943069306930693, 0.9443099273607748, 0.9405469678953626, 0.9568862275449102, 0.9389221556886228, 0.946987951807229, 0.9509803921568627, 0.9407407407407408, 0.9579326923076923, 0.9279811097992916, 0.9399509803921569, 0.957972805933251, 0.9636363636363636, 0.940677966101695, 0.9498164014687882, 0.9392812887236679, 0.9501246882793017, 0.9392133492252682, 0.9436274509803921, 0.9421182266009852, 0.9438339438339438, 0.9486552567237164, 0.9416058394160584, 0.9362745098039216, 0.9390986601705238, 0.943558282208589, 0.929678188319428, 0.9381188118811881, 0.9363207547169812, 0.9312883435582822, 0.9549228944246738, 0.9508393285371702, 0.9482120838471023, 0.9493975903614458, 0.9548229548229549, 0.9485294117647058, 0.9568434032059187, 0.9491945477075588, 0.9452554744525548, 0.9493029150823827, 0.9530456852791879, 0.946236559139785, 0.9497005988023952, 0.9462759462759462, 0.9527363184079602, 0.953939393939394, 0.9371980676328503, 0.9584859584859585, 0.9399509803921569, 0.9490909090909091, 0.9443099273607748, 0.9443099273607748, 0.9459459459459459, 0.9467680608365019, 0.938347718865598, 0.950186799501868, 0.9313725490196079, 0.9556354916067147, 0.9230769230769231, 0.9466019417475728, 0.9423312883435583, 0.9359129383313181, 0.9682926829268292, 0.942643391521197, 0.9406175771971497, 0.9451219512195121, 0.943577430972389, 0.9518072289156626, 0.9412484700122399, 0.9347826086956522, 0.9383313180169287, 0.9502427184466019, 0.9496402877697842, 0.9547169811320755, 0.9448621553884712, 0.9504232164449818, 0.940959409594096, 0.9521472392638037, 0.9468599033816425, 0.9469240048250904, 0.9520958083832335, 0.9398496240601504, 0.9468479604449939]\n",
            "sp: [0.9439707673568819, 0.9519807923169268, 0.9459459459459459, 0.933253873659118, 0.9392466585662211, 0.9644607843137255, 0.9507803121248499, 0.9378806333739342, 0.9537712895377128, 0.9481302774427021, 0.9527363184079602, 0.9517326732673267, 0.964329643296433, 0.9409681227863046, 0.9444444444444444, 0.9602409638554217, 0.9445812807881774, 0.95125, 0.9427527405602923, 0.9421686746987952, 0.9408866995073891, 0.9548229548229549, 0.9601990049751243, 0.9548693586698337, 0.9337423312883436, 0.951530612244898, 0.9419191919191919, 0.9475032010243278, 0.9550970873786407, 0.944792973651192, 0.95, 0.921760391198044, 0.9419588875453446, 0.9398315282791817, 0.9505562422744128, 0.9423312883435583, 0.9493201483312732, 0.958128078817734, 0.9645476772616137, 0.9375764993880049, 0.9457459926017263, 0.9459459459459459, 0.9532374100719424, 0.9379474940334129, 0.9445129469790382, 0.9471744471744472, 0.9304029304029304, 0.9462102689486552, 0.9538834951456311, 0.9408926417370326, 0.9433734939759036, 0.944920440636475, 0.9551122194513716, 0.9476961394769614, 0.9471788715486195, 0.9322459222082811, 0.9500594530321046, 0.9359430604982206, 0.9502427184466019, 0.9463414634146341, 0.9494584837545126, 0.9571938168846611, 0.9411042944785276, 0.9341317365269461, 0.9483793517406963, 0.9526123936816525, 0.9356376638855781, 0.957972805933251, 0.9516908212560387, 0.9464068209500609, 0.9583333333333334, 0.9307411907654921, 0.931077694235589, 0.9458483754512635, 0.9544334975369458, 0.951073985680191, 0.9300361881785284, 0.9346793349168646, 0.9566294919454771, 0.9229813664596274, 0.9405204460966543, 0.94625, 0.9573170731707317, 0.94026284348865, 0.947560975609756, 0.9527272727272728, 0.9302325581395349, 0.9484029484029484, 0.9408866995073891, 0.9536019536019537, 0.9333333333333333, 0.9347290640394089, 0.9333333333333333, 0.9471744471744472, 0.9458794587945879, 0.96375, 0.9478787878787879, 0.9445114595898673, 0.9390386869871044, 0.9582836710369488, 0.9520958083832335, 0.9588528678304239, 0.9500609013398295, 0.9443772672309553, 0.9472361809045227, 0.9547123623011016, 0.9464922711058263, 0.9411042944785276, 0.948905109489051, 0.9441141498216409, 0.944920440636475, 0.9554753309265944, 0.9311594202898551, 0.9286592865928659, 0.9539007092198581, 0.9523212045169385, 0.928136419001218, 0.9428223844282239, 0.9552795031055901, 0.9476248477466505, 0.9358024691358025, 0.9476885644768857, 0.9243697478991597, 0.9389788293897883, 0.9598445595854922, 0.9368029739776952, 0.9419035846724351, 0.940389294403893, 0.9271844660194175, 0.9385749385749386, 0.9377431906614786, 0.9508982035928144, 0.9538274605103281, 0.9434194341943419, 0.9447174447174447, 0.940677966101695, 0.9524940617577197, 0.9459134615384616, 0.947560975609756, 0.9418457648546145, 0.9526011560693641, 0.9466666666666667, 0.9422604422604423, 0.9475, 0.925700365408039, 0.9517326732673267, 0.9481132075471698, 0.9525593008739076, 0.9315403422982885, 0.9526699029126213, 0.9478260869565217, 0.9482120838471023, 0.9376498800959233, 0.9482758620689655, 0.946047678795483, 0.9319041614123581, 0.9392812887236679, 0.939540507859734, 0.9409638554216867, 0.9521410579345088, 0.9415971394517283, 0.9461444308445532, 0.9537712895377128, 0.9396135265700483, 0.9427527405602923, 0.9411764705882353, 0.936969696969697, 0.9330900243309003, 0.9442379182156134, 0.9356060606060606, 0.9528415961305925, 0.9483173076923077, 0.9232643118148599, 0.9444444444444444, 0.9520807061790668, 0.9493365500603136, 0.938949938949939, 0.957983193277311, 0.9431680773881499, 0.9504830917874396, 0.943609022556391, 0.947560975609756, 0.939622641509434, 0.9518518518518518, 0.9464720194647201, 0.9583333333333334, 0.9416873449131513, 0.9482758620689655, 0.9443742098609356, 0.9398034398034398, 0.9396135265700483, 0.950363196125908, 0.9622641509433962, 0.950530035335689, 0.9411042944785276, 0.9504232164449818, 0.9443786982248521, 0.9532374100719424, 0.943558282208589, 0.9416058394160584, 0.9393203883495146, 0.9425837320574163, 0.9423312883435583, 0.94377990430622, 0.9433734939759036, 0.9534883720930233, 0.9432098765432099, 0.9504232164449818, 0.9467455621301775, 0.9476813317479191, 0.9333333333333333, 0.9447236180904522, 0.9555555555555556, 0.9503030303030303, 0.9476885644768857, 0.9470365699873896, 0.9436274509803921, 0.9451728247914184, 0.9410288582183187, 0.9488139825218477, 0.9380315917375456, 0.9532828282828283, 0.9511599511599511, 0.9501246882793017, 0.9331713244228432, 0.932061978545888, 0.9372647427854455, 0.9423769507803121, 0.9397163120567376, 0.9529553679131484, 0.9466327827191868, 0.9469135802469136, 0.9476885644768857, 0.9395061728395062, 0.9544334975369458, 0.9568965517241379, 0.9497549019607843, 0.943069306930693, 0.9389788293897883, 0.9416342412451362, 0.9434889434889435, 0.9512195121951219, 0.9505428226779252, 0.9442379182156134, 0.9418181818181818, 0.9364303178484108, 0.948626045400239, 0.9438202247191011, 0.9371221281741233, 0.9452054794520548, 0.934040047114252, 0.9494451294697904, 0.9434416365824309, 0.9329268292682927, 0.9507389162561576, 0.94375, 0.9404052443384983, 0.9364303178484108, 0.9399038461538461, 0.9452554744525548, 0.9382566585956417, 0.9393203883495146, 0.9546568627450981, 0.9348093480934809, 0.9338327091136079, 0.9517923362175525, 0.9288343558282208, 0.9415971394517283, 0.9400479616306955, 0.9445129469790382, 0.9502427184466019, 0.9504232164449818, 0.9444444444444444, 0.9496402877697842, 0.945054945054945, 0.956949569495695, 0.9351740696278511, 0.9563106796116505, 0.9349881796690307, 0.959409594095941, 0.9427148194271482, 0.9405339805825242, 0.9598051157125457, 0.9544364508393285, 0.9456118665018541, 0.9556650246305419, 0.9430604982206405, 0.9427480916030534, 0.9442424242424242, 0.9419588875453446, 0.9430604982206405, 0.9448529411764706, 0.9553140096618358, 0.9479905437352246, 0.9504232164449818, 0.9515738498789347, 0.9415274463007159, 0.95, 0.9495192307692307, 0.9496932515337423, 0.9382566585956417, 0.9478908188585607, 0.9436964504283966, 0.9317904993909866, 0.941320293398533, 0.9365269461077844, 0.943127962085308, 0.9459119496855346, 0.9246658566221142, 0.9407313997477932, 0.9343434343434344, 0.9480840543881335, 0.9426829268292682, 0.9493975903614458, 0.9342723004694836, 0.9443037974683545, 0.9400244798041616, 0.944640753828033, 0.9455006337135615, 0.9477521263669502, 0.9502617801047121, 0.9395807644882861, 0.9418742586002372, 0.9372693726937269, 0.9338942307692307, 0.9534883720930233, 0.9413173652694611, 0.9381818181818182, 0.9474969474969475, 0.9433734939759036, 0.9556354916067147, 0.9490291262135923, 0.9452554744525548, 0.9526699029126213, 0.9331742243436754, 0.9579207920792079, 0.95079950799508, 0.960431654676259, 0.9505562422744128, 0.9461444308445532, 0.9344660194174758, 0.9540636042402827, 0.9432367149758454, 0.946078431372549, 0.9449760765550239, 0.9459459459459459, 0.930778739184178, 0.9445129469790382, 0.9408926417370326, 0.9469026548672567, 0.954653937947494, 0.9363525091799265, 0.941320293398533, 0.9559902200488998, 0.9499404052443385, 0.9473039215686274, 0.9425149700598803, 0.9419035846724351, 0.9543269230769231, 0.9477521263669502, 0.9453681710213777, 0.9426829268292682, 0.951397326852977, 0.9458438287153652, 0.9464720194647201, 0.9412484700122399, 0.9418181818181818, 0.950530035335689, 0.946236559139785, 0.9390862944162437, 0.9564691656590084, 0.9385342789598109, 0.940959409594096, 0.9468354430379747, 0.9375764993880049, 0.9525, 0.9609467455621302, 0.9438202247191011, 0.9431680773881499, 0.9465174129353234, 0.9505562422744128, 0.939877300613497, 0.93375, 0.9405204460966543, 0.9460154241645244, 0.9519343493552169, 0.9461538461538461, 0.9582822085889571, 0.9459783913565426, 0.941320293398533, 0.937192118226601, 0.9422110552763819, 0.9297163995067818, 0.9335748792270532, 0.9405339805825242, 0.9486552567237164, 0.9495192307692307, 0.9369592088998764, 0.9506781750924784, 0.944233206590621, 0.9464922711058263, 0.9502427184466019, 0.9418181818181818, 0.9495192307692307, 0.9504716981132075, 0.9500609013398295, 0.9577464788732394, 0.9509433962264151, 0.9467956469165659, 0.9459459459459459, 0.9466327827191868, 0.9405940594059405, 0.9281210592686002, 0.9526184538653366, 0.9552238805970149, 0.950363196125908, 0.930622009569378, 0.9479418886198547, 0.9562043795620438, 0.9514792899408284, 0.954653937947494, 0.9515738498789347, 0.9498806682577565, 0.959902794653706, 0.9435294117647058, 0.9457177322074789, 0.9519807923169268, 0.9428223844282239, 0.9326923076923077, 0.9204406364749081, 0.9436964504283966, 0.93681652490887, 0.95, 0.9435096153846154, 0.9517326732673267, 0.9486552567237164, 0.947109471094711, 0.950363196125908, 0.9348612786489746, 0.949569495694957, 0.931980906921241, 0.9462759462759462, 0.9445812807881774, 0.9508599508599509, 0.9466666666666667, 0.9445812807881774, 0.9446540880503145, 0.9464068209500609, 0.9454094292803971, 0.9500624219725343, 0.9408284023668639, 0.944801026957638, 0.9571788413098237, 0.9345454545454546, 0.9399249061326659, 0.9503030303030303, 0.9481481481481482, 0.9495798319327731, 0.9438902743142145, 0.9412484700122399, 0.939540507859734, 0.953714981729598, 0.9469135802469136, 0.9476885644768857, 0.9414519906323185, 0.925700365408039, 0.9523809523809523, 0.9465174129353234, 0.9373433583959899, 0.9343065693430657, 0.9350811485642946, 0.921968787515006, 0.9381953028430161, 0.9582822085889571, 0.9519704433497537, 0.957002457002457, 0.9432799013563502, 0.9534313725490197, 0.943728018757327, 0.9463171036204744, 0.9502427184466019, 0.938347718865598, 0.9511323003575686, 0.9438470728793309, 0.9353293413173652, 0.9514218009478673, 0.9493827160493827, 0.9416767922235723, 0.9470443349753694, 0.9536340852130326, 0.9481302774427021, 0.9510050251256281, 0.9428223844282239, 0.93125, 0.9373433583959899, 0.9453416149068323, 0.9417360285374554, 0.9586877278250304, 0.9463414634146341, 0.951073985680191, 0.9266272189349113, 0.9442424242424242, 0.9372738238841978, 0.9413919413919414, 0.9493975903614458, 0.9515938606847698, 0.9516324062877872, 0.9406565656565656, 0.9520295202952029, 0.9425149700598803, 0.949874686716792, 0.9416243654822335, 0.9429611650485437, 0.9621489621489622, 0.9506024096385542, 0.9352078239608802, 0.9411764705882353, 0.9300873907615481, 0.948019801980198, 0.9431396786155748, 0.939622641509434, 0.9536585365853658, 0.9483173076923077, 0.9562575941676792, 0.9411764705882353, 0.9313358302122348, 0.9539951573849879, 0.9451887941534713, 0.9492753623188406, 0.9381313131313131, 0.9404617253948967, 0.9507389162561576, 0.9511599511599511, 0.9513715710723192, 0.9424280350438048, 0.9464508094645081, 0.9563543003851092, 0.9517326732673267, 0.942643391521197, 0.9433734939759036, 0.9390243902439024, 0.9520958083832335, 0.9462102689486552, 0.9522012578616352, 0.959409594095941, 0.9525547445255474, 0.9553140096618358, 0.9416666666666667, 0.9412484700122399, 0.9485924112607099, 0.9429611650485437, 0.9373493975903614, 0.9452054794520548, 0.9350490196078431, 0.9344660194174758, 0.9442467378410438, 0.9444444444444444, 0.9475566150178785, 0.9474940334128878, 0.9510664993726474, 0.9443037974683545, 0.9390386869871044, 0.9486552567237164, 0.9498207885304659, 0.925, 0.940389294403893, 0.9448441247002398, 0.9432098765432099, 0.9308641975308642, 0.942652329749104, 0.9463414634146341, 0.948051948051948, 0.9457831325301205, 0.9390243902439024, 0.9645476772616137, 0.9463414634146341, 0.9532019704433498, 0.9418181818181818, 0.939540507859734, 0.9442424242424242, 0.9586466165413534, 0.9440389294403893, 0.9454329774614472, 0.945, 0.9426129426129426, 0.933997509339975, 0.9293544457978076, 0.9546012269938651, 0.9345679012345679, 0.9556354916067147, 0.9318181818181818, 0.9299287410926366, 0.9459459459459459, 0.9527272727272728, 0.9415422885572139, 0.9514925373134329, 0.9602977667493796, 0.9504950495049505, 0.9424724602203183, 0.9517923362175525, 0.9447852760736196, 0.9463840399002493, 0.9417596034696406, 0.9473684210526315, 0.933253873659118, 0.9425837320574163, 0.9459783913565426, 0.9527272727272728, 0.9473039215686274, 0.9452554744525548, 0.930622009569378, 0.943558282208589, 0.9512195121951219, 0.9417596034696406, 0.9400244798041616, 0.9500624219725343, 0.9405940594059405, 0.9464720194647201, 0.9439809296781884, 0.9473039215686274, 0.9416873449131513, 0.9224030037546934, 0.9360393603936039, 0.9524390243902439, 0.9447779111644657, 0.9492574257425742, 0.9325153374233128, 0.9422835633626098, 0.9482551143200962, 0.9443069306930693, 0.9405339805825242, 0.9426129426129426, 0.9454545454545454, 0.950661853188929, 0.9476248477466505, 0.9186785260482846, 0.9465558194774347, 0.9492140266021766, 0.951188986232791, 0.943577430972389, 0.946987951807229, 0.9467821782178217, 0.9425, 0.9357142857142857, 0.9357575757575758, 0.9455696202531646, 0.9357142857142857, 0.9570552147239264, 0.93625, 0.9466824644549763, 0.9635220125786164, 0.93173198482933, 0.9466666666666667, 0.9481132075471698, 0.9498164014687882, 0.9368686868686869, 0.9437652811735942, 0.936969696969697, 0.9468354430379747, 0.9577804583835947, 0.9419753086419753, 0.9620563035495716, 0.942189421894219, 0.9507389162561576, 0.9277708592777086, 0.9542168674698795, 0.941320293398533, 0.9252450980392157, 0.9319562575941677, 0.9420289855072463, 0.9395807644882861, 0.957124842370744, 0.947109471094711, 0.9528415961305925, 0.941320293398533, 0.9479289940828403, 0.9393564356435643, 0.9502427184466019, 0.9580764488286067, 0.9437939110070258, 0.9383033419023136, 0.9453416149068323, 0.9420468557336621, 0.94026284348865, 0.9503105590062112, 0.9504337050805453, 0.9455864570737605, 0.9478155339805825, 0.9436274509803921, 0.957286432160804, 0.9394673123486683, 0.9454770755885997, 0.9368029739776952, 0.941747572815534, 0.9392857142857143, 0.9496314496314496, 0.9311594202898551, 0.9379562043795621, 0.93681652490887, 0.9463840399002493, 0.9467312348668281, 0.9385194479297365, 0.9402439024390243, 0.9389788293897883, 0.9519906323185011, 0.9477521263669502, 0.941031941031941, 0.9425149700598803, 0.9479553903345725, 0.9457177322074789, 0.9391727493917275, 0.9389952153110048, 0.9379562043795621, 0.9495073891625616, 0.9525, 0.9468599033816425, 0.9523809523809523, 0.937046004842615, 0.9420654911838791, 0.9523809523809523, 0.9566294919454771, 0.9408212560386473, 0.951188986232791, 0.9448441247002398, 0.945679012345679, 0.9394673123486683, 0.9441069258809235, 0.9390986601705238, 0.9368811881188119, 0.9451530612244898, 0.9413833528722158, 0.9511599511599511, 0.9348612786489746, 0.9545454545454546, 0.947560975609756, 0.9359605911330049, 0.9463647199046484, 0.925609756097561, 0.9515366430260047, 0.9502487562189055, 0.9329479768786128, 0.9468354430379747, 0.9454094292803971, 0.9415422885572139, 0.9433497536945813, 0.942080378250591, 0.9517490952955368, 0.9540816326530612, 0.9552599758162031, 0.9408805031446541, 0.9488859764089121, 0.9433734939759036, 0.9462759462759462, 0.9473039215686274, 0.9443069306930693, 0.9371221281741233, 0.9416767922235723, 0.9526123936816525, 0.9330143540669856, 0.9436964504283966, 0.9470517448856799, 0.9503030303030303, 0.9411764705882353, 0.9528985507246377, 0.9479418886198547, 0.946236559139785, 0.9503030303030303, 0.9406060606060606, 0.9533742331288344, 0.9526699029126213, 0.9399509803921569, 0.9359129383313181, 0.9496932515337423, 0.9207317073170732, 0.9280487804878049, 0.9522643818849449, 0.9546599496221663, 0.9485924112607099, 0.9401709401709402, 0.95, 0.9354838709677419, 0.9509803921568627, 0.9521472392638037, 0.9523241954707986, 0.9446494464944649, 0.9588377723970944, 0.9364508393285371, 0.9476248477466505, 0.9488428745432399, 0.9472329472329473, 0.932349323493235, 0.9405204460966543, 0.9500609013398295, 0.944640753828033, 0.9445843828715366, 0.9563164108618654, 0.9452054794520548, 0.9583333333333334, 0.9527363184079602, 0.9426129426129426, 0.9425427872860636, 0.9352869352869353, 0.948780487804878, 0.939622641509434, 0.9446450060168472, 0.9456118665018541, 0.9480048367593712, 0.9346485819975339, 0.9495586380832283, 0.950661853188929, 0.955, 0.9373493975903614, 0.944920440636475, 0.9496932515337423, 0.948905109489051, 0.9476961394769614, 0.9286563614744352, 0.9331683168316832, 0.9359605911330049, 0.9533742331288344, 0.9482758620689655, 0.9474313022700119, 0.9346485819975339, 0.9392059553349876, 0.9399249061326659, 0.9477521263669502, 0.9559748427672956, 0.9465838509316771, 0.9547123623011016, 0.9480676328502415, 0.9511599511599511, 0.9395807644882861, 0.9526123936816525, 0.9230769230769231, 0.941034897713598, 0.9491525423728814, 0.9538077403245943, 0.9582309582309583, 0.9470517448856799, 0.9359605911330049, 0.9376558603491272, 0.9515738498789347, 0.9447174447174447, 0.9541616405307599, 0.9431818181818182, 0.9563636363636364, 0.9490049751243781, 0.95, 0.9479418886198547, 0.9565217391304348, 0.9344660194174758, 0.9481302774427021, 0.9396863691194209, 0.9426129426129426, 0.9447174447174447, 0.9390547263681592, 0.951188986232791, 0.9451371571072319, 0.9349693251533743, 0.9504337050805453, 0.9499374217772215, 0.9458128078817734, 0.9365269461077844, 0.9387254901960784, 0.9498806682577565, 0.9519112207151664, 0.9372693726937269, 0.9474969474969475, 0.9543269230769231, 0.9469135802469136, 0.9540372670807453, 0.9444444444444444, 0.9375, 0.937888198757764, 0.9443099273607748, 0.9487179487179487, 0.9478787878787879, 0.9452054794520548, 0.9283065512978986, 0.9152334152334153, 0.9481481481481482, 0.9532374100719424, 0.942189421894219, 0.9265944645006017, 0.9502427184466019, 0.9497549019607843, 0.9386503067484663, 0.9210526315789473, 0.9451073985680191, 0.9648484848484848, 0.9535452322738386, 0.9420468557336621, 0.9538274605103281, 0.9416873449131513, 0.9451219512195121, 0.9367396593673966, 0.947109471094711, 0.94026284348865, 0.9449877750611247, 0.9339045287637698, 0.9418886198547215, 0.9618696186961869, 0.9306431273644389, 0.9512195121951219, 0.9466019417475728, 0.9426129426129426, 0.9398496240601504, 0.945321992709599, 0.9566294919454771, 0.9544334975369458, 0.9347290640394089, 0.9612121212121212, 0.9556354916067147, 0.9429250891795482, 0.932285368802902, 0.9641975308641976, 0.9424724602203183, 0.943069306930693, 0.9443099273607748, 0.9405469678953626, 0.9568862275449102, 0.9389221556886228, 0.946987951807229, 0.9509803921568627, 0.9407407407407408, 0.9579326923076923, 0.9279811097992916, 0.9399509803921569, 0.957972805933251, 0.9636363636363636, 0.940677966101695, 0.9498164014687882, 0.9392812887236679, 0.9501246882793017, 0.9392133492252682, 0.9436274509803921, 0.9421182266009852, 0.9438339438339438, 0.9486552567237164, 0.9416058394160584, 0.9362745098039216, 0.9390986601705238, 0.943558282208589, 0.929678188319428, 0.9381188118811881, 0.9363207547169812, 0.9312883435582822, 0.9549228944246738, 0.9508393285371702, 0.9482120838471023, 0.9493975903614458, 0.9548229548229549, 0.9485294117647058, 0.9568434032059187, 0.9491945477075588, 0.9452554744525548, 0.9493029150823827, 0.9530456852791879, 0.946236559139785, 0.9497005988023952, 0.9462759462759462, 0.9527363184079602, 0.953939393939394, 0.9371980676328503, 0.9584859584859585, 0.9399509803921569, 0.9490909090909091, 0.9443099273607748, 0.9443099273607748, 0.9459459459459459, 0.9467680608365019, 0.938347718865598, 0.950186799501868, 0.9313725490196079, 0.9556354916067147, 0.9230769230769231, 0.9466019417475728, 0.9423312883435583, 0.9359129383313181, 0.9682926829268292, 0.942643391521197, 0.9406175771971497, 0.9451219512195121, 0.943577430972389, 0.9518072289156626, 0.9412484700122399, 0.9347826086956522, 0.9383313180169287, 0.9502427184466019, 0.9496402877697842, 0.9547169811320755, 0.9448621553884712, 0.9504232164449818, 0.940959409594096, 0.9521472392638037, 0.9468599033816425, 0.9469240048250904, 0.9520958083832335, 0.9398496240601504, 0.9468479604449939]\n",
            "sd_acc: 0.008563681917266554\n",
            "sd_f1: 0.006204512849919482\n",
            "sd_mcc: 0.021243946185022393\n",
            "sd_sn: 0.007803315891296779\n",
            "sd_sp: 0.007803315891296779\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.79      0.82       331\n",
            "           1       0.92      0.94      0.93       818\n",
            "\n",
            "    accuracy                           0.90      1149\n",
            "   macro avg       0.89      0.87      0.88      1149\n",
            "weighted avg       0.90      0.90      0.90      1149\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.008563681917266554, 0.021243946185022393, 0.006204512849919482)"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "error_rate(testing_labels, predicted_lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3925tQGbaKnK",
        "outputId": "0fcac878-4bcc-4709-c080-1a49b037feb0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f1f84740100>]"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGdCAYAAADAAnMpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAygElEQVR4nO3de2xU17n38d8eZmwMxJiLXRMcbIwvBDA3k7QinBaCmriN3woSiJJUao9oUahylD96mp6UKirVAalOqzZRiFRVVE1oCwHRA+UmmjQhagtIaSEBzM1cG8AY7JixA9iDx7PeP8wMNh6DZzyz91y+HwnFs71n5vGTCf5lrbXXtowxRgAAAA5xOV0AAABIb4QRAADgKMIIAABwFGEEAAA4ijACAAAcRRgBAACOIowAAABHEUYAAICjCCMAAMBRhBEAAOAot9MFROLq1avy+/0xfc3c3Fw1NjbG9DXRG322D722B322B322R7z67Ha7NWLEiHufF/N3jiO/36+Ojo6YvZ5lWaHX5RY98UOf7UOv7UGf7UGf7ZEIfWaaBgAAOIowAgAAHEUYAQAAjiKMAAAARxFGAACAowgjAADAUYQRAADgKMIIAABwFGEEAAA4KuIdWI8ePaqtW7fq7Nmzunr1qn7wgx/o4Ycfvutzjhw5orVr1+r8+fMaNWqUnnrqKc2dOzfamgEAQAqJeGTE5/OpqKhI3/nOd/p1/pUrV/Szn/1MkydP1quvvqonnnhCv/71r/XJJ59E+tYAACAFRTwyMmPGDM2YMaPf57/77rvKy8vTt771LUlSQUGBjh8/rh07dmj69OmRvj0AAEgxcb9R3smTJ1VRUdHj2LRp0/TWW2/1+ZyOjo4eN8SzLEtZWVmhr2Ml+FqxfE30Rp/tQ6/tQZ/tQZ8HxnR2Su1tUvuNW/9sk2m7/XXwuGlvU7PLkuY9IWvEaEdqjXsY8Xq9Gj58eI9jw4cPV1tbm27evKmMjIxez9m8ebM2bdoUejx+/HjV1NQoNzc3LjXm5+fH5XXRE322D722B322R7r02RgjddxU4MZ1mbYbd/zzugJtN7r+eePW1zduPzZtNxRo6/k8c9PX7/e+Lilv/hPKHDMmfj/gXcQ9jERj4cKFqq6uDj0OpuLGxkb5/f6YvY9lWcrPz1dDQwO3p44j+mwfem0P+myPZOizMUbytfccfWhvk3qMQLTJtIcfkVD3c31tUmdn7It0e6TBWbf/ZA2RlXn7sZU1RMNG56nZb6RLl2L71m53vwYS4h5GcnJy1NLS0uNYS0uLsrKywo6KSJLH45HH4wn7vXh8II0xCftBTyX02T702h702R6x7vPt6Yu23tMYYY71DhPdvu9rl+LxGcgc3BUWMrvCQyg4ZGZJWVk9w8XgIbLueNz9+5Y7/O/TIMuyNHzMGN24dMmxz3Pcw0hpaak+/vjjHscOHTqksrKyeL81ACBFmI6OXgFB7Te6hYdbIwy+bgHijseh79+8GfsCLdcdAaFbGAgTEHoFiKwhXcFjcJY0eLAs16DY15jAIg4j7e3tamhoCD2+cuWKzp07p2HDhmn06NFat26dmpub9V//9V+SpMcee0x/+ctf9Ic//EHz5s1TbW2t9u3bp5dffjl2PwUAIKEYY6Sbvp5TELcCQShA+HpOZ/QefWjTxZs+BW5clzpjN0UfMsjdNcqQ2S0QBEcfBvcckQiOUFh9jD4oI5OFtgMQcRg5ffq0fvrTn4Yer127VpL0la98RS+88IKuXr2qpqam0Pfz8vL08ssv6+2339bOnTs1atQoLVu2jMt6ASDBmECn1N5+xwhEmPUNPUYf2sKPPrS3SyYw4Jp6vUJGZtjpiPCjD3eMQISCx63Q0cdyANjPMkk04dnY2Njjkt+BsixLY8aM0SUH58nSAX22D722RyL12fg7pLZboSE0+hAMD7eOtfUcbehz/UMEV1/0m2XdHlnoPtqQeStAZHUbeRg8RMrKCoUHa/AQ5Y4rVOPn12QyB3c9Z1B6TV/YIZ6fZ4/HkxgLWAEAt3VNX9y89/qH7t9vuyETdvShTYrhFYYhgwb1McoQbgRiSLfjd659yJIyB0c9fWFZljxjxsi6dCk+i0SRMAgjAHAPJhDoumqi2xREe8OnCtRfvL1I8q6Xct7xJwbTF71kZNxl9GFI2NEJK8zVGhqcJbk9rH+ArQgjAFKS8XeECQJ3rH/odflm+PPla+/1+o0DLdCybl++2a/1D/e4fJPpCyQxwgiAhBDcfbLX9EXbrVGGbushul+J0efUhj9268tCBg0KjS547stWh9sTemxl3QoIodGHrLuPPmQMluWK+F6lQEoijACImgkEpJvtdyyQvNfoQx+LJ9vbpEAcpi88GXe/wiLMaEOfl296MmRZVtfOoAmygBVIBYQRIM0Yv7/npZjdFkSG3Z66/Y6ba/nuGJ2Ih8xwQaHb+of+rn3IzJLl5q85INHxXymQ4IwxXVMObf25+qJdzS5LnVc/uxUgwiyi7IjD7pMuV6/RhdsBovfah7uOPmQyfQGkG8IIEAfhbp7Ve/Sh2+hC+42uzaPu2KkymptnXe/viT1unjWk79GHbptFhYJF93tjZA6RMjK4+gJA1AgjwC2pcfOsIbovN0/X/J0ymYP7Hn0YfO+bZwGAXQgjSGoDvnlW99EH22+edUdAuDUa0WPzqO57Q/Tj5lmJcPdNAIgUYQSOMK1XpRs3wt88656jD93+xOPmWW73XUYfIln/wPQFAPQHYQS2C/zx1zIf7ozti/Z586w7Rx+yQvfACHtfjMHcPAsA7EYYga1Mx02Zve93PQgGhztu0231WBx5x+WbYa++YPdJAEhmhBHY60Rt151Bc0bK9ervmMIAAIiL+WErc+ifkiRr6kMEEQCAJMIIbGSM6RFGAACQCCOwU/2n0mdXujbbmjjV6WoAAAmCMALbBEdFNHGqrMzBzhYDAEgYhBHYJnDoX5KYogEA9EQYgS06P2+RTh2TJFlTZzlcDQAgkRBGYIv2/fskE5DGFsoaled0OQCABEIYgS3aP/q7JEZFAAC9EUYQd6azU23/2iuJ9SIAgN4II4i/08dlrn8uDb1PKi53uhoAQIIhjCDuAsGNzioqZbm4hwwAoCfCCOLOHPpIElM0AIDwCCOIK9PYINWfl1yDZE2e6XQ5AIAERBhBXJnDXRudZU6eJmvoMIerAQAkIsII4iq4Bfzgh/7D4UoAAImKMIK4Me1t0onDkqSshwkjAIDwCCOIn2MHJb9fys2Xu6DQ6WoAAAmKMIK4CU7RWFMfkmVZDlcDAEhUhBHEhQkEZA7vl8QlvQCAuyOMID7On5FamqXMLFllU5yuBgCQwAgjiAtzsGuKRpOmyfJ4nC0GAJDQCCOIi+7rRQAAuBvCCGLOeJulf5+SJFkVsxyuBgCQ6AgjiDlT27VwVUWlsoaPcLYYAEDCI4wg5piiAQBEgjCCmDIdHdLRTyRJ1lSmaAAA90YYQWzV1Uq+dmn4SOmBYqerAQAkAcIIYip4l15r6ixZLj5eAIB747cFYsYYI3PwI0lM0QAA+o8wgthpuCA1XZbcbmniNKerAQAkCcIIYiZ4FY3KK2QNznK2GABA0iCMIGa4pBcAEA3CCGLCXL8mnTomiV1XAQCRIYwgJsyRA1IgIN0/TlZuvtPlAACSCGEEsRGcomFUBAAQIcIIBsx0dsrUHpDEehEAQOQIIxi4Myek659LQ4ZJEyY6XQ0AIMkQRjBg5vCtKZoplbIGDXK4GgBAsiGMYMDMoa4t4MWuqwCAKBBGMCCm6bJ08d+S5ZI1ZabT5QAAkhBhBAMSvDGeSibKGnqfs8UAAJISYQQDEpyi4SoaAEC0CCOImvG1S8cPSSKMAACi547mSbt27dK2bdvk9XpVWFioJUuWqKSkpM/zd+zYoXfffVdNTU3Kzs7WF7/4RT333HPKyMiIunAkgGMHJX+HNCpPGvOA09UAAJJUxCMje/fu1dq1a7Vo0SLV1NSosLBQq1atUktLS9jz//GPf2jdunVavHixfvWrX2nZsmXat2+f1q9fP+Di4azuN8azLMvhagAAySriMLJ9+3bNnz9f8+bNU0FBgZYuXaqMjAzt3r077PknTpxQeXm55syZo7y8PE2bNk2PPPKITp06NeDi4RxjTGjxKlM0AICBiGiaxu/368yZM1qwYEHomMvlUkVFherq6sI+p7y8XH//+9916tQplZSU6PLly/r444/1H//xH32+T0dHhzo6OkKPLctSVlZW6OtYCb4W/1cfhfNnJG+zlDlY1sSKu/aQPtuHXtuDPtuDPtsjEfocURhpbW1VIBBQTk5Oj+M5OTmqr68P+5w5c+aotbVVr7zyiiSps7NTX/3qV/Xkk0/2+T6bN2/Wpk2bQo/Hjx+vmpoa5ebmRlJuv+Xnc5fZSLV8uEOtkrJmfkmjxxX26zn02T702h702R702R5O9jmqBayROHLkiDZv3qzvfve7Ki0tVUNDg373u99p06ZNWrRoUdjnLFy4UNXV1aHHwbTW2Ngov98fs9osy1J+fr4aGhpkjInZ66YD/54PJEm+0im6dOnSXc+lz/ah1/agz/agz/aIZ5/dbne/BhIiCiPZ2dlyuVzyer09jnu93l6jJUEbNmzQl7/8Zc2fP1+SNG7cOLW3t+s3v/mNnnzySblcvZeteDweeTyesK8Xjw+kMYYPegRM61Xp7K1puYrKfveOPtuHXtuDPtuDPtvDyT5HtIDV7XaruLhYtbW1oWOBQEC1tbUqKysL+xyfz9drHipcAEHyMIf3d31RWCIrZ5SzxQAAkl7E0zTV1dV68803VVxcrJKSEu3cuVM+n09z586VJK1evVojR47Uc889J0mqrKzUjh07NH78+NA0zYYNG1RZWUkoSVK3d13lxngAgIGLOIzMnj1bra2t2rhxo7xer4qKirR8+fLQNE1TU1OPkZCnnnpKlmXpnXfeUXNzs7Kzs1VZWalnn302Zj8E7GP8HdKRjyVxSS8AIDaiWsBaVVWlqqqqsN9bsWJFj8eDBg3S4sWLtXjx4mjeComm7ojka5Oyc6RxE5yuBgCQApgnQURCu65WzJLFNBsAIAb4bYJ+M8b02AIeAIBYIIyg/y5flBobJLdbmjTN6WoAACmCMIJ+C46KqGyKrMFDnC0GAJAyCCPot9uX9DJFAwCIHcII+sXcuCadPCKpa/EqAACxQhhBv5gjn0iBgDTmAVl5Y5wuBwCQQggj6J/QVTSMigAAYoswgnsygU6Z2lvrRSpYLwIAiC3CCO7tTJ107XNpyFBpwkSnqwEApBjCCO4ptNHZ5Jmy3FHdQQAAgD4RRnBP5nDXFI24pBcAEAeEEdyV+axRunBOslyypsx0uhwAQAoijOCuzOFbu65OKJc1LNvZYgAAKYkwgrti11UAQLwRRtAn4/NJxw9JIowAAOKHMIK+HT8kddyURuVJ949zuhoAQIoijKBPoUt6K2bJsiyHqwEApCrCCMIyxtwOI0zRAADiiDCC8M6flbyfSRmZ0sQKp6sBAKQwwgjCCm109uA0WZ4MZ4sBAKQ0wgjCYooGAGAXwgh6Ma1e6WydpK7FqwAAxBNhBL2Y2v2SMdK4YlkjRjldDgAgxRFG0AtTNAAAOxFG0IPxd0hHP5FEGAEA2IMwgp5OHpXabkj3DZcKS5yuBgCQBggj6CF0Y7yKWbJcfDwAAPHHbxv0wHoRAIDdCCMIMQ0XpSv10iC3NGm60+UAANIEYQQhoV1XyybLyhribDEAgLRBGEHI7SkaNjoDANiHMAJJkrlxXTp5RBLrRQAA9iKMoMvRj6XOTil/rKy8+52uBgCQRggjkNTtkl5GRQAANiOMQCbQ2XU/GhFGAAD2I4xAOntS+rxFyhoqTXjQ6WoAAGmGMILbUzSTZ8hyux2uBgCQbggjCF3SK6ZoAAAOIIykOdPcJF04K1mWrCmVTpcDAEhDhJE0F9p1tbhc1n3ZzhYDAEhLhJE0F9p1tYJdVwEAziCMpDHj80nHDkqSrGmsFwEAOIMwks5OHJI6bkojR0tji5yuBgCQpggjaSy4XsSa+pAsy3K4GgBAuiKMpCljDOtFAAAJgTCSri6ek5qbpIwMaeJUp6sBAKQxwkiaMgdvbXQ2cZqsjExniwEApDXCSJrqvl4EAAAnEUbSkPm8VTpzQhLrRQAAziOMpCFTu18yRioYL2vkaKfLAQCkOcJIOgpeRcMUDQAgARBG0ozx+2WOHJAkWVOZogEAOI8wkm5OH5Pabkj3DZfGlzpdDQAAhJF0E9robMpMWa5BDlcDAABhJO0Y1osAABKMO5on7dq1S9u2bZPX61VhYaGWLFmikpKSPs+/fv261q9fr48++kjXrl1Tbm6uvv3tb2vmzJlRF47Imcv1UsNFadAgadIMp8sBAEBSFGFk7969Wrt2rZYuXarS0lLt2LFDq1at0muvvabhw4f3Ot/v92vlypXKzs7W97//fY0cOVJNTU0aMmRITH4A9J85fGvX1dLJsoYMdbYYAABuiTiMbN++XfPnz9e8efMkSUuXLtWBAwe0e/duLViwoNf5H3zwga5du6b//d//ldvd9XZ5eXkDqxpRMYfYdRUAkHgiCiN+v19nzpzpETpcLpcqKipUV1cX9jn79+9XaWmpfvvb3+pf//qXsrOz9cgjj2jBggVyucIvWeno6FBHR0fosWVZysrKCn0dK8HXiuVrJirTdkOqOyJJck19yNafOZ367DR6bQ/6bA/6bI9E6HNEYaS1tVWBQEA5OTk9jufk5Ki+vj7scy5fvqzGxkbNmTNHP/rRj9TQ0KA1a9aos7NTixcvDvuczZs3a9OmTaHH48ePV01NjXJzcyMpt9/y8/Pj8rqJ5Mae9/VZp1/u+8dpzAxn9hdJhz4nCnptD/psD/psDyf7HNUC1kgYY5Sdna3nn39eLpdLxcXFam5u1tatW/sMIwsXLlR1dXXocTCtNTY2yu/3x6w2y7KUn5+vhoYGGWNi9rqJqPPDd7v+OXmGLl26ZOt7p1OfnUav7UGf7UGf7RHPPrvd7n4NJEQURrKzs+VyueT1ensc93q9vUZLgnJycuR2u3tMyYwdO1Zer1d+vz+0jqQ7j8cjj8cT9vXi8YE0xqT0B90EArfv0lsxy7GfNdX7nEjotT3osz3osz2c7HNE+4y43W4VFxertrY2dCwQCKi2tlZlZWVhn1NeXq6GhgYFAoHQsUuXLmnEiBFhgwji4N+npM9bpKwhUukkp6sBAKCHiDc9q66u1vvvv68PP/xQFy5c0Jo1a+Tz+TR37lxJ0urVq7Vu3brQ+Y899piuXbumt956S/X19Tpw4IA2b96sxx9/PGY/BO4uuNGZJk2X5Q4/4gQAgFMiHpqYPXu2WltbtXHjRnm9XhUVFWn58uWhaZqmpqYeK3JHjx6tH//4x3r77bf10ksvaeTIkfra174W9jJgxAe7rgIAEllU8yRVVVWqqqoK+70VK1b0OlZWVqZVq1ZF81YYIHP1M+nTM5JlyZpS6XQ5AAD0wr1pUlxw4arGl8nKznG0FgAAwiGMpLjQFE2FM3uLAABwL4SRFGZu+qRjByWxXgQAkLgII6nsRK100yfljJIeGO90NQAAhEUYSWHdr6Lh3g4AgERFGElRxpjbu64yRQMASGCEkVRV/6n02RXJkyFNnOp0NQAA9IkwkqJCu65OnCorM9PZYgAAuAvCSIq6vV6ES3oBAImNMJKCzLVW6fQJSZJVwXoRAEBiI4ykIFN7QDIBqaBI1qhcp8sBAOCuCCOpiF1XAQBJhDCSYozfL3PkgCQu6QUAJAfCSKo5fVy6cV0adp9UXOZ0NQAA3BNhJMWYw7emaKbMkuUa5HA1AADcG2EkxZhDXbuuikt6AQBJgjCSQsyVS9Kl85LLJWvyDKfLAQCgXwgjKSR4LxqVTpY1ZJizxQAA0E+EkRTCrqsAgGREGEkRpv2GVFcriUt6AQDJhTCSKo4elPx+KTdf+sJYp6sBAKDfCCMp4vYUzUOyLMvhagAA6D/CSAowgUBo8SpTNACAZEMYSQX/Pi21eqXMLKlsstPVAAAQEcJICgjuuqrJM2S5Pc4WAwBAhAgjKSC46yqX9AIAkhFhJMkZ72fSv09JkqyKSoerAQAgcoSRJGcO7+/6YnyZrOwRzhYDAEAUCCNJjikaAECyI4wkMdNxUzr2iSTJquCSXgBAciKMJLMTtZKvXcoZKY0rdroaAACiQhhJYqFdVytmsesqACBpEUaSlDGmxxbwAAAkK8JIsrp0XvrsiuT2SA9Oc7oaAACiRhhJUsFREU2skJU52NliAAAYAMJIkmKKBgCQKggjSchc/1w6dVxS1+JVAACSGWEkCZnaA5IJSGMLZY3+gtPlAAAwIISRZBTcdZVREQBACiCMJBnT2SlT23U/GtaLAABSAWEk2Zw+Lt24Jg29Tyoud7oaAAAGjDCSZEJX0UyZKWvQIIerAQBg4AgjScYc7lovIqZoAAApgjCSRExjg1T/qeRyyZo80+lyAACICcJIEgmNipQ8KGvoMGeLAQAgRggjSYRdVwEAqYgwkiRMe5t04rAkwggAILUQRpLF8YOS3y/l5kv5BU5XAwBAzBBGkoTptuuqZVkOVwMAQOwQRpKACQRuhxGmaAAAKYYwkgzOn5FamqXMwVLZFKerAQAgpggjScAc7LqKRpOmy/J4nC0GAIAYI4wkgeD+ItylFwCQiggjCc60XJXOnZREGAEApCbCSIIL7bpaWCIrZ6SzxQAAEAeEkQTHrqsAgFTnjuZJu3bt0rZt2+T1elVYWKglS5aopKTkns/bs2ePXn/9dc2aNUs//OEPo3nrtGI6OqSjn0iSrGmEEQBAaop4ZGTv3r1au3atFi1apJqaGhUWFmrVqlVqaWm56/OuXLmi3//+93rwwQejLjbtnKyVfO3S8BHSA8VOVwMAQFxEHEa2b9+u+fPna968eSooKNDSpUuVkZGh3bt39/mcQCCgN954Q08//bTy8vIGVHA66bHrqosZNQBAaopomsbv9+vMmTNasGBB6JjL5VJFRYXq6ur6fN6mTZuUnZ2tRx99VMeOHbvn+3R0dKijoyP02LIsZWVlhb6OleBrJeL26sYYmYMfSeqaoknEGvsrkfucaui1PeizPeizPRKhzxGFkdbWVgUCAeXk5PQ4npOTo/r6+rDPOX78uD744AO9+uqr/X6fzZs3a9OmTaHH48ePV01NjXJzcyMpt9/y8/Pj8roD0fHpWTU0XZbcHo2ZVyVX1hCnSxqwROxzqqLX9qDP9qDP9nCyz1EtYO2vtrY2vfHGG3r++eeVnZ3d7+ctXLhQ1dXVocfBtNbY2Ci/3x+z+izLUn5+vhoaGmSMidnrxkLgg52SJKu8Qpe9LZL37mtyElki9znV0Gt70Gd70Gd7xLPPbre7XwMJEYWR7OxsuVwueb3eHse9Xm+v0RJJunz5shobG1VTUxM6FvxBn3nmGb322mthk5jH45Gnj23P4/GBNMYk3Ac9ENwCvmJWwtUWrUTsc6qi1/agz/agz/Zwss8RhRG3263i4mLV1tbq4YcfltS1OLW2tlZVVVW9zr///vv1i1/8osexd955R+3t7frP//xPjR49egClpy5z/Zp06qgkyZrKrqsAgNQW8TRNdXW13nzzTRUXF6ukpEQ7d+6Uz+fT3LlzJUmrV6/WyJEj9dxzzykjI0Pjxo3r8fyhQ4dKUq/juM0cOSAFAtKYB2TlMlcKAEhtEYeR2bNnq7W1VRs3bpTX61VRUZGWL18emqZpampi5fNAsesqACCNRLWAtaqqKuy0jCStWLHirs994YUXonnLtGECnTK1ByQxRQMASA/spJVozpyQrn8uDRkqTWC3WgBA6iOMJJjQjfGmVMoaNMjhagAAiD/CSIIJbgEv1osAANIEYSSBmM+uSBf/LVkuWVNmOl0OAAC2IIwkkNCoyISJsobe52wxAADYhDCSQAyX9AIA0hBhJEEYX7t0/JAkwggAIL0QRhLFsYOSv0MalSfd/4DT1QAAYBvCSIIwh7vWi1hTH2IHWwBAWiGMJABjTLf1Iuy6CgBIL4SRRHD+jORtljIypfIKp6sBAMBWhJEEEBwV0aTpsjwZzhYDAIDNCCMJILi/CFfRAADSEWHEYab1qnTupCTJqqh0uBoAAOxHGHGYOXxAMkYaN0FWziinywEAwHaEEYex6yoAIN0RRhxk/B3SkY8lEUYAAOmLMOKkuiOSr03KzpEKJzhdDQAAjiCMOCi062pFpSwX/yoAAOmJ34AOMcbIHPxIElM0AID0RhhxyuWLUmODNMgtTZrudDUAADiGMOKQ0K6r5VNkDR7ibDEAADiIMOIQdl0FAKALYcQB5sY16dRRSZJVwV16AQDpjTDiAHPkE6mzU8ovkJU3xulyAABwFGHECey6CgBACGHEZibQKVPLehEAAIIII3Y7e1K69rmUNVSaMNHpagAAcBxhxGahG+NNmSnL7Xa4GgAAnEcYsVlof5GpXEUDAIBEGLGV+axRunBOslyyJlc6XQ4AAAmBMGIjc/jWqMiEcln3ZTtbDAAACYIwYqPQrqtsdAYAQAhhxCbG55OOH5LEJb0AAHRHGLHL8UNSx01pZK40ttDpagAASBiEEZuYbruuWpblcDUAACQOwogNjDEyh4O7rrJeBACA7ggjdrhwTrraJGVkSOUVTlcDAEBCIYzYILTR2YPTZWVkOlsMAAAJhjBig9vrRZiiAQDgToSRODOft0hn6yRJVgWX9AIAcCfCSJyZw/slY6QHxssaMcrpcgAASDiEkXjrdkkvAADojTASR8bfIXP0Y0mEEQAA+kIYiaeTR6W2G9J9w6WiUqerAQAgIRFG4qj7jfEsF60GACAcfkPGEbuuAgBwb4SRODENF6XLF6VBg6RJM5wuBwCAhEUYiZPgqIjKpsjKGuJsMQAAJDDCSJyw6yoAAP1DGIkDc+O6dPKIJHZdBQDgXggj8XDsE6mzU/rCWFlfuN/pagAASGiEkTgwB5miAQCgvwgjMWYCnTK1+yWx6yoAAP1BGIm1syelz1ukrCFSySSnqwEAIOERRmIstNHZpBmy3G6HqwEAIPFF9dty165d2rZtm7xerwoLC7VkyRKVlJSEPfevf/2r/va3v+n8+fOSpOLiYj377LN9np/sgpf0iikaAAD6JeKRkb1792rt2rVatGiRampqVFhYqFWrVqmlpSXs+UePHtUjjzyin/zkJ1q5cqVGjRqllStXqrm5ecDFJxrT3CSdPytZlqyKSqfLAQAgKUQcRrZv36758+dr3rx5Kigo0NKlS5WRkaHdu3eHPf/FF1/U448/rqKiIo0dO1bLli2TMUaHDx8ecPGJJrTranG5rPuGO1sMAABJIqJpGr/frzNnzmjBggWhYy6XSxUVFaqrq+vXa/h8Pvn9fg0bNqzPczo6OtTR0RF6bFmWsrKyQl/HSvC1Yvaat6ZoXFMfimmdyS7mfUaf6LU96LM96LM9EqHPEYWR1tZWBQIB5eTk9Diek5Oj+vr6fr3GH//4R40cOVIVFRV9nrN582Zt2rQp9Hj8+PGqqalRbm5uJOX2W35+/oBfI+BrV/2JQ5Kk3Ee/powxYwb8mqkmFn1G/9Bre9Bne9BnezjZZ1sv99iyZYv27NmjFStWKCMjo8/zFi5cqOrq6tDjYFprbGyU3++PWT2WZSk/P18NDQ0yxgzotQKH/iXj80kjRqtp8DBZly7FqMrkF8s+4+7otT3osz3osz3i2We3292vgYSIwkh2drZcLpe8Xm+P416vt9doyZ22bt2qLVu26JVXXlFhYeFdz/V4PPJ4PGG/F48PpDFmwK9rDn0k6fauq/yH01ss+oz+odf2oM/2oM/2cLLPES1gdbvdKi4uVm1tbehYIBBQbW2tysrK+nzen//8Z/3pT3/S8uXLNWHChOirTVDGmG536eWSXgAAIhHx1TTV1dV6//339eGHH+rChQtas2aNfD6f5s6dK0lavXq11q1bFzp/y5Yt2rBhg773ve8pLy9PXq9XXq9X7e3tMfshHHfxnNTcJHkypPKpTlcDAEBSiXjNyOzZs9Xa2qqNGzfK6/WqqKhIy5cvD03TNDU19ViR+95778nv9+uXv/xlj9dZtGiRnn766YFVnyDMoVuX9E6cKisz09liAABIMlEtYK2qqlJVVVXY761YsaLH4zfffDOat0gqTNEAABA97k0zQObzVunMCUm3F68CAID+I4wMkKndLxkjFYyXNTI++6AAAJDKCCMDFbxLL6MiAABEhTAyAMbvl6k9IIn1IgAARIswMhCnj0lt16Vh2dL4UqerAQAgKRFGBiB0FU1FpSzXIIerAQAgORFGBoBLegEAGDjCSJTMlXqp4aI0aJA0aYbT5QAAkLQII1EK7bpaMknWkKHOFgMAQBIjjESJKRoAAGKDMBIF03ZDqjsiiTACAMBAEUaicfQTqdMv5Y2RlT/W6WoAAEhqhJEomMNM0QAAECuEkQiZQCC0eJUwAgDAwBFGIvXvU9LnLdLgLKl0ktPVAACQ9AgjEQpeRaPJM2S5Pc4WAwBACiCMRCg0RVPBFA0AALFAGImAufqZ9OlpybJkVcx0uhwAAFICYSQC5vCtXVeLSmVlj3C2GAAAUgRhJALsugoAQOwRRvrJdNyUjh2URBgBACCWCCP9deKwdNMn5YySHhjvdDUAAKQMwkg/3Z6imSXLshyuBgCA1EEY6QdjDLuuAgAQJ4SR/qj/VPrsiuTJkCZOc7oaAABSCmGkH0K7rpZXyMrMdLYYAABSDGGkH5iiAQAgfggj92CutUqnj0vqWrwKAABiizByD6b2gGQC0thCWaPynC4HAICUQxi5F3ZdBQAgrggjd2E6O2WOHJDEFA0AAPFCGLmb08ekG9elofdJxeVOVwMAQEoijNxFaNfVikpZrkEOVwMAQGoijNxF8JJesV4EAIC4IYz0wTQ2SJfOSy6XrMkznC4HAICURRjpQ2hUpGSSrCHDnC0GAIAURhjpg+GSXgAAbEEYCcO035DqDksijAAAEG+EkXCOHpT8fik3X8of63Q1AACkNMJIGN2naCzLcrgaAABSG2HkDiYQkKndL4ldVwEAsANh5E6fnpZarkqZWVLpFKerAQAg5RFG7hCcotHk6bI8HmeLAQAgDRBG7hDcX4SraAAAsAdhpBvjbZb+fUqSZE2pdLgaAADSA2GkG3P41q6rRaWyho9wthgAANIEYaQbpmgAALAfYeQW03FTOvaJJMIIAAB2IowEnaiVfO3S8JHSuGKnqwEAIG0QRm4Jrhexps5i11UAAGxEGJFkjOm2BTy7rgIAYCfCiCRdOi81XZbcHmniNKerAQAgrRBGJJmDt3ZdnVgha3CWs8UAAJBmCCOSAsEpmgqmaAAAsFvah5HOz1uk08ckEUYAAHBC2oeR9v37pEBAun+crNx8p8sBACDtuKN50q5du7Rt2zZ5vV4VFhZqyZIlKikp6fP8ffv2acOGDWpsbFR+fr6++c1vaubMmVEXHUvt//yHJDY6AwDAKRGPjOzdu1dr167VokWLVFNTo8LCQq1atUotLS1hzz9x4oRef/11Pfroo6qpqdFDDz2kn//85/r0008HXPxAmc5Otf9rryTCCAAATok4jGzfvl3z58/XvHnzVFBQoKVLlyojI0O7d+8Oe/7OnTs1ffp0feMb31BBQYGeeeYZFRcXa9euXQMufsBOH1fgWqs0ZJhUXO50NQAApKWIpmn8fr/OnDmjBQsWhI65XC5VVFSorq4u7HPq6upUXV3d49i0adP0z3/+s8/36ejoUEdHR+ixZVnKysoKfR0rgeCuqxWVcrmjmrFCPwT/nbGzbfzRa3vQZ3vQZ3skQp8j+g3c2tqqQCCgnJycHsdzcnJUX18f9jler1fDhw/vcWz48OHyer19vs/mzZu1adOm0OPx48erpqZGubm5kZR7T5eOHlBA0ogvf1VDx4yJ6Wujt/x8FgjbhV7bgz7bgz7bw8k+J+RwwMKFC3uMpgTTWmNjo/x+f0zewxgjfW2xhtQdVmvBBLVeuhST10VvlmUpPz9fDQ0NXX1H3NBre9Bne9Bne8Szz263u18DCRGFkezsbLlcrl6jGl6vt9doSVBOTk6vxa0tLS19ni9JHo9HHo8n7Pdi2Shr1hyN+n+LdenSJT7oNjDG0Geb0Gt70Gd70Gd7ONnniBawut1uFRcXq7a2NnQsEAiotrZWZWVlYZ9TVlamw4cP9zh26NAhlZaWRlEuAABINRFfTVNdXa33339fH374oS5cuKA1a9bI5/Np7ty5kqTVq1dr3bp1ofO//vWv6+DBg9q2bZsuXryojRs36vTp06qqqorZDwEAAJJXxGtGZs+erdbWVm3cuFFer1dFRUVavnx5aNqlqampx4rc8vJyvfjii3rnnXe0fv16jRkzRi+99JLGjRsXsx8CAAAkL8sk0URcY2Njj0t+B8qyLI0ZM4Y1I3FGn+1Dr+1Bn+1Bn+0Rzz57PJ5+LWBN+3vTAAAAZxFGAACAowgjAADAUYQRAADgKMIIAABwFGEEAAA4ijACAAAcRRgBAACOIowAAABHRbwdvJPc7viUG6/XRU/02T702h702R702R7x6HN/XzOptoMHAACpJ62nadra2vQ///M/amtrc7qUlEaf7UOv7UGf7UGf7ZEIfU7rMGKM0dmzZ7kBU5zRZ/vQa3vQZ3vQZ3skQp/TOowAAADnEUYAAICj0jqMeDweLVq0SB6Px+lSUhp9tg+9tgd9tgd9tkci9JmraQAAgKPSemQEAAA4jzACAAAcRRgBAACOIowAAABHpfyG/7t27dK2bdvk9XpVWFioJUuWqKSkpM/z9+3bpw0bNqixsVH5+fn65je/qZkzZ9pYcXKKpM9//etf9be//U3nz5+XJBUXF+vZZ5+9678X3BbpZzpoz549ev311zVr1iz98Ic/tKHS5BZpn69fv67169fro48+0rVr15Sbm6tvf/vb/P1xD5H2eceOHXr33XfV1NSk7OxsffGLX9Rzzz2njIwMG6tOLkePHtXWrVt19uxZXb16VT/4wQ/08MMP3/U5R44c0dq1a3X+/HmNGjVKTz31lObOnRu3GlN6ZGTv3r1au3atFi1apJqaGhUWFmrVqlVqaWkJe/6JEyf0+uuv69FHH1VNTY0eeugh/fznP9enn35qc+XJJdI+Hz16VI888oh+8pOfaOXKlRo1apRWrlyp5uZmmytPPpH2OujKlSv6/e9/rwcffNCmSpNbpH32+/1auXKlGhsb9f3vf1+vvfaann/+eY0cOdLmypNLpH3+xz/+oXXr1mnx4sX61a9+pWXLlmnfvn1av369zZUnF5/Pp6KiIn3nO9/p1/lXrlzRz372M02ePFmvvvqqnnjiCf3617/WJ598ErcaUzqMbN++XfPnz9e8efNUUFCgpUuXKiMjQ7t37w57/s6dOzV9+nR94xvfUEFBgZ555hkVFxdr165dNleeXCLt84svvqjHH39cRUVFGjt2rJYtWyZjjA4fPmxz5ckn0l5LUiAQ0BtvvKGnn35aeXl5NlabvCLt8wcffKBr167ppZde0sSJE5WXl6dJkyapqKjI3sKTTKR9PnHihMrLyzVnzhzl5eVp2rRpeuSRR3Tq1CmbK08uM2bM0DPPPHPP0ZCgd999V3l5efrWt76lgoICVVVV6Utf+pJ27NgRtxpTNoz4/X6dOXNGFRUVoWMul0sVFRWqq6sL+5y6uroe50vStGnTdPLkybjWmsyi6fOdfD6f/H6/hg0bFq8yU0K0vd60aZOys7P16KOP2lFm0oumz/v371dpaal++9vfaunSpfrv//5v/d///Z8CgYBdZSedaPpcXl6uM2fOhMLH5cuX9fHHH2vGjBm21JwuTp48GfZ3YX//To9Gyq4ZaW1tVSAQUE5OTo/jOTk5qq+vD/scr9er4cOH9zg2fPhweb3eOFWZ/KLp853++Mc/auTIkb0+/Ogpml4fP35cH3zwgV599VUbKkwN0fT58uXLamxs1Jw5c/SjH/1IDQ0NWrNmjTo7O7V48WIbqk4+0fR5zpw5am1t1SuvvCJJ6uzs1Fe/+lU9+eST8S43rfT1u7CtrU03b96My/qclA0jSA5btmzRnj17tGLFChagxVhbW5veeOMNPf/888rOzna6nJRmjFF2draef/55uVwuFRcXq7m5WVu3biWMxNCRI0e0efNmffe731VpaakaGhr0u9/9Tps2bdKiRYucLg8DkLJhJDs7Wy6Xq9eohtfr7ZXEg3JycnotnGppaenzfETX56CtW7dqy5YteuWVV1RYWBi/IlNEpL0O/t96TU1N6Fjw7g/PPPOMXnvtNeXn58ez5KQU7d8dbrdbLtftme+xY8fK6/XK7/fL7U7Zv2qjFk2fN2zYoC9/+cuaP3++JGncuHFqb2/Xb37zGz355JM9+o/o9fW7MCsrK27/05iy/+bcbreKi4tVW1sbOhYIBFRbW6uysrKwzykrK+u1iPLQoUMqLS2Na63JLJo+S9Kf//xn/elPf9Ly5cs1YcIEO0pNepH2+v7779cvfvELvfrqq6E/lZWVoRXyo0ePtrP8pBHNZ7q8vFwNDQ091ohcunRJI0aMIIj0IZo++3w+WZbV4xgBJPZKS0vD/i6829/pA5XS/xarq6v1/vvv68MPP9SFCxe0Zs0a+Xy+0LXSq1ev1rp160Lnf/3rX9fBgwe1bds2Xbx4URs3btTp06dVVVXl0E+QHCLt85YtW7RhwwZ973vfU15enrxer7xer9rb2x36CZJHJL3OyMjQuHHjevwZOnSoBg8erHHjxvFL8i4i/Uw/9thjunbtmt566y3V19frwIED2rx5sx5//HGHfoLkEGmfKysr9d5772nPnj26cuWKDh06pA0bNqiyspJQchft7e06d+6czp07J6nr0t1z586pqalJkrRu3TqtXr06dP5jjz2mK1eu6A9/+IMuXryov/zlL9q3b5+eeOKJuNWY0n8bzZ49W62trdq4caO8Xq+Kioq0fPny0BBgU1NTj5RdXl6uF198Ue+8847Wr1+vMWPG6KWXXtK4ceMc+gmSQ6R9fu+99+T3+/XLX/6yx+ssWrRITz/9tJ2lJ51Ie43oRNrn0aNH68c//rHefvttvfTSSxo5cqS+9rWvacGCBc78AEki0j4/9dRTsixL77zzjpqbm5Wdna3Kyko9++yzDv0EyeH06dP66U9/Gnq8du1aSdJXvvIVvfDCC7p69WoomEhSXl6eXn75Zb399tvauXOnRo0apWXLlmn69Olxq9EywUlkAAAABzCuBQAAHEUYAQAAjiKMAAAARxFGAACAowgjAADAUYQRAADgKMIIAABwFGEEAAA4ijACAAAcRRgBAACOIowAAABHEUYAAICj/j/MyaBNGNw4FgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "fpr_LR , tpr_LR, thresholds_LR = roc_curve(testing_labels, predicted_lr)\n",
        "auc_score = roc_auc_score(testing_labels, predicted_lr)\n",
        "plt.plot(fpr_LR, tpr_LR, label= \"LR ({:.2f})\".format(auc_score))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-_slUeFV6bfC"
      },
      "outputs": [],
      "source": [
        "df =pd.DataFrame(train_dataset)\n",
        "df.to_excel('/content/lr_train_dataset.xlsx')\n",
        "df =pd.DataFrame(training_labels)\n",
        "df.to_excel('/content/lr_training_labels.xlsx')\n",
        "df =pd.DataFrame(predicted_training_labels)\n",
        "df.to_excel('/content/lr_pred_train_labels.xlsx')\n",
        "\n",
        "df =pd.DataFrame(test_dataset)\n",
        "df.to_excel('/content/lr_test_dataset.xlsx')\n",
        "df =pd.DataFrame(testing_labels)\n",
        "df.to_excel('/content/lr_testing_labels.xlsx')\n",
        "df =pd.DataFrame(predicted_lr)\n",
        "df.to_excel('/content/lr_pred_test_labels.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPUQwR-eKR38",
        "outputId": "2f4542c7-8349-4c46-9e26-3f47e58c46e6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-13-286145be1eb1>:9: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
            "  ax.set_xticklabels([''] + labels)\n",
            "<ipython-input-13-286145be1eb1>:10: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
            "  ax.set_yticklabels([''] + labels)\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "classes = np.unique(testing_labels)\n",
        "\n",
        "confusion_matrix_data = confusion_matrix(testing_labels, predicted_lr, labels=classes)\n",
        "conf_matrix(confusion_matrix_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dq2RkIo-1Jdb",
        "outputId": "80fdc58a-d57a-4e0a-d7cf-1598276db228"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "773 262 69 45\n",
            "0.9449877750611247\n",
            "0.7915407854984894\n"
          ]
        }
      ],
      "source": [
        "TP = confusion_matrix_data[1,1]\n",
        "TN = confusion_matrix_data[0,0]\n",
        "FP = confusion_matrix_data[0,1]\n",
        "FN = confusion_matrix_data[1,0]\n",
        "\n",
        "print(TP,TN, FP, FN)\n",
        "\n",
        "sn = TP / float(TP + FN)\n",
        "print(sn)\n",
        "sp = TN / float(TN + FP)\n",
        "print(sp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zpez8nyOvPOF"
      },
      "source": [
        "### MLP with Grid Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ItWPg-FUXUTu"
      },
      "outputs": [],
      "source": [
        "from numpy.random import seed\n",
        "seed(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6RO9jsBSXUTv"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import set_random_seed\n",
        "set_random_seed(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vug_DpuSylO-"
      },
      "outputs": [],
      "source": [
        "# Basic Protocol 3 — Step 8\n",
        "\n",
        "multilayerperceptron = MLPClassifier(solver='lbfgs', random_state=10, max_iter=1000)\n",
        "\n",
        "parameters = {\n",
        "    'hidden_layer_sizes': [(2560,), (1024,)],\n",
        "    #'learning_rate_init': [0.001, 0.0001, 0.01],\n",
        "    'solver':['sgd'],\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8uCe4Yoce2Q"
      },
      "outputs": [],
      "source": [
        "metrics = {'accuracy':make_scorer(accuracy_score,greater_is_better=True),'f1':make_scorer(f1_score,greater_is_better=True),'mcc':make_scorer(matthews_corrcoef,greater_is_better=True)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qlth5o02yn-d",
        "outputId": "d3de4080-3192-4d6e-92ac-b2930c97d13e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Basic Protocol 3 — Step 9\n",
        "\n",
        "classifiers = GridSearchCV(multilayerperceptron, parameters, cv=10, scoring=metrics, refit='mcc')\n",
        "history = classifiers.fit(train_dataset, training_labels)\n",
        "classifier = classifiers.best_estimator_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMv8_JC0WcSQ",
        "outputId": "881aa6a4-0ab8-4404-95d2-003ce76356e6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'activation': 'relu',\n",
              " 'alpha': 0.0001,\n",
              " 'batch_size': 'auto',\n",
              " 'beta_1': 0.9,\n",
              " 'beta_2': 0.999,\n",
              " 'early_stopping': False,\n",
              " 'epsilon': 1e-08,\n",
              " 'hidden_layer_sizes': (2560,),\n",
              " 'learning_rate': 'constant',\n",
              " 'learning_rate_init': 0.001,\n",
              " 'max_fun': 15000,\n",
              " 'max_iter': 1000,\n",
              " 'momentum': 0.9,\n",
              " 'n_iter_no_change': 10,\n",
              " 'nesterovs_momentum': True,\n",
              " 'power_t': 0.5,\n",
              " 'random_state': 10,\n",
              " 'shuffle': True,\n",
              " 'solver': 'sgd',\n",
              " 'tol': 0.0001,\n",
              " 'validation_fraction': 0.1,\n",
              " 'verbose': False,\n",
              " 'warm_start': False}"
            ]
          },
          "execution_count": 132,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "params = classifier.get_params()\n",
        "params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FkLigimLWROf"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from pandas import DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "id": "kMYQTWrF_05C",
        "outputId": "dee94dc0-6c72-4b6e-c391-7772ede598b3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-2bbef76e-c0f9-490b-9515-7cbbf7d9b111\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mean_fit_time</th>\n",
              "      <th>std_fit_time</th>\n",
              "      <th>mean_score_time</th>\n",
              "      <th>std_score_time</th>\n",
              "      <th>param_hidden_layer_sizes</th>\n",
              "      <th>param_solver</th>\n",
              "      <th>params</th>\n",
              "      <th>split0_test_accuracy</th>\n",
              "      <th>split1_test_accuracy</th>\n",
              "      <th>split2_test_accuracy</th>\n",
              "      <th>...</th>\n",
              "      <th>split3_test_mcc</th>\n",
              "      <th>split4_test_mcc</th>\n",
              "      <th>split5_test_mcc</th>\n",
              "      <th>split6_test_mcc</th>\n",
              "      <th>split7_test_mcc</th>\n",
              "      <th>split8_test_mcc</th>\n",
              "      <th>split9_test_mcc</th>\n",
              "      <th>mean_test_mcc</th>\n",
              "      <th>std_test_mcc</th>\n",
              "      <th>rank_test_mcc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>280.930786</td>\n",
              "      <td>3.425812</td>\n",
              "      <td>0.019157</td>\n",
              "      <td>0.004272</td>\n",
              "      <td>(2560,)</td>\n",
              "      <td>sgd</td>\n",
              "      <td>{'hidden_layer_sizes': (2560,), 'solver': 'sgd'}</td>\n",
              "      <td>0.867925</td>\n",
              "      <td>0.867925</td>\n",
              "      <td>0.867925</td>\n",
              "      <td>...</td>\n",
              "      <td>0.773504</td>\n",
              "      <td>0.775401</td>\n",
              "      <td>0.623754</td>\n",
              "      <td>0.660486</td>\n",
              "      <td>0.781874</td>\n",
              "      <td>0.736092</td>\n",
              "      <td>0.706923</td>\n",
              "      <td>0.727166</td>\n",
              "      <td>0.048431</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>117.117928</td>\n",
              "      <td>0.834640</td>\n",
              "      <td>0.011756</td>\n",
              "      <td>0.004624</td>\n",
              "      <td>(1024,)</td>\n",
              "      <td>sgd</td>\n",
              "      <td>{'hidden_layer_sizes': (1024,), 'solver': 'sgd'}</td>\n",
              "      <td>0.849057</td>\n",
              "      <td>0.867925</td>\n",
              "      <td>0.867925</td>\n",
              "      <td>...</td>\n",
              "      <td>0.736092</td>\n",
              "      <td>0.775401</td>\n",
              "      <td>0.547931</td>\n",
              "      <td>0.660486</td>\n",
              "      <td>0.781874</td>\n",
              "      <td>0.736092</td>\n",
              "      <td>0.811966</td>\n",
              "      <td>0.722538</td>\n",
              "      <td>0.070822</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2 rows × 46 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2bbef76e-c0f9-490b-9515-7cbbf7d9b111')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2bbef76e-c0f9-490b-9515-7cbbf7d9b111 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2bbef76e-c0f9-490b-9515-7cbbf7d9b111');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-7e2d7047-9f04-40b6-8bb3-51b72a445a5a\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7e2d7047-9f04-40b6-8bb3-51b72a445a5a')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-7e2d7047-9f04-40b6-8bb3-51b72a445a5a button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
              "0     280.930786      3.425812         0.019157        0.004272   \n",
              "1     117.117928      0.834640         0.011756        0.004624   \n",
              "\n",
              "  param_hidden_layer_sizes param_solver  \\\n",
              "0                  (2560,)          sgd   \n",
              "1                  (1024,)          sgd   \n",
              "\n",
              "                                             params  split0_test_accuracy  \\\n",
              "0  {'hidden_layer_sizes': (2560,), 'solver': 'sgd'}              0.867925   \n",
              "1  {'hidden_layer_sizes': (1024,), 'solver': 'sgd'}              0.849057   \n",
              "\n",
              "   split1_test_accuracy  split2_test_accuracy  ...  split3_test_mcc  \\\n",
              "0              0.867925              0.867925  ...         0.773504   \n",
              "1              0.867925              0.867925  ...         0.736092   \n",
              "\n",
              "   split4_test_mcc  split5_test_mcc  split6_test_mcc  split7_test_mcc  \\\n",
              "0         0.775401         0.623754         0.660486         0.781874   \n",
              "1         0.775401         0.547931         0.660486         0.781874   \n",
              "\n",
              "   split8_test_mcc  split9_test_mcc  mean_test_mcc  std_test_mcc  \\\n",
              "0         0.736092         0.706923       0.727166      0.048431   \n",
              "1         0.736092         0.811966       0.722538      0.070822   \n",
              "\n",
              "   rank_test_mcc  \n",
              "0              1  \n",
              "1              2  \n",
              "\n",
              "[2 rows x 46 columns]"
            ]
          },
          "execution_count": 134,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "DataFrame(classifiers.cv_results_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6vFSrlejaAc5"
      },
      "outputs": [],
      "source": [
        "df =pd.DataFrame(classifiers.cv_results_)\n",
        "new_path = '/content/test.xls'\n",
        "writer = pd.ExcelWriter(new_path, engine='xlsxwriter')\n",
        "df.to_excel('/content/drive/MyDrive/Acidophilic/ESM23B20.xlsx')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PxYcUCSOo7dN",
        "outputId": "cebb6210-2e2b-4384-8e31-4195afa2fbc7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "acc: 0.9320754716981132\n",
            "f1: 0.9310344827586208\n",
            "mcc: 0.8645449874794131\n",
            "sn: [0.9100719424460432, 0.9022556390977443, 0.936, 0.9424778761061947, 0.8909090909090909, 0.9069767441860465, 0.9115384615384615, 0.8697318007662835, 0.91796875, 0.933852140077821, 0.8876404494382022, 0.9302325581395349, 0.8988326848249028, 0.9172932330827067, 0.9267399267399268, 0.9186046511627907, 0.8932384341637011, 0.8853046594982079, 0.926530612244898, 0.9144981412639405, 0.8843283582089553, 0.9307692307692308, 0.9011406844106464, 0.9104477611940298, 0.9014084507042254, 0.8971631205673759, 0.9029850746268657, 0.9087591240875912, 0.9060150375939849, 0.8984375, 0.9308943089430894, 0.9343629343629344, 0.9242424242424242, 0.9298892988929889, 0.8955223880597015, 0.9347826086956522, 0.9188191881918819, 0.9467680608365019, 0.9251968503937008, 0.9169811320754717, 0.9111111111111111, 0.9115384615384615, 0.9105691056910569, 0.9077490774907749, 0.9233576642335767, 0.9222614840989399, 0.9330708661417323, 0.8984375, 0.9318181818181818, 0.9142857142857143, 0.9169811320754717, 0.8851851851851852, 0.9400749063670412, 0.937037037037037, 0.916058394160584, 0.8969465648854962, 0.9584905660377359, 0.9257950530035336, 0.922509225092251, 0.9101123595505618, 0.9186046511627907, 0.9298892988929889, 0.9214285714285714, 0.927710843373494, 0.9067164179104478, 0.9507575757575758, 0.9409282700421941, 0.9384057971014492, 0.8856088560885609, 0.910958904109589, 0.9358490566037736, 0.8793774319066148, 0.9205776173285198, 0.9126984126984127, 0.903114186851211, 0.933579335793358, 0.945054945054945, 0.863849765258216, 0.9111111111111111, 0.9301470588235294, 0.9310344827586207, 0.9400749063670412, 0.9049429657794676, 0.9157894736842105, 0.915057915057915, 0.9024390243902439, 0.9516129032258065, 0.909433962264151, 0.9125475285171103, 0.926923076923077, 0.9147286821705426, 0.8870967741935484, 0.9389312977099237, 0.9390681003584229, 0.9354838709677419, 0.89453125, 0.9338235294117647, 0.9251968503937008, 0.917910447761194, 0.9296296296296296, 0.9056603773584906, 0.9233716475095786, 0.9104477611940298, 0.9100719424460432, 0.9398496240601504, 0.9139784946236559, 0.9291044776119403, 0.9358490566037736, 0.9377431906614786, 0.902127659574468, 0.8937007874015748, 0.90625, 0.9067164179104478, 0.8744588744588745, 0.9154411764705882, 0.9094076655052264, 0.9176029962546817, 0.9, 0.9262295081967213, 0.9259259259259259, 0.9285714285714286, 0.9114391143911439, 0.9304029304029304, 0.9069767441860465, 0.91796875, 0.9523809523809523, 0.9042145593869731, 0.9213483146067416, 0.9115384615384615, 0.9354838709677419, 0.8981818181818182, 0.8943396226415095, 0.92, 0.9296875, 0.9188191881918819, 0.9137254901960784, 0.9428571428571428, 0.9342560553633218, 0.934156378600823, 0.9477911646586346, 0.9305555555555556, 0.9548872180451128, 0.896797153024911, 0.952755905511811, 0.9306569343065694, 0.9027777777777778, 0.9358490566037736, 0.8896797153024911, 0.916030534351145, 0.9135338345864662, 0.89272030651341, 0.9083969465648855, 0.9227642276422764, 0.9111111111111111, 0.914396887159533, 0.9114391143911439, 0.9172661870503597, 0.891566265060241, 0.9267399267399268, 0.9356060606060606, 0.9153225806451613, 0.9285714285714286, 0.9288389513108615, 0.9285714285714286, 0.9, 0.9029850746268657, 0.924187725631769, 0.8941605839416058, 0.896, 0.9230769230769231, 0.8933823529411765, 0.9172932330827067, 0.940959409594096, 0.9166666666666666, 0.9330543933054394, 0.9056603773584906, 0.9063670411985019, 0.8914728682170543, 0.9312977099236641, 0.9175257731958762, 0.899641577060932, 0.9176029962546817, 0.9139344262295082, 0.896, 0.9097744360902256, 0.9090909090909091, 0.9157509157509157, 0.9108527131782945, 0.8892988929889298, 0.9300411522633745, 0.9398496240601504, 0.9124087591240876, 0.8953068592057761, 0.8647540983606558, 0.8932806324110671, 0.9180327868852459, 0.9267399267399268, 0.9267399267399268, 0.9207547169811321, 0.926829268292683, 0.9473684210526315, 0.9204545454545454, 0.8969465648854962, 0.9028776978417267, 0.9011406844106464, 0.9395161290322581, 0.9758064516129032, 0.9215686274509803, 0.9298892988929889, 0.9274809160305344, 0.9415807560137457, 0.8821428571428571, 0.9266409266409267, 0.9384615384615385, 0.9007633587786259, 0.9061371841155235, 0.9398496240601504, 0.9236363636363636, 0.9118773946360154, 0.928, 0.9014598540145985, 0.8873239436619719, 0.9236641221374046, 0.8823529411764706, 0.9018867924528302, 0.9291338582677166, 0.9204545454545454, 0.932, 0.9067164179104478, 0.9037037037037037, 0.9252669039145908, 0.92578125, 0.9372549019607843, 0.9259259259259259, 0.8849206349206349, 0.9148148148148149, 0.9064748201438849, 0.9166666666666666, 0.9239130434782609, 0.9007936507936508, 0.9330708661417323, 0.9204545454545454, 0.8817204301075269, 0.90625, 0.9272030651340997, 0.8951612903225806, 0.93359375, 0.9218106995884774, 0.9317269076305221, 0.9111969111969112, 0.912, 0.9285714285714286, 0.9204545454545454, 0.9045936395759717, 0.9194139194139194, 0.907258064516129, 0.9201520912547528, 0.9228070175438596, 0.945054945054945, 0.9183673469387755, 0.9450980392156862, 0.8819188191881919, 0.929368029739777, 0.9340659340659341, 0.9395017793594306, 0.9343629343629344, 0.9653846153846154, 0.9423076923076923, 0.9182156133828996, 0.9448818897637795, 0.8995983935742972, 0.9173228346456693, 0.9, 0.8964285714285715, 0.8981818181818182, 0.9219858156028369, 0.9, 0.9547169811320755, 0.9325842696629213, 0.9300411522633745, 0.9007633587786259, 0.9018867924528302, 0.9071729957805907, 0.9060150375939849, 0.9094076655052264, 0.9320754716981132, 0.9007633587786259, 0.9327731092436975, 0.8988326848249028, 0.9356060606060606, 0.916058394160584, 0.9116465863453815, 0.9034749034749034, 0.9034749034749034, 0.917910447761194, 0.9098039215686274, 0.9133574007220217, 0.9377289377289377, 0.9309090909090909, 0.9545454545454546, 0.8969465648854962, 0.9283018867924528, 0.929368029739777, 0.9185185185185185, 0.9272030651340997, 0.921875, 0.9366197183098591, 0.8819444444444444, 0.9330357142857143, 0.9182156133828996, 0.9043824701195219, 0.9227941176470589, 0.9185185185185185, 0.9325396825396826, 0.9065040650406504, 0.8966789667896679, 0.9056603773584906, 0.8863636363636364, 0.89272030651341, 0.9100719424460432, 0.8995983935742972, 0.9101123595505618, 0.9518518518518518, 0.915057915057915, 0.9185185185185185, 0.9420289855072463, 0.9330855018587361, 0.91701244813278, 0.8996282527881041, 0.9182879377431906, 0.94140625, 0.9012345679012346, 0.929368029739777, 0.9026217228464419, 0.9393939393939394, 0.9481481481481482, 0.9390681003584229, 0.9084507042253521, 0.920863309352518, 0.8921933085501859, 0.900398406374502, 0.8806584362139918, 0.9288389513108615, 0.9239543726235742, 0.9049586776859504, 0.9124087591240876, 0.8992537313432836, 0.9122137404580153, 0.9003690036900369, 0.9097472924187726, 0.921146953405018, 0.9195402298850575, 0.9166666666666666, 0.9101123595505618, 0.920863309352518, 0.8939393939393939, 0.9201520912547528, 0.9485294117647058, 0.9094488188976378, 0.9301470588235294, 0.9272030651340997, 0.9416342412451362, 0.9124087591240876, 0.909433962264151, 0.9239543726235742, 0.9354838709677419, 0.8924302788844621, 0.8901960784313725, 0.8977272727272727, 0.9307692307692308, 0.9068100358422939, 0.9452554744525548, 0.9291044776119403, 0.908745247148289, 0.8774703557312253, 0.9067164179104478, 0.915057915057915, 0.8778625954198473, 0.9138576779026217, 0.896797153024911, 0.9411764705882353, 0.9157509157509157, 0.9264705882352942, 0.8784722222222222, 0.917910447761194, 0.9090909090909091, 0.9407665505226481, 0.9236641221374046, 0.9067164179104478, 0.894927536231884, 0.9043824701195219, 0.889763779527559, 0.9347826086956522, 0.9101123595505618, 0.9213483146067416, 0.9294117647058824, 0.8929889298892989, 0.9446494464944649, 0.9, 0.9293286219081273, 0.8817204301075269, 0.8873239436619719, 0.9007352941176471, 0.9233576642335767, 0.9352226720647774, 0.926056338028169, 0.9058823529411765, 0.916058394160584, 0.9250936329588015, 0.9236947791164659, 0.9375, 0.9358490566037736, 0.9506172839506173, 0.9250936329588015, 0.9191176470588235, 0.9457364341085271, 0.9104477611940298, 0.8555133079847909, 0.9137254901960784, 0.8939393939393939, 0.9120879120879121, 0.8920863309352518, 0.889763779527559, 0.9213483146067416, 0.9024390243902439, 0.9584905660377359, 0.933579335793358, 0.9007633587786259, 0.9242424242424242, 0.9034749034749034, 0.9007352941176471, 0.9247311827956989, 0.9330708661417323, 0.9259259259259259, 0.8992537313432836, 0.9053030303030303, 0.9117647058823529, 0.9038461538461539, 0.9264705882352942, 0.9083969465648855, 0.8888888888888888, 0.9431818181818182, 0.8893129770992366, 0.9343065693430657, 0.9107806691449815, 0.931899641577061, 0.9227941176470589, 0.9137254901960784, 0.9367588932806324, 0.9090909090909091, 0.8996282527881041, 0.8973384030418251, 0.9083333333333333, 0.9348659003831418, 0.9277566539923955, 0.9393939393939394, 0.9291044776119403, 0.9163498098859315, 0.9245283018867925, 0.9198473282442748, 0.8851851851851852, 0.9133858267716536, 0.8828125, 0.9051383399209486, 0.9296296296296296, 0.9087301587301587, 0.9246031746031746, 0.9624060150375939, 0.9040590405904059, 0.9153846153846154, 0.9068100358422939, 0.9438202247191011, 0.8955223880597015, 0.9027237354085603, 0.90625, 0.9069767441860465, 0.9194139194139194, 0.9142857142857143, 0.9188191881918819, 0.899641577060932, 0.8695652173913043, 0.9242424242424242, 0.914396887159533, 0.9149797570850202, 0.9154411764705882, 0.9288389513108615, 0.9224806201550387, 0.917910447761194, 0.9280575539568345, 0.9142857142857143, 0.9246031746031746, 0.946969696969697, 0.9212598425196851, 0.908745247148289, 0.946236559139785, 0.89272030651341, 0.926923076923077, 0.9382239382239382, 0.9246575342465754, 0.9137254901960784, 0.8973384030418251, 0.8929889298892989, 0.9320754716981132, 0.8983739837398373, 0.9015151515151515, 0.9242424242424242, 0.9192307692307692, 0.8931297709923665, 0.9477911646586346, 0.931899641577061, 0.9203187250996016, 0.9037037037037037, 0.914396887159533, 0.9333333333333333, 0.8571428571428571, 0.8953488372093024, 0.9392712550607287, 0.9014084507042254, 0.9288256227758007, 0.9101123595505618, 0.9130434782608695, 0.9311594202898551, 0.9558232931726908, 0.9094202898550725, 0.8901960784313725, 0.8992248062015504, 0.9133858267716536, 0.9035714285714286, 0.8876404494382022, 0.9294117647058824, 0.8814814814814815, 0.9003690036900369, 0.9444444444444444, 0.9090909090909091, 0.9636363636363636, 0.928030303030303, 0.8818897637795275, 0.9185185185185185, 0.9224806201550387, 0.9261992619926199, 0.946969696969697, 0.9011406844106464, 0.9245283018867925, 0.9325842696629213, 0.9288256227758007, 0.9106529209621993, 0.9125475285171103, 0.8962962962962963, 0.9230769230769231, 0.9333333333333333, 0.9007633587786259, 0.9163636363636364, 0.948, 0.9382239382239382, 0.924187725631769, 0.9263565891472868, 0.9318181818181818, 0.9379562043795621, 0.9259259259259259, 0.9063670411985019, 0.915057915057915, 0.8927038626609443, 0.8976377952755905, 0.928030303030303, 0.9117647058823529, 0.9239543726235742, 0.9254901960784314, 0.9360902255639098, 0.8901515151515151, 0.9361702127659575, 0.9060150375939849, 0.9101123595505618, 0.9360902255639098, 0.875, 0.9262295081967213, 0.9153225806451613, 0.928, 0.9261992619926199, 0.9473684210526315, 0.933579335793358, 0.9053030303030303, 0.9277566539923955, 0.905511811023622, 0.9285714285714286, 0.9267399267399268, 0.9256505576208178, 0.9045936395759717, 0.9180327868852459, 0.9025270758122743, 0.9392857142857143, 0.937037037037037, 0.9228070175438596, 0.8904593639575972, 0.9172661870503597, 0.922509225092251, 0.9310344827586207, 0.9068825910931174, 0.9310344827586207, 0.9202898550724637, 0.89453125, 0.9616858237547893, 0.8954703832752613, 0.914396887159533, 0.9118773946360154, 0.9389312977099237, 0.9021739130434783, 0.9216417910447762, 0.9067164179104478, 0.9420849420849421, 0.9345454545454546, 0.948905109489051, 0.9356060606060606, 0.9328358208955224, 0.912, 0.875, 0.9042145593869731, 0.9132075471698113, 0.9175257731958762, 0.9315589353612167, 0.9283018867924528, 0.9195402298850575, 0.9176029962546817, 0.9111111111111111, 0.89272030651341, 0.921875, 0.8888888888888888, 0.9456521739130435, 0.9444444444444444, 0.9016393442622951, 0.9148936170212766, 0.9051094890510949, 0.8818565400843882, 0.8941605839416058, 0.9192307692307692, 0.8970099667774086, 0.9260700389105059, 0.9219330855018587, 0.9135338345864662, 0.912, 0.9128919860627178, 0.9393939393939394, 0.9471698113207547, 0.9201520912547528, 0.9256198347107438, 0.9222222222222223, 0.9227642276422764, 0.926923076923077, 0.9182879377431906, 0.925531914893617, 0.9047619047619048, 0.9127272727272727, 0.889763779527559, 0.8801498127340824, 0.9148148148148149, 0.9123505976095617, 0.9352226720647774, 0.9034749034749034, 0.9148936170212766, 0.9230769230769231, 0.8880866425992779, 0.9330855018587361, 0.9025270758122743, 0.9227941176470589, 0.9490909090909091, 0.8880597014925373, 0.9239543726235742, 0.9446494464944649, 0.9128787878787878, 0.9266409266409267, 0.9136690647482014, 0.9330708661417323, 0.8906882591093117, 0.9148936170212766, 0.8862745098039215, 0.915057915057915, 0.8918918918918919, 0.9196787148594378, 0.9320754716981132, 0.935361216730038, 0.9045801526717557, 0.9453125, 0.9206896551724137, 0.9133574007220217, 0.9191176470588235, 0.9186046511627907, 0.9222222222222223, 0.916058394160584, 0.9051094890510949, 0.9224489795918367, 0.909433962264151, 0.9139784946236559, 0.8988326848249028, 0.9166666666666666, 0.8902439024390244, 0.9175627240143369, 0.9036144578313253, 0.911660777385159, 0.9473684210526315, 0.9197080291970803, 0.9107142857142857, 0.8989169675090253, 0.924, 0.9225352112676056, 0.9304029304029304, 0.9127272727272727, 0.9250936329588015, 0.9019607843137255, 0.92, 0.9068100358422939, 0.943089430894309, 0.9423076923076923, 0.9465648854961832, 0.9256505576208178, 0.9087301587301587, 0.92578125, 0.8974358974358975, 0.940959409594096, 0.9094488188976378, 0.914179104477612, 0.9083969465648855, 0.9259259259259259, 0.875968992248062, 0.9477611940298507, 0.9210526315789473, 0.9259259259259259, 0.933852140077821, 0.9169675090252708, 0.9446494464944649, 0.9067164179104478, 0.9058823529411765, 0.9263157894736842, 0.93359375, 0.9330708661417323, 0.9138576779026217, 0.9036144578313253, 0.9253731343283582, 0.917910447761194, 0.9230769230769231, 0.9074074074074074, 0.88, 0.9185185185185185, 0.9210526315789473, 0.9409282700421941, 0.8924302788844621, 0.9077490774907749, 0.9325842696629213, 0.9022556390977443, 0.9142857142857143, 0.9377431906614786, 0.9498207885304659, 0.8970588235294118, 0.9032258064516129, 0.9135338345864662, 0.9343065693430657, 0.9058823529411765, 0.9330985915492958, 0.9133574007220217, 0.93359375, 0.9570815450643777, 0.9230769230769231, 0.8904109589041096, 0.9151943462897526, 0.9260700389105059, 0.9047619047619048, 0.9372549019607843, 0.9274809160305344, 0.9054545454545454, 0.9312977099236641, 0.900398406374502, 0.9534883720930233, 0.9264705882352942, 0.9325842696629213, 0.9568627450980393, 0.9084249084249084, 0.9305019305019305, 0.9166666666666666, 0.9341085271317829, 0.93359375, 0.932, 0.926829268292683, 0.9272727272727272, 0.9173228346456693, 0.937037037037037, 0.946969696969697, 0.9137254901960784, 0.8996138996138996, 0.9455782312925171, 0.9423868312757202, 0.937984496124031, 0.9157509157509157, 0.8955223880597015, 0.9063670411985019, 0.8937007874015748, 0.9330985915492958, 0.9230769230769231, 0.9198473282442748, 0.9090909090909091, 0.9190283400809717, 0.9071428571428571, 0.9116465863453815, 0.9263157894736842, 0.9196787148594378, 0.9101123595505618, 0.9169811320754717, 0.8955823293172691, 0.912, 0.8951310861423221, 0.9124087591240876, 0.9098039215686274, 0.9090909090909091, 0.9230769230769231, 0.8917910447761194, 0.922509225092251, 0.9216417910447762, 0.9154411764705882, 0.9037037037037037, 0.9045801526717557, 0.8867924528301887, 0.9328358208955224, 0.9055944055944056, 0.9080882352941176, 0.9135338345864662, 0.9115384615384615, 0.9094202898550725, 0.9133574007220217, 0.9011857707509882, 0.8863636363636364, 0.89568345323741, 0.9135338345864662, 0.9137254901960784, 0.9074074074074074, 0.9230769230769231, 0.9154411764705882, 0.914179104477612, 0.8910505836575876, 0.916030534351145, 0.9142857142857143, 0.9340659340659341, 0.8929889298892989, 0.9145907473309609, 0.9037037037037037, 0.8892988929889298, 0.9, 0.9578544061302682, 0.9301470588235294, 0.9144981412639405, 0.9212598425196851, 0.9236641221374046, 0.8782287822878229, 0.9243027888446215, 0.9045801526717557, 0.9083665338645418, 0.9122137404580153, 0.9078014184397163, 0.9384057971014492, 0.8901960784313725, 0.93359375, 0.9014598540145985, 0.93359375, 0.9034749034749034, 0.9346938775510204, 0.909433962264151, 0.9400749063670412, 0.9076305220883534, 0.9178571428571428, 0.8674698795180723, 0.8941176470588236, 0.91796875, 0.895910780669145, 0.9192307692307692, 0.9204545454545454, 0.9037037037037037, 0.9351145038167938, 0.91796875, 0.9221311475409836, 0.9350180505415162, 0.9122137404580153, 0.9314516129032258, 0.8854961832061069, 0.9423076923076923, 0.9204545454545454, 0.9015151515151515, 0.9425287356321839, 0.9277566539923955, 0.8859315589353612, 0.9386281588447654, 0.9400749063670412, 0.8984962406015038, 0.8875968992248062, 0.909433962264151, 0.9372693726937269, 0.9375, 0.9322709163346613, 0.9344262295081968, 0.9038461538461539, 0.9128787878787878, 0.9520295202952029, 0.9262295081967213, 0.9045801526717557, 0.945054945054945, 0.9358490566037736, 0.9341085271317829, 0.908745247148289, 0.937007874015748, 0.8909774436090225, 0.8980392156862745, 0.9215686274509803, 0.9330708661417323, 0.9049295774647887, 0.9224806201550387, 0.9296875, 0.92578125, 0.9058823529411765, 0.9166666666666666, 0.9063670411985019, 0.9042145593869731, 0.8949416342412452, 0.8893129770992366, 0.9251968503937008, 0.896, 0.9137254901960784, 0.9166666666666666, 0.8937728937728938, 0.9172932330827067, 0.936, 0.9037800687285223, 0.9138576779026217, 0.9094202898550725, 0.9362549800796812, 0.9197080291970803, 0.8869257950530035, 0.9372693726937269, 0.8828125, 0.922509225092251, 0.8941176470588236, 0.9029850746268657, 0.9299610894941635, 0.9094650205761317, 0.9444444444444444, 0.9326241134751773, 0.9360902255639098, 0.8996282527881041, 0.9423076923076923, 0.9418604651162791, 0.9514925373134329, 0.9302325581395349, 0.8981132075471698, 0.9298245614035088, 0.935251798561151, 0.8872727272727273, 0.8924731182795699, 0.9227642276422764, 0.8676470588235294, 0.9321428571428572, 0.9330985915492958, 0.9080882352941176, 0.9349593495934959, 0.953125, 0.9122137404580153, 0.9285714285714286, 0.911660777385159, 0.8953488372093024, 0.91015625, 0.9233716475095786, 0.9015151515151515, 0.9111111111111111, 0.9413919413919414, 0.9243027888446215, 0.9094488188976378, 0.9392712550607287, 0.897119341563786, 0.9032258064516129, 0.9350180505415162, 0.9224806201550387, 0.9029850746268657, 0.8945454545454545, 0.9260700389105059, 0.9429657794676806, 0.8876404494382022, 0.9173228346456693, 0.8931297709923665, 0.9207547169811321, 0.9247311827956989, 0.8884615384615384, 0.9230769230769231, 0.9042145593869731, 0.8968253968253969, 0.8947368421052632, 0.9192307692307692, 0.9135338345864662, 0.9083969465648855, 0.9242424242424242, 0.9247311827956989, 0.9310344827586207, 0.9056603773584906, 0.9188191881918819, 0.9215686274509803, 0.9182879377431906, 0.92, 0.9411764705882353]\n",
            "sp: [0.9100719424460432, 0.9022556390977443, 0.936, 0.9424778761061947, 0.8909090909090909, 0.9069767441860465, 0.9115384615384615, 0.8697318007662835, 0.91796875, 0.933852140077821, 0.8876404494382022, 0.9302325581395349, 0.8988326848249028, 0.9172932330827067, 0.9267399267399268, 0.9186046511627907, 0.8932384341637011, 0.8853046594982079, 0.926530612244898, 0.9144981412639405, 0.8843283582089553, 0.9307692307692308, 0.9011406844106464, 0.9104477611940298, 0.9014084507042254, 0.8971631205673759, 0.9029850746268657, 0.9087591240875912, 0.9060150375939849, 0.8984375, 0.9308943089430894, 0.9343629343629344, 0.9242424242424242, 0.9298892988929889, 0.8955223880597015, 0.9347826086956522, 0.9188191881918819, 0.9467680608365019, 0.9251968503937008, 0.9169811320754717, 0.9111111111111111, 0.9115384615384615, 0.9105691056910569, 0.9077490774907749, 0.9233576642335767, 0.9222614840989399, 0.9330708661417323, 0.8984375, 0.9318181818181818, 0.9142857142857143, 0.9169811320754717, 0.8851851851851852, 0.9400749063670412, 0.937037037037037, 0.916058394160584, 0.8969465648854962, 0.9584905660377359, 0.9257950530035336, 0.922509225092251, 0.9101123595505618, 0.9186046511627907, 0.9298892988929889, 0.9214285714285714, 0.927710843373494, 0.9067164179104478, 0.9507575757575758, 0.9409282700421941, 0.9384057971014492, 0.8856088560885609, 0.910958904109589, 0.9358490566037736, 0.8793774319066148, 0.9205776173285198, 0.9126984126984127, 0.903114186851211, 0.933579335793358, 0.945054945054945, 0.863849765258216, 0.9111111111111111, 0.9301470588235294, 0.9310344827586207, 0.9400749063670412, 0.9049429657794676, 0.9157894736842105, 0.915057915057915, 0.9024390243902439, 0.9516129032258065, 0.909433962264151, 0.9125475285171103, 0.926923076923077, 0.9147286821705426, 0.8870967741935484, 0.9389312977099237, 0.9390681003584229, 0.9354838709677419, 0.89453125, 0.9338235294117647, 0.9251968503937008, 0.917910447761194, 0.9296296296296296, 0.9056603773584906, 0.9233716475095786, 0.9104477611940298, 0.9100719424460432, 0.9398496240601504, 0.9139784946236559, 0.9291044776119403, 0.9358490566037736, 0.9377431906614786, 0.902127659574468, 0.8937007874015748, 0.90625, 0.9067164179104478, 0.8744588744588745, 0.9154411764705882, 0.9094076655052264, 0.9176029962546817, 0.9, 0.9262295081967213, 0.9259259259259259, 0.9285714285714286, 0.9114391143911439, 0.9304029304029304, 0.9069767441860465, 0.91796875, 0.9523809523809523, 0.9042145593869731, 0.9213483146067416, 0.9115384615384615, 0.9354838709677419, 0.8981818181818182, 0.8943396226415095, 0.92, 0.9296875, 0.9188191881918819, 0.9137254901960784, 0.9428571428571428, 0.9342560553633218, 0.934156378600823, 0.9477911646586346, 0.9305555555555556, 0.9548872180451128, 0.896797153024911, 0.952755905511811, 0.9306569343065694, 0.9027777777777778, 0.9358490566037736, 0.8896797153024911, 0.916030534351145, 0.9135338345864662, 0.89272030651341, 0.9083969465648855, 0.9227642276422764, 0.9111111111111111, 0.914396887159533, 0.9114391143911439, 0.9172661870503597, 0.891566265060241, 0.9267399267399268, 0.9356060606060606, 0.9153225806451613, 0.9285714285714286, 0.9288389513108615, 0.9285714285714286, 0.9, 0.9029850746268657, 0.924187725631769, 0.8941605839416058, 0.896, 0.9230769230769231, 0.8933823529411765, 0.9172932330827067, 0.940959409594096, 0.9166666666666666, 0.9330543933054394, 0.9056603773584906, 0.9063670411985019, 0.8914728682170543, 0.9312977099236641, 0.9175257731958762, 0.899641577060932, 0.9176029962546817, 0.9139344262295082, 0.896, 0.9097744360902256, 0.9090909090909091, 0.9157509157509157, 0.9108527131782945, 0.8892988929889298, 0.9300411522633745, 0.9398496240601504, 0.9124087591240876, 0.8953068592057761, 0.8647540983606558, 0.8932806324110671, 0.9180327868852459, 0.9267399267399268, 0.9267399267399268, 0.9207547169811321, 0.926829268292683, 0.9473684210526315, 0.9204545454545454, 0.8969465648854962, 0.9028776978417267, 0.9011406844106464, 0.9395161290322581, 0.9758064516129032, 0.9215686274509803, 0.9298892988929889, 0.9274809160305344, 0.9415807560137457, 0.8821428571428571, 0.9266409266409267, 0.9384615384615385, 0.9007633587786259, 0.9061371841155235, 0.9398496240601504, 0.9236363636363636, 0.9118773946360154, 0.928, 0.9014598540145985, 0.8873239436619719, 0.9236641221374046, 0.8823529411764706, 0.9018867924528302, 0.9291338582677166, 0.9204545454545454, 0.932, 0.9067164179104478, 0.9037037037037037, 0.9252669039145908, 0.92578125, 0.9372549019607843, 0.9259259259259259, 0.8849206349206349, 0.9148148148148149, 0.9064748201438849, 0.9166666666666666, 0.9239130434782609, 0.9007936507936508, 0.9330708661417323, 0.9204545454545454, 0.8817204301075269, 0.90625, 0.9272030651340997, 0.8951612903225806, 0.93359375, 0.9218106995884774, 0.9317269076305221, 0.9111969111969112, 0.912, 0.9285714285714286, 0.9204545454545454, 0.9045936395759717, 0.9194139194139194, 0.907258064516129, 0.9201520912547528, 0.9228070175438596, 0.945054945054945, 0.9183673469387755, 0.9450980392156862, 0.8819188191881919, 0.929368029739777, 0.9340659340659341, 0.9395017793594306, 0.9343629343629344, 0.9653846153846154, 0.9423076923076923, 0.9182156133828996, 0.9448818897637795, 0.8995983935742972, 0.9173228346456693, 0.9, 0.8964285714285715, 0.8981818181818182, 0.9219858156028369, 0.9, 0.9547169811320755, 0.9325842696629213, 0.9300411522633745, 0.9007633587786259, 0.9018867924528302, 0.9071729957805907, 0.9060150375939849, 0.9094076655052264, 0.9320754716981132, 0.9007633587786259, 0.9327731092436975, 0.8988326848249028, 0.9356060606060606, 0.916058394160584, 0.9116465863453815, 0.9034749034749034, 0.9034749034749034, 0.917910447761194, 0.9098039215686274, 0.9133574007220217, 0.9377289377289377, 0.9309090909090909, 0.9545454545454546, 0.8969465648854962, 0.9283018867924528, 0.929368029739777, 0.9185185185185185, 0.9272030651340997, 0.921875, 0.9366197183098591, 0.8819444444444444, 0.9330357142857143, 0.9182156133828996, 0.9043824701195219, 0.9227941176470589, 0.9185185185185185, 0.9325396825396826, 0.9065040650406504, 0.8966789667896679, 0.9056603773584906, 0.8863636363636364, 0.89272030651341, 0.9100719424460432, 0.8995983935742972, 0.9101123595505618, 0.9518518518518518, 0.915057915057915, 0.9185185185185185, 0.9420289855072463, 0.9330855018587361, 0.91701244813278, 0.8996282527881041, 0.9182879377431906, 0.94140625, 0.9012345679012346, 0.929368029739777, 0.9026217228464419, 0.9393939393939394, 0.9481481481481482, 0.9390681003584229, 0.9084507042253521, 0.920863309352518, 0.8921933085501859, 0.900398406374502, 0.8806584362139918, 0.9288389513108615, 0.9239543726235742, 0.9049586776859504, 0.9124087591240876, 0.8992537313432836, 0.9122137404580153, 0.9003690036900369, 0.9097472924187726, 0.921146953405018, 0.9195402298850575, 0.9166666666666666, 0.9101123595505618, 0.920863309352518, 0.8939393939393939, 0.9201520912547528, 0.9485294117647058, 0.9094488188976378, 0.9301470588235294, 0.9272030651340997, 0.9416342412451362, 0.9124087591240876, 0.909433962264151, 0.9239543726235742, 0.9354838709677419, 0.8924302788844621, 0.8901960784313725, 0.8977272727272727, 0.9307692307692308, 0.9068100358422939, 0.9452554744525548, 0.9291044776119403, 0.908745247148289, 0.8774703557312253, 0.9067164179104478, 0.915057915057915, 0.8778625954198473, 0.9138576779026217, 0.896797153024911, 0.9411764705882353, 0.9157509157509157, 0.9264705882352942, 0.8784722222222222, 0.917910447761194, 0.9090909090909091, 0.9407665505226481, 0.9236641221374046, 0.9067164179104478, 0.894927536231884, 0.9043824701195219, 0.889763779527559, 0.9347826086956522, 0.9101123595505618, 0.9213483146067416, 0.9294117647058824, 0.8929889298892989, 0.9446494464944649, 0.9, 0.9293286219081273, 0.8817204301075269, 0.8873239436619719, 0.9007352941176471, 0.9233576642335767, 0.9352226720647774, 0.926056338028169, 0.9058823529411765, 0.916058394160584, 0.9250936329588015, 0.9236947791164659, 0.9375, 0.9358490566037736, 0.9506172839506173, 0.9250936329588015, 0.9191176470588235, 0.9457364341085271, 0.9104477611940298, 0.8555133079847909, 0.9137254901960784, 0.8939393939393939, 0.9120879120879121, 0.8920863309352518, 0.889763779527559, 0.9213483146067416, 0.9024390243902439, 0.9584905660377359, 0.933579335793358, 0.9007633587786259, 0.9242424242424242, 0.9034749034749034, 0.9007352941176471, 0.9247311827956989, 0.9330708661417323, 0.9259259259259259, 0.8992537313432836, 0.9053030303030303, 0.9117647058823529, 0.9038461538461539, 0.9264705882352942, 0.9083969465648855, 0.8888888888888888, 0.9431818181818182, 0.8893129770992366, 0.9343065693430657, 0.9107806691449815, 0.931899641577061, 0.9227941176470589, 0.9137254901960784, 0.9367588932806324, 0.9090909090909091, 0.8996282527881041, 0.8973384030418251, 0.9083333333333333, 0.9348659003831418, 0.9277566539923955, 0.9393939393939394, 0.9291044776119403, 0.9163498098859315, 0.9245283018867925, 0.9198473282442748, 0.8851851851851852, 0.9133858267716536, 0.8828125, 0.9051383399209486, 0.9296296296296296, 0.9087301587301587, 0.9246031746031746, 0.9624060150375939, 0.9040590405904059, 0.9153846153846154, 0.9068100358422939, 0.9438202247191011, 0.8955223880597015, 0.9027237354085603, 0.90625, 0.9069767441860465, 0.9194139194139194, 0.9142857142857143, 0.9188191881918819, 0.899641577060932, 0.8695652173913043, 0.9242424242424242, 0.914396887159533, 0.9149797570850202, 0.9154411764705882, 0.9288389513108615, 0.9224806201550387, 0.917910447761194, 0.9280575539568345, 0.9142857142857143, 0.9246031746031746, 0.946969696969697, 0.9212598425196851, 0.908745247148289, 0.946236559139785, 0.89272030651341, 0.926923076923077, 0.9382239382239382, 0.9246575342465754, 0.9137254901960784, 0.8973384030418251, 0.8929889298892989, 0.9320754716981132, 0.8983739837398373, 0.9015151515151515, 0.9242424242424242, 0.9192307692307692, 0.8931297709923665, 0.9477911646586346, 0.931899641577061, 0.9203187250996016, 0.9037037037037037, 0.914396887159533, 0.9333333333333333, 0.8571428571428571, 0.8953488372093024, 0.9392712550607287, 0.9014084507042254, 0.9288256227758007, 0.9101123595505618, 0.9130434782608695, 0.9311594202898551, 0.9558232931726908, 0.9094202898550725, 0.8901960784313725, 0.8992248062015504, 0.9133858267716536, 0.9035714285714286, 0.8876404494382022, 0.9294117647058824, 0.8814814814814815, 0.9003690036900369, 0.9444444444444444, 0.9090909090909091, 0.9636363636363636, 0.928030303030303, 0.8818897637795275, 0.9185185185185185, 0.9224806201550387, 0.9261992619926199, 0.946969696969697, 0.9011406844106464, 0.9245283018867925, 0.9325842696629213, 0.9288256227758007, 0.9106529209621993, 0.9125475285171103, 0.8962962962962963, 0.9230769230769231, 0.9333333333333333, 0.9007633587786259, 0.9163636363636364, 0.948, 0.9382239382239382, 0.924187725631769, 0.9263565891472868, 0.9318181818181818, 0.9379562043795621, 0.9259259259259259, 0.9063670411985019, 0.915057915057915, 0.8927038626609443, 0.8976377952755905, 0.928030303030303, 0.9117647058823529, 0.9239543726235742, 0.9254901960784314, 0.9360902255639098, 0.8901515151515151, 0.9361702127659575, 0.9060150375939849, 0.9101123595505618, 0.9360902255639098, 0.875, 0.9262295081967213, 0.9153225806451613, 0.928, 0.9261992619926199, 0.9473684210526315, 0.933579335793358, 0.9053030303030303, 0.9277566539923955, 0.905511811023622, 0.9285714285714286, 0.9267399267399268, 0.9256505576208178, 0.9045936395759717, 0.9180327868852459, 0.9025270758122743, 0.9392857142857143, 0.937037037037037, 0.9228070175438596, 0.8904593639575972, 0.9172661870503597, 0.922509225092251, 0.9310344827586207, 0.9068825910931174, 0.9310344827586207, 0.9202898550724637, 0.89453125, 0.9616858237547893, 0.8954703832752613, 0.914396887159533, 0.9118773946360154, 0.9389312977099237, 0.9021739130434783, 0.9216417910447762, 0.9067164179104478, 0.9420849420849421, 0.9345454545454546, 0.948905109489051, 0.9356060606060606, 0.9328358208955224, 0.912, 0.875, 0.9042145593869731, 0.9132075471698113, 0.9175257731958762, 0.9315589353612167, 0.9283018867924528, 0.9195402298850575, 0.9176029962546817, 0.9111111111111111, 0.89272030651341, 0.921875, 0.8888888888888888, 0.9456521739130435, 0.9444444444444444, 0.9016393442622951, 0.9148936170212766, 0.9051094890510949, 0.8818565400843882, 0.8941605839416058, 0.9192307692307692, 0.8970099667774086, 0.9260700389105059, 0.9219330855018587, 0.9135338345864662, 0.912, 0.9128919860627178, 0.9393939393939394, 0.9471698113207547, 0.9201520912547528, 0.9256198347107438, 0.9222222222222223, 0.9227642276422764, 0.926923076923077, 0.9182879377431906, 0.925531914893617, 0.9047619047619048, 0.9127272727272727, 0.889763779527559, 0.8801498127340824, 0.9148148148148149, 0.9123505976095617, 0.9352226720647774, 0.9034749034749034, 0.9148936170212766, 0.9230769230769231, 0.8880866425992779, 0.9330855018587361, 0.9025270758122743, 0.9227941176470589, 0.9490909090909091, 0.8880597014925373, 0.9239543726235742, 0.9446494464944649, 0.9128787878787878, 0.9266409266409267, 0.9136690647482014, 0.9330708661417323, 0.8906882591093117, 0.9148936170212766, 0.8862745098039215, 0.915057915057915, 0.8918918918918919, 0.9196787148594378, 0.9320754716981132, 0.935361216730038, 0.9045801526717557, 0.9453125, 0.9206896551724137, 0.9133574007220217, 0.9191176470588235, 0.9186046511627907, 0.9222222222222223, 0.916058394160584, 0.9051094890510949, 0.9224489795918367, 0.909433962264151, 0.9139784946236559, 0.8988326848249028, 0.9166666666666666, 0.8902439024390244, 0.9175627240143369, 0.9036144578313253, 0.911660777385159, 0.9473684210526315, 0.9197080291970803, 0.9107142857142857, 0.8989169675090253, 0.924, 0.9225352112676056, 0.9304029304029304, 0.9127272727272727, 0.9250936329588015, 0.9019607843137255, 0.92, 0.9068100358422939, 0.943089430894309, 0.9423076923076923, 0.9465648854961832, 0.9256505576208178, 0.9087301587301587, 0.92578125, 0.8974358974358975, 0.940959409594096, 0.9094488188976378, 0.914179104477612, 0.9083969465648855, 0.9259259259259259, 0.875968992248062, 0.9477611940298507, 0.9210526315789473, 0.9259259259259259, 0.933852140077821, 0.9169675090252708, 0.9446494464944649, 0.9067164179104478, 0.9058823529411765, 0.9263157894736842, 0.93359375, 0.9330708661417323, 0.9138576779026217, 0.9036144578313253, 0.9253731343283582, 0.917910447761194, 0.9230769230769231, 0.9074074074074074, 0.88, 0.9185185185185185, 0.9210526315789473, 0.9409282700421941, 0.8924302788844621, 0.9077490774907749, 0.9325842696629213, 0.9022556390977443, 0.9142857142857143, 0.9377431906614786, 0.9498207885304659, 0.8970588235294118, 0.9032258064516129, 0.9135338345864662, 0.9343065693430657, 0.9058823529411765, 0.9330985915492958, 0.9133574007220217, 0.93359375, 0.9570815450643777, 0.9230769230769231, 0.8904109589041096, 0.9151943462897526, 0.9260700389105059, 0.9047619047619048, 0.9372549019607843, 0.9274809160305344, 0.9054545454545454, 0.9312977099236641, 0.900398406374502, 0.9534883720930233, 0.9264705882352942, 0.9325842696629213, 0.9568627450980393, 0.9084249084249084, 0.9305019305019305, 0.9166666666666666, 0.9341085271317829, 0.93359375, 0.932, 0.926829268292683, 0.9272727272727272, 0.9173228346456693, 0.937037037037037, 0.946969696969697, 0.9137254901960784, 0.8996138996138996, 0.9455782312925171, 0.9423868312757202, 0.937984496124031, 0.9157509157509157, 0.8955223880597015, 0.9063670411985019, 0.8937007874015748, 0.9330985915492958, 0.9230769230769231, 0.9198473282442748, 0.9090909090909091, 0.9190283400809717, 0.9071428571428571, 0.9116465863453815, 0.9263157894736842, 0.9196787148594378, 0.9101123595505618, 0.9169811320754717, 0.8955823293172691, 0.912, 0.8951310861423221, 0.9124087591240876, 0.9098039215686274, 0.9090909090909091, 0.9230769230769231, 0.8917910447761194, 0.922509225092251, 0.9216417910447762, 0.9154411764705882, 0.9037037037037037, 0.9045801526717557, 0.8867924528301887, 0.9328358208955224, 0.9055944055944056, 0.9080882352941176, 0.9135338345864662, 0.9115384615384615, 0.9094202898550725, 0.9133574007220217, 0.9011857707509882, 0.8863636363636364, 0.89568345323741, 0.9135338345864662, 0.9137254901960784, 0.9074074074074074, 0.9230769230769231, 0.9154411764705882, 0.914179104477612, 0.8910505836575876, 0.916030534351145, 0.9142857142857143, 0.9340659340659341, 0.8929889298892989, 0.9145907473309609, 0.9037037037037037, 0.8892988929889298, 0.9, 0.9578544061302682, 0.9301470588235294, 0.9144981412639405, 0.9212598425196851, 0.9236641221374046, 0.8782287822878229, 0.9243027888446215, 0.9045801526717557, 0.9083665338645418, 0.9122137404580153, 0.9078014184397163, 0.9384057971014492, 0.8901960784313725, 0.93359375, 0.9014598540145985, 0.93359375, 0.9034749034749034, 0.9346938775510204, 0.909433962264151, 0.9400749063670412, 0.9076305220883534, 0.9178571428571428, 0.8674698795180723, 0.8941176470588236, 0.91796875, 0.895910780669145, 0.9192307692307692, 0.9204545454545454, 0.9037037037037037, 0.9351145038167938, 0.91796875, 0.9221311475409836, 0.9350180505415162, 0.9122137404580153, 0.9314516129032258, 0.8854961832061069, 0.9423076923076923, 0.9204545454545454, 0.9015151515151515, 0.9425287356321839, 0.9277566539923955, 0.8859315589353612, 0.9386281588447654, 0.9400749063670412, 0.8984962406015038, 0.8875968992248062, 0.909433962264151, 0.9372693726937269, 0.9375, 0.9322709163346613, 0.9344262295081968, 0.9038461538461539, 0.9128787878787878, 0.9520295202952029, 0.9262295081967213, 0.9045801526717557, 0.945054945054945, 0.9358490566037736, 0.9341085271317829, 0.908745247148289, 0.937007874015748, 0.8909774436090225, 0.8980392156862745, 0.9215686274509803, 0.9330708661417323, 0.9049295774647887, 0.9224806201550387, 0.9296875, 0.92578125, 0.9058823529411765, 0.9166666666666666, 0.9063670411985019, 0.9042145593869731, 0.8949416342412452, 0.8893129770992366, 0.9251968503937008, 0.896, 0.9137254901960784, 0.9166666666666666, 0.8937728937728938, 0.9172932330827067, 0.936, 0.9037800687285223, 0.9138576779026217, 0.9094202898550725, 0.9362549800796812, 0.9197080291970803, 0.8869257950530035, 0.9372693726937269, 0.8828125, 0.922509225092251, 0.8941176470588236, 0.9029850746268657, 0.9299610894941635, 0.9094650205761317, 0.9444444444444444, 0.9326241134751773, 0.9360902255639098, 0.8996282527881041, 0.9423076923076923, 0.9418604651162791, 0.9514925373134329, 0.9302325581395349, 0.8981132075471698, 0.9298245614035088, 0.935251798561151, 0.8872727272727273, 0.8924731182795699, 0.9227642276422764, 0.8676470588235294, 0.9321428571428572, 0.9330985915492958, 0.9080882352941176, 0.9349593495934959, 0.953125, 0.9122137404580153, 0.9285714285714286, 0.911660777385159, 0.8953488372093024, 0.91015625, 0.9233716475095786, 0.9015151515151515, 0.9111111111111111, 0.9413919413919414, 0.9243027888446215, 0.9094488188976378, 0.9392712550607287, 0.897119341563786, 0.9032258064516129, 0.9350180505415162, 0.9224806201550387, 0.9029850746268657, 0.8945454545454545, 0.9260700389105059, 0.9429657794676806, 0.8876404494382022, 0.9173228346456693, 0.8931297709923665, 0.9207547169811321, 0.9247311827956989, 0.8884615384615384, 0.9230769230769231, 0.9042145593869731, 0.8968253968253969, 0.8947368421052632, 0.9192307692307692, 0.9135338345864662, 0.9083969465648855, 0.9242424242424242, 0.9247311827956989, 0.9310344827586207, 0.9056603773584906, 0.9188191881918819, 0.9215686274509803, 0.9182879377431906, 0.92, 0.9411764705882353]\n",
            "sd_acc: 0.011006159478120154\n",
            "sd_f1: 0.011640122823134313\n",
            "sd_mcc: 0.021909374969213254\n",
            "sd_sn: 0.01769744536125409\n",
            "sd_sp: 0.01769744536125409\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.95      0.93       265\n",
            "           1       0.95      0.92      0.93       265\n",
            "\n",
            "    accuracy                           0.93       530\n",
            "   macro avg       0.93      0.93      0.93       530\n",
            "weighted avg       0.93      0.93      0.93       530\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.011006159478120154, 0.021909374969213254, 0.011640122823134313)"
            ]
          },
          "execution_count": 136,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predicted_training_labels = classifier.predict(train_dataset)\n",
        "error_rate(training_labels, predicted_training_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6XZ9vilAJoD",
        "outputId": "6389fb31-991c-40c3-9509-f9b09f11d0c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Our model has an accuracy of 0.85\n"
          ]
        }
      ],
      "source": [
        "# Basic Protocol 3 — Step 10\n",
        "\n",
        "predicted_mlp = classifier.predict(test_dataset)\n",
        "accuracy = accuracy_score(testing_labels, predicted_mlp)\n",
        "\n",
        "print(f\"Our model has an accuracy of {accuracy:.2}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PS3MxVMkSRXK",
        "outputId": "e3bcb320-dd99-4778-b61c-85a07de7b54f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "acc: 0.8494342906875544\n",
            "f1: 0.8883150419625565\n",
            "mcc: 0.6694452535921178\n",
            "sn: [0.8310727496917386, 0.8561320754716981, 0.8300970873786407, 0.8528678304239401, 0.8407720144752714, 0.8656174334140436, 0.85375, 0.8478260869565217, 0.8496815286624204, 0.8247298919567827, 0.8214716525934861, 0.8540880503144654, 0.8470873786407767, 0.8376068376068376, 0.806060606060606, 0.8256097560975609, 0.8431845597104946, 0.811965811965812, 0.8516687268232386, 0.8501228501228502, 0.8446601941747572, 0.8461538461538461, 0.808252427184466, 0.8439803439803439, 0.820125786163522, 0.8262454434993924, 0.8297619047619048, 0.8418491484184915, 0.84375, 0.8365853658536585, 0.855036855036855, 0.838107098381071, 0.8224076281287247, 0.8434886499402628, 0.8307134220072552, 0.8557692307692307, 0.8257668711656442, 0.8514970059880239, 0.8291215403128761, 0.851140456182473, 0.850546780072904, 0.8376068376068376, 0.823030303030303, 0.8364779874213837, 0.8168316831683168, 0.8424242424242424, 0.8501827040194885, 0.8463414634146341, 0.8590522478736331, 0.8409937888198757, 0.8467153284671532, 0.8382352941176471, 0.8228915662650602, 0.855, 0.8368098159509203, 0.8351920693928129, 0.8438661710037175, 0.8451053283767038, 0.8329238329238329, 0.8229548229548229, 0.8395209580838323, 0.8404907975460123, 0.8411552346570397, 0.8436724565756824, 0.8421052631578947, 0.8588807785888077, 0.8172959805115713, 0.8288177339901478, 0.8357843137254902, 0.86, 0.8544152744630071, 0.8193384223918575, 0.8531994981179423, 0.8465227817745803, 0.8528678304239401, 0.8412887828162291, 0.8317073170731707, 0.8727272727272727, 0.8556085918854416, 0.8424317617866005, 0.8465286236297198, 0.831942789034565, 0.8372093023255814, 0.8415366146458584, 0.8502923976608188, 0.8415094339622642, 0.8321342925659473, 0.8453364817001181, 0.8335403726708075, 0.824390243902439, 0.8394332939787486, 0.8692033293697978, 0.8607442977190877, 0.851985559566787, 0.8392204628501827, 0.8247162673392182, 0.8618925831202046, 0.8548387096774194, 0.8377403846153846, 0.8425584255842559, 0.850546780072904, 0.8223039215686274, 0.8428053204353083, 0.8339622641509434, 0.8309352517985612, 0.8295739348370927, 0.8649289099526066, 0.8508771929824561, 0.8266331658291457, 0.8568019093078759, 0.8557213930348259, 0.8592057761732852, 0.823019801980198, 0.8522167487684729, 0.8569682151589242, 0.8456937799043063, 0.8552472858866104, 0.8261405672009864, 0.8471177944862155, 0.8498817966903073, 0.8242424242424242, 0.8351126927639383, 0.82875, 0.8439086294416244, 0.8546583850931677, 0.8502415458937198, 0.8335451080050826, 0.8060975609756098, 0.8463414634146341, 0.8514357053682896, 0.8205128205128205, 0.8633540372670807, 0.8364083640836408, 0.8365384615384616, 0.8317307692307693, 0.830568720379147, 0.8554070473876063, 0.8366336633663366, 0.8214285714285714, 0.8449519230769231, 0.8442028985507246, 0.8243243243243243, 0.8212157330154947, 0.8474784747847478, 0.8327183271832719, 0.8545006165228114, 0.8270401948842875, 0.8497596153846154, 0.8294190358467244, 0.8348045397225725, 0.8549810844892812, 0.845679012345679, 0.8345498783454988, 0.8498817966903073, 0.8426698450536353, 0.8195488721804511, 0.8286755771567437, 0.8513011152416357, 0.8592411260709915, 0.8503740648379052, 0.8445859872611465, 0.8353960396039604, 0.8101736972704715, 0.8181818181818182, 0.8477466504263094, 0.8523409363745498, 0.8245614035087719, 0.8211678832116789, 0.8618181818181818, 0.8048484848484848, 0.8460591133004927, 0.8337468982630273, 0.8257756563245824, 0.8428053204353083, 0.8511979823455234, 0.8397515527950311, 0.8606060606060606, 0.8375757575757575, 0.8420413122721749, 0.864321608040201, 0.8387878787878787, 0.8156288156288156, 0.8349633251833741, 0.8437118437118437, 0.8420398009950248, 0.8583850931677018, 0.8608374384236454, 0.8496894409937888, 0.8372379778051788, 0.8493975903614458, 0.8515439429928741, 0.8331257783312578, 0.8406326034063261, 0.827710843373494, 0.8327272727272728, 0.8366583541147132, 0.8444165621079046, 0.8422939068100358, 0.8394768133174791, 0.8615196078431373, 0.8319018404907975, 0.8358913813459268, 0.8247914183551848, 0.8393719806763285, 0.8478527607361963, 0.8376068376068376, 0.8288508557457213, 0.8466257668711656, 0.8281444582814446, 0.8449328449328449, 0.8404384896467723, 0.8574938574938575, 0.8082026537997588, 0.8333333333333334, 0.8302354399008675, 0.8396572827417381, 0.8586956521739131, 0.8580097087378641, 0.828641370869033, 0.8508771929824561, 0.8329238329238329, 0.8626237623762376, 0.8196517412935324, 0.8138424821002387, 0.8522167487684729, 0.8396674584323041, 0.8480697384806973, 0.8538083538083538, 0.8647990255785627, 0.8306351183063512, 0.8381430363864492, 0.8294478527607362, 0.8131313131313131, 0.8514492753623188, 0.8416458852867831, 0.8166047087980174, 0.8618654073199528, 0.8418491484184915, 0.8416565164433617, 0.8383594692400482, 0.8353658536585366, 0.8578255675029869, 0.8485221674876847, 0.8341645885286783, 0.8488943488943489, 0.8522050059594756, 0.833134684147795, 0.8464373464373465, 0.8433734939759037, 0.8285714285714286, 0.8363417569193743, 0.8528347406513872, 0.83270911360799, 0.8260869565217391, 0.847394540942928, 0.8423028785982478, 0.8085365853658537, 0.8345679012345679, 0.83375, 0.8521212121212122, 0.8346055979643766, 0.845398773006135, 0.8539976825028969, 0.8383961117861483, 0.8116959064327486, 0.822360248447205, 0.8491484184914841, 0.8322903629536921, 0.8451776649746193, 0.8624401913875598, 0.8087167070217918, 0.8440145102781137, 0.8473091364205256, 0.8515337423312883, 0.8319018404907975, 0.8402948402948403, 0.8576555023923444, 0.8500604594921403, 0.863030303030303, 0.8518072289156626, 0.8663366336633663, 0.845223700120919, 0.8467153284671532, 0.8498759305210918, 0.832541567695962, 0.8331332533013205, 0.8378712871287128, 0.8430160692212608, 0.850187265917603, 0.8454106280193237, 0.8233173076923077, 0.8230088495575221, 0.8424908424908425, 0.8619824341279799, 0.8355501813784765, 0.8327359617682198, 0.8428745432399513, 0.850356294536817, 0.8333333333333334, 0.8700980392156863, 0.8139255702280912, 0.8162291169451074, 0.8310387984981227, 0.8518971848225214, 0.8585732165206508, 0.8365617433414043, 0.8428571428571429, 0.837378640776699, 0.845088161209068, 0.8463414634146341, 0.8507281553398058, 0.856272838002436, 0.8264058679706602, 0.8478802992518704, 0.8405275779376499, 0.8333333333333334, 0.8403361344537815, 0.8765133171912833, 0.828009828009828, 0.82875, 0.8486293206197855, 0.8223289315726291, 0.8331288343558282, 0.8424821002386634, 0.8424242424242424, 0.8473748473748474, 0.8293601003764115, 0.8301435406698564, 0.8233861144945189, 0.8347718865598027, 0.8748510131108462, 0.8466334164588528, 0.8363636363636363, 0.8368355995055624, 0.8505603985056039, 0.8407079646017699, 0.8356973995271868, 0.8261390887290168, 0.833130328867235, 0.8351126927639383, 0.8519417475728155, 0.8403361344537815, 0.8616504854368932, 0.8282208588957055, 0.8400488400488401, 0.8525798525798526, 0.8461538461538461, 0.8391959798994975, 0.8471720818291215, 0.8491484184914841, 0.8459657701711492, 0.8359659781287971, 0.8491965389369592, 0.8421052631578947, 0.8343789209535759, 0.8429447852760736, 0.8656174334140436, 0.8437118437118437, 0.8217821782178217, 0.82625, 0.8199753390875463, 0.8660933660933661, 0.8415094339622642, 0.8557575757575757, 0.8523153942428036, 0.8388625592417062, 0.8378378378378378, 0.8499399759903962, 0.8438287153652393, 0.8446362515413071, 0.8175, 0.8233830845771144, 0.8348946135831382, 0.8475390156062425, 0.8559423769507803, 0.8650234741784038, 0.8450363196125908, 0.8666666666666667, 0.8555825242718447, 0.8300492610837439, 0.8488943488943489, 0.8524203069657615, 0.8403869407496977, 0.8323353293413174, 0.8515723270440252, 0.8281053952321205, 0.8531553398058253, 0.8443337484433375, 0.8327272727272728, 0.8331332533013205, 0.8371810449574727, 0.8187422934648582, 0.8243080625752106, 0.7995079950799509, 0.8375959079283888, 0.8317422434367542, 0.8418491484184915, 0.8423707440100883, 0.8357314148681055, 0.8447432762836186, 0.8381642512077294, 0.8516687268232386, 0.8408262454434994, 0.8410194174757282, 0.8455882352941176, 0.8422330097087378, 0.8567990373044525, 0.8312655086848635, 0.8602520045819015, 0.8383594692400482, 0.8618654073199528, 0.8653846153846154, 0.809102402022756, 0.8407185628742515, 0.8511166253101737, 0.8341645885286783, 0.8424821002386634, 0.8416075650118203, 0.8431137724550898, 0.816953316953317, 0.8433451118963486, 0.8425480769230769, 0.8192918192918193, 0.8434343434343434, 0.8264669163545568, 0.857849196538937, 0.8467153284671532, 0.8451536643026005, 0.8379052369077307, 0.8558786346396966, 0.8462469733656174, 0.8110709987966306, 0.8403263403263403, 0.8482587064676617, 0.8661710037174721, 0.866421568627451, 0.8482252141982864, 0.8430379746835444, 0.8493827160493828, 0.8236775818639799, 0.8539603960396039, 0.8460620525059666, 0.843236409608091, 0.85, 0.8449131513647643, 0.857487922705314, 0.8364083640836408, 0.8310893512851897, 0.8237410071942446, 0.8358585858585859, 0.8587223587223587, 0.8207663782447466, 0.8380614657210402, 0.8522167487684729, 0.8327183271832719, 0.8349753694581281, 0.8202653799758746, 0.8337408312958435, 0.8208955223880597, 0.850546780072904, 0.8335388409371147, 0.8646341463414634, 0.8236024844720496, 0.8670731707317073, 0.8377358490566038, 0.8323207776427703, 0.823019801980198, 0.8414481897627965, 0.8203221809169765, 0.8478002378121284, 0.8322903629536921, 0.8319226118500604, 0.8424821002386634, 0.8341463414634146, 0.8353365384615384, 0.844059405940594, 0.8366093366093366, 0.8524788391777509, 0.8134328358208955, 0.8343409915356711, 0.8216867469879519, 0.8483353884093712, 0.8022113022113022, 0.8403990024937655, 0.8412698412698413, 0.8319226118500604, 0.8270858524788391, 0.821831869510665, 0.8440925700365408, 0.8335403726708075, 0.8327228327228328, 0.8290398126463701, 0.8302808302808303, 0.8374384236453202, 0.8331332533013205, 0.8385542168674699, 0.833530106257379, 0.8484107579462102, 0.8329268292682926, 0.8463414634146341, 0.8351515151515152, 0.8412698412698413, 0.8487084870848709, 0.83, 0.8335287221570926, 0.8309352517985612, 0.8566084788029925, 0.8396111786148238, 0.86125, 0.84, 0.8441890166028098, 0.8438661710037175, 0.8256658595641646, 0.8185185185185185, 0.8345771144278606, 0.8449328449328449, 0.8528301886792453, 0.8636911942098915, 0.8155339805825242, 0.8262910798122066, 0.8667481662591687, 0.8319123020706456, 0.8365384615384616, 0.829683698296837, 0.8580024067388689, 0.8349633251833741, 0.8397129186602871, 0.8465286236297198, 0.8496894409937888, 0.8370098039215687, 0.8509852216748769, 0.8381995133819952, 0.8259303721488596, 0.8366583541147132, 0.8525402726146221, 0.8322981366459627, 0.8410194174757282, 0.8318804483188045, 0.8375451263537906, 0.8600478468899522, 0.8548387096774194, 0.8606965174129353, 0.8275862068965517, 0.8583850931677018, 0.8388683886838868, 0.8481481481481481, 0.8188494492044064, 0.8343057176196033, 0.8034610630407911, 0.8212974296205631, 0.8450184501845018, 0.8313397129186603, 0.8199052132701422, 0.8303030303030303, 0.8370098039215687, 0.7995018679950187, 0.8553299492385786, 0.8493150684931506, 0.8497596153846154, 0.834319526627219, 0.8506172839506173, 0.8533007334963325, 0.82625, 0.8507462686567164, 0.8604060913705583, 0.8561064087061668, 0.8418549346016647, 0.8445532435740514, 0.8416458852867831, 0.8431845597104946, 0.8566131025957973, 0.8423645320197044, 0.8490566037735849, 0.8573170731707317, 0.8453105968331304, 0.8230088495575221, 0.8416458852867831, 0.8321428571428572, 0.8461538461538461, 0.8175182481751825, 0.8478260869565217, 0.8298906439854192, 0.8323353293413174, 0.857487922705314, 0.8587360594795539, 0.8402366863905325, 0.8387096774193549, 0.8476190476190476, 0.8488943488943489, 0.838107098381071, 0.820543093270366, 0.8482252141982864, 0.8333333333333334, 0.8339483394833949, 0.83, 0.845398773006135, 0.8431372549019608, 0.8495788206979543, 0.8626237623762376, 0.8365853658536585, 0.852760736196319, 0.8391356542617047, 0.8407960199004975, 0.8385922330097088, 0.8506024096385543, 0.8604091456077015, 0.8266033254156769, 0.8513011152416357, 0.8446969696969697, 0.8496894409937888, 0.8498168498168498, 0.8289473684210527, 0.8357843137254902, 0.8418491484184915, 0.8486682808716707, 0.8588957055214724, 0.8415366146458584, 0.8381294964028777, 0.8539458186101295, 0.8547008547008547, 0.8531139835487661, 0.8532675709001233, 0.8649318463444857, 0.8481012658227848, 0.8413284132841329, 0.8559523809523809, 0.8604651162790697, 0.8479899497487438, 0.8380024360535931, 0.8373493975903614, 0.839390386869871, 0.8606658446362515, 0.8319623971797885, 0.8521212121212122, 0.8140947752126367, 0.8461538461538461, 0.8501228501228502, 0.8418491484184915, 0.8529769137302552, 0.8152709359605911, 0.8541666666666666, 0.8321167883211679, 0.8440145102781137, 0.8316708229426434, 0.8286066584463625, 0.853416149068323, 0.8675742574257426, 0.8406862745098039, 0.8237454100367197, 0.8478260869565217, 0.8611793611793612, 0.8267813267813268, 0.844139650872818, 0.8573185731857319, 0.8635809987819733, 0.8362068965517241, 0.8583535108958837, 0.8443337484433375, 0.8431618569636136, 0.8426150121065376, 0.8374689826302729, 0.8463444857496902, 0.8392204628501827, 0.8244084682440846, 0.8374689826302729, 0.8362068965517241, 0.8440925700365408, 0.8567901234567902, 0.8381995133819952, 0.8439450686641697, 0.8580171358629131, 0.8473748473748474, 0.8313106796116505, 0.8313106796116505, 0.826302729528536, 0.8316953316953317, 0.8483009708737864, 0.854320987654321, 0.8438661710037175, 0.8353365384615384, 0.8534798534798534, 0.8350877192982457, 0.8413284132841329, 0.8569682151589242, 0.8605230386052304, 0.8685162846803377, 0.8453105968331304, 0.8494492044063647, 0.8528708133971292, 0.8327228327228328, 0.8419753086419753, 0.8305489260143198, 0.8440145102781137, 0.8560975609756097, 0.8383233532934131, 0.8461538461538461, 0.8533178114086146, 0.8248792270531401, 0.8463414634146341, 0.8508014796547472, 0.8327272727272728, 0.8292682926829268, 0.8497536945812808, 0.85625, 0.8394607843137255, 0.853566958698373, 0.8604651162790697, 0.847877358490566, 0.8719211822660099, 0.8564102564102564, 0.8567990373044525, 0.8384710234278668, 0.8358024691358025, 0.8256658595641646, 0.8341404358353511, 0.8617788461538461, 0.8389830508474576, 0.8327272727272728, 0.8325123152709359, 0.8069738480697385, 0.8489646772228989, 0.8207663782447466, 0.8304668304668305, 0.8329383886255924, 0.8382877526753865, 0.8392204628501827, 0.8608490566037735, 0.8441717791411043, 0.8127294981640147, 0.8393285371702638, 0.8379351740696278, 0.8457831325301205, 0.8680387409200968, 0.84688995215311, 0.8507281553398058, 0.8359659781287971, 0.8357664233576643, 0.8528347406513872, 0.8574850299401198, 0.8387909319899244, 0.8317307692307693, 0.8480861244019139, 0.825925925925926, 0.843710292249047, 0.8256658595641646, 0.8486682808716707, 0.8469015795868773, 0.8351783517835178, 0.8390941597139452, 0.8489646772228989, 0.8585247883917775, 0.8386308068459658, 0.861557478368356, 0.8347718865598027, 0.84375, 0.8273291925465839, 0.8611464968152867, 0.85125, 0.8291517323775388, 0.8267326732673267, 0.8341463414634146, 0.8319327731092437, 0.8233128834355828, 0.8245192307692307, 0.8244746600741656, 0.8389021479713604, 0.8372641509433962, 0.8425584255842559, 0.8333333333333334, 0.8347934918648311, 0.8526570048309179, 0.8362989323843416, 0.8496420047732697, 0.851089588377724, 0.8408262454434994, 0.8163265306122449, 0.8353221957040573, 0.8354114713216958, 0.8627450980392157, 0.827503015681544, 0.8459627329192546, 0.82625, 0.8312655086848635, 0.8319327731092437, 0.8427518427518428, 0.8536880290205562, 0.8227383863080685, 0.8548972188633616, 0.837772397094431, 0.8423586040914561, 0.8395657418576599, 0.8275434243176178, 0.8350125944584383, 0.8263546798029556, 0.8318804483188045, 0.8529048207663782, 0.8461538461538461, 0.8272506082725061, 0.8433292533659731, 0.8343634116192831, 0.8062953995157385, 0.8415233415233415, 0.8451053283767038, 0.8535414165666266, 0.8411330049261084, 0.8585131894484412, 0.8449519230769231, 0.8253382533825339, 0.8314465408805032, 0.8691931540342298, 0.8291457286432161, 0.8275862068965517, 0.8148584905660378, 0.8495684340320592, 0.8065296251511487, 0.8560885608856088, 0.8411330049261084, 0.8426270136307311, 0.843065693430657, 0.8337408312958435, 0.8431845597104946, 0.8479087452471483, 0.8613496932515338, 0.8574908647990256, 0.8475390156062425, 0.8451219512195122, 0.8416075650118203, 0.8465473145780051, 0.8474784747847478, 0.835820895522388, 0.8351648351648352, 0.8315018315018315, 0.8374083129584352, 0.8581907090464548, 0.8157248157248157, 0.8481927710843373, 0.8501259445843828, 0.8566084788029925, 0.8460591133004927, 0.8660049627791563, 0.8266832917705735, 0.8503740648379052, 0.8316831683168316, 0.8480861244019139, 0.8122699386503067, 0.8407407407407408, 0.8312807881773399, 0.8189448441247003, 0.8542413381123058, 0.8345679012345679, 0.8347406513872135, 0.8226779252110977, 0.8617283950617284, 0.8492159227985525, 0.8192161820480405, 0.8433014354066986, 0.8205128205128205, 0.8383084577114428, 0.8301404853128991, 0.8439024390243902, 0.835985312117503, 0.8784933171324423, 0.8435619735258725, 0.8583850931677018, 0.8405797101449275, 0.8264058679706602, 0.846441947565543, 0.8417874396135265, 0.8436018957345972, 0.8185185185185185, 0.8529411764705882, 0.8295042321644498, 0.8258706467661692, 0.8557336621454994, 0.8217338217338217, 0.8307322929171669, 0.8353960396039604, 0.853566958698373, 0.8412698412698413, 0.845223700120919, 0.8426150121065376, 0.846244131455399, 0.8523985239852399, 0.8434782608695652, 0.8448275862068966, 0.8220338983050848, 0.8492647058823529, 0.8559322033898306, 0.8481613285883749, 0.8421052631578947, 0.8353080568720379, 0.8412698412698413, 0.8618581907090465, 0.8611111111111112, 0.8271752085816448, 0.8425381903642774, 0.8432563791008505, 0.8426829268292683, 0.8488943488943489, 0.8571428571428571, 0.83270911360799, 0.8260325406758448, 0.842603550295858, 0.8376470588235294, 0.8483353884093712, 0.8592411260709915, 0.8513674197384067, 0.8589420654911839, 0.8443908323281062, 0.8313397129186603, 0.8523153942428036, 0.8578431372549019, 0.8384332925336597, 0.8253382533825339, 0.8404522613065326, 0.84625, 0.8443037974683544, 0.8663366336633663, 0.8226600985221675, 0.8564417177914111, 0.8304878048780487, 0.8304297328687572, 0.86, 0.8463444857496902, 0.8252184769038702, 0.8554360812425329, 0.833743842364532, 0.8438256658595642, 0.8405797101449275, 0.8366834170854272, 0.8535353535353535, 0.8378378378378378, 0.8530510585305106, 0.8350125944584383, 0.8414634146341463, 0.8154613466334164, 0.8141263940520446, 0.8295739348370927, 0.8751500600240096, 0.8411552346570397, 0.8478802992518704, 0.8481166464155528, 0.8575, 0.8098765432098766, 0.8365853658536585, 0.8353510895883777, 0.8368098159509203, 0.8405797101449275, 0.8088235294117647, 0.8484136310223267, 0.8329207920792079, 0.832541567695962, 0.845679012345679, 0.8347509113001215, 0.8395061728395061, 0.8213851761846902, 0.8516284680337757, 0.8590686274509803, 0.8372670807453416, 0.8260869565217391, 0.8480392156862745, 0.8514150943396226, 0.8585365853658536, 0.8409090909090909, 0.8770186335403727, 0.8610086100861009, 0.8547418967587035, 0.8376068376068376, 0.8366394399066511, 0.8286066584463625, 0.859375, 0.8529048207663782, 0.8427370948379351, 0.8709677419354839, 0.850546780072904, 0.8154981549815498, 0.8179581795817958, 0.8566131025957973, 0.8355828220858895, 0.8298906439854192, 0.8386699507389163, 0.8220024721878862, 0.8310727496917386, 0.8508557457212714, 0.8277571251548946, 0.8377358490566038, 0.8536880290205562, 0.8198529411764706, 0.8253382533825339, 0.8529048207663782, 0.8518072289156626, 0.8327137546468402, 0.8383458646616542, 0.8439024390243902, 0.8514492753623188, 0.8393077873918418, 0.8237454100367197, 0.8513189448441247, 0.866751269035533, 0.8337408312958435]\n",
            "sp: [0.8310727496917386, 0.8561320754716981, 0.8300970873786407, 0.8528678304239401, 0.8407720144752714, 0.8656174334140436, 0.85375, 0.8478260869565217, 0.8496815286624204, 0.8247298919567827, 0.8214716525934861, 0.8540880503144654, 0.8470873786407767, 0.8376068376068376, 0.806060606060606, 0.8256097560975609, 0.8431845597104946, 0.811965811965812, 0.8516687268232386, 0.8501228501228502, 0.8446601941747572, 0.8461538461538461, 0.808252427184466, 0.8439803439803439, 0.820125786163522, 0.8262454434993924, 0.8297619047619048, 0.8418491484184915, 0.84375, 0.8365853658536585, 0.855036855036855, 0.838107098381071, 0.8224076281287247, 0.8434886499402628, 0.8307134220072552, 0.8557692307692307, 0.8257668711656442, 0.8514970059880239, 0.8291215403128761, 0.851140456182473, 0.850546780072904, 0.8376068376068376, 0.823030303030303, 0.8364779874213837, 0.8168316831683168, 0.8424242424242424, 0.8501827040194885, 0.8463414634146341, 0.8590522478736331, 0.8409937888198757, 0.8467153284671532, 0.8382352941176471, 0.8228915662650602, 0.855, 0.8368098159509203, 0.8351920693928129, 0.8438661710037175, 0.8451053283767038, 0.8329238329238329, 0.8229548229548229, 0.8395209580838323, 0.8404907975460123, 0.8411552346570397, 0.8436724565756824, 0.8421052631578947, 0.8588807785888077, 0.8172959805115713, 0.8288177339901478, 0.8357843137254902, 0.86, 0.8544152744630071, 0.8193384223918575, 0.8531994981179423, 0.8465227817745803, 0.8528678304239401, 0.8412887828162291, 0.8317073170731707, 0.8727272727272727, 0.8556085918854416, 0.8424317617866005, 0.8465286236297198, 0.831942789034565, 0.8372093023255814, 0.8415366146458584, 0.8502923976608188, 0.8415094339622642, 0.8321342925659473, 0.8453364817001181, 0.8335403726708075, 0.824390243902439, 0.8394332939787486, 0.8692033293697978, 0.8607442977190877, 0.851985559566787, 0.8392204628501827, 0.8247162673392182, 0.8618925831202046, 0.8548387096774194, 0.8377403846153846, 0.8425584255842559, 0.850546780072904, 0.8223039215686274, 0.8428053204353083, 0.8339622641509434, 0.8309352517985612, 0.8295739348370927, 0.8649289099526066, 0.8508771929824561, 0.8266331658291457, 0.8568019093078759, 0.8557213930348259, 0.8592057761732852, 0.823019801980198, 0.8522167487684729, 0.8569682151589242, 0.8456937799043063, 0.8552472858866104, 0.8261405672009864, 0.8471177944862155, 0.8498817966903073, 0.8242424242424242, 0.8351126927639383, 0.82875, 0.8439086294416244, 0.8546583850931677, 0.8502415458937198, 0.8335451080050826, 0.8060975609756098, 0.8463414634146341, 0.8514357053682896, 0.8205128205128205, 0.8633540372670807, 0.8364083640836408, 0.8365384615384616, 0.8317307692307693, 0.830568720379147, 0.8554070473876063, 0.8366336633663366, 0.8214285714285714, 0.8449519230769231, 0.8442028985507246, 0.8243243243243243, 0.8212157330154947, 0.8474784747847478, 0.8327183271832719, 0.8545006165228114, 0.8270401948842875, 0.8497596153846154, 0.8294190358467244, 0.8348045397225725, 0.8549810844892812, 0.845679012345679, 0.8345498783454988, 0.8498817966903073, 0.8426698450536353, 0.8195488721804511, 0.8286755771567437, 0.8513011152416357, 0.8592411260709915, 0.8503740648379052, 0.8445859872611465, 0.8353960396039604, 0.8101736972704715, 0.8181818181818182, 0.8477466504263094, 0.8523409363745498, 0.8245614035087719, 0.8211678832116789, 0.8618181818181818, 0.8048484848484848, 0.8460591133004927, 0.8337468982630273, 0.8257756563245824, 0.8428053204353083, 0.8511979823455234, 0.8397515527950311, 0.8606060606060606, 0.8375757575757575, 0.8420413122721749, 0.864321608040201, 0.8387878787878787, 0.8156288156288156, 0.8349633251833741, 0.8437118437118437, 0.8420398009950248, 0.8583850931677018, 0.8608374384236454, 0.8496894409937888, 0.8372379778051788, 0.8493975903614458, 0.8515439429928741, 0.8331257783312578, 0.8406326034063261, 0.827710843373494, 0.8327272727272728, 0.8366583541147132, 0.8444165621079046, 0.8422939068100358, 0.8394768133174791, 0.8615196078431373, 0.8319018404907975, 0.8358913813459268, 0.8247914183551848, 0.8393719806763285, 0.8478527607361963, 0.8376068376068376, 0.8288508557457213, 0.8466257668711656, 0.8281444582814446, 0.8449328449328449, 0.8404384896467723, 0.8574938574938575, 0.8082026537997588, 0.8333333333333334, 0.8302354399008675, 0.8396572827417381, 0.8586956521739131, 0.8580097087378641, 0.828641370869033, 0.8508771929824561, 0.8329238329238329, 0.8626237623762376, 0.8196517412935324, 0.8138424821002387, 0.8522167487684729, 0.8396674584323041, 0.8480697384806973, 0.8538083538083538, 0.8647990255785627, 0.8306351183063512, 0.8381430363864492, 0.8294478527607362, 0.8131313131313131, 0.8514492753623188, 0.8416458852867831, 0.8166047087980174, 0.8618654073199528, 0.8418491484184915, 0.8416565164433617, 0.8383594692400482, 0.8353658536585366, 0.8578255675029869, 0.8485221674876847, 0.8341645885286783, 0.8488943488943489, 0.8522050059594756, 0.833134684147795, 0.8464373464373465, 0.8433734939759037, 0.8285714285714286, 0.8363417569193743, 0.8528347406513872, 0.83270911360799, 0.8260869565217391, 0.847394540942928, 0.8423028785982478, 0.8085365853658537, 0.8345679012345679, 0.83375, 0.8521212121212122, 0.8346055979643766, 0.845398773006135, 0.8539976825028969, 0.8383961117861483, 0.8116959064327486, 0.822360248447205, 0.8491484184914841, 0.8322903629536921, 0.8451776649746193, 0.8624401913875598, 0.8087167070217918, 0.8440145102781137, 0.8473091364205256, 0.8515337423312883, 0.8319018404907975, 0.8402948402948403, 0.8576555023923444, 0.8500604594921403, 0.863030303030303, 0.8518072289156626, 0.8663366336633663, 0.845223700120919, 0.8467153284671532, 0.8498759305210918, 0.832541567695962, 0.8331332533013205, 0.8378712871287128, 0.8430160692212608, 0.850187265917603, 0.8454106280193237, 0.8233173076923077, 0.8230088495575221, 0.8424908424908425, 0.8619824341279799, 0.8355501813784765, 0.8327359617682198, 0.8428745432399513, 0.850356294536817, 0.8333333333333334, 0.8700980392156863, 0.8139255702280912, 0.8162291169451074, 0.8310387984981227, 0.8518971848225214, 0.8585732165206508, 0.8365617433414043, 0.8428571428571429, 0.837378640776699, 0.845088161209068, 0.8463414634146341, 0.8507281553398058, 0.856272838002436, 0.8264058679706602, 0.8478802992518704, 0.8405275779376499, 0.8333333333333334, 0.8403361344537815, 0.8765133171912833, 0.828009828009828, 0.82875, 0.8486293206197855, 0.8223289315726291, 0.8331288343558282, 0.8424821002386634, 0.8424242424242424, 0.8473748473748474, 0.8293601003764115, 0.8301435406698564, 0.8233861144945189, 0.8347718865598027, 0.8748510131108462, 0.8466334164588528, 0.8363636363636363, 0.8368355995055624, 0.8505603985056039, 0.8407079646017699, 0.8356973995271868, 0.8261390887290168, 0.833130328867235, 0.8351126927639383, 0.8519417475728155, 0.8403361344537815, 0.8616504854368932, 0.8282208588957055, 0.8400488400488401, 0.8525798525798526, 0.8461538461538461, 0.8391959798994975, 0.8471720818291215, 0.8491484184914841, 0.8459657701711492, 0.8359659781287971, 0.8491965389369592, 0.8421052631578947, 0.8343789209535759, 0.8429447852760736, 0.8656174334140436, 0.8437118437118437, 0.8217821782178217, 0.82625, 0.8199753390875463, 0.8660933660933661, 0.8415094339622642, 0.8557575757575757, 0.8523153942428036, 0.8388625592417062, 0.8378378378378378, 0.8499399759903962, 0.8438287153652393, 0.8446362515413071, 0.8175, 0.8233830845771144, 0.8348946135831382, 0.8475390156062425, 0.8559423769507803, 0.8650234741784038, 0.8450363196125908, 0.8666666666666667, 0.8555825242718447, 0.8300492610837439, 0.8488943488943489, 0.8524203069657615, 0.8403869407496977, 0.8323353293413174, 0.8515723270440252, 0.8281053952321205, 0.8531553398058253, 0.8443337484433375, 0.8327272727272728, 0.8331332533013205, 0.8371810449574727, 0.8187422934648582, 0.8243080625752106, 0.7995079950799509, 0.8375959079283888, 0.8317422434367542, 0.8418491484184915, 0.8423707440100883, 0.8357314148681055, 0.8447432762836186, 0.8381642512077294, 0.8516687268232386, 0.8408262454434994, 0.8410194174757282, 0.8455882352941176, 0.8422330097087378, 0.8567990373044525, 0.8312655086848635, 0.8602520045819015, 0.8383594692400482, 0.8618654073199528, 0.8653846153846154, 0.809102402022756, 0.8407185628742515, 0.8511166253101737, 0.8341645885286783, 0.8424821002386634, 0.8416075650118203, 0.8431137724550898, 0.816953316953317, 0.8433451118963486, 0.8425480769230769, 0.8192918192918193, 0.8434343434343434, 0.8264669163545568, 0.857849196538937, 0.8467153284671532, 0.8451536643026005, 0.8379052369077307, 0.8558786346396966, 0.8462469733656174, 0.8110709987966306, 0.8403263403263403, 0.8482587064676617, 0.8661710037174721, 0.866421568627451, 0.8482252141982864, 0.8430379746835444, 0.8493827160493828, 0.8236775818639799, 0.8539603960396039, 0.8460620525059666, 0.843236409608091, 0.85, 0.8449131513647643, 0.857487922705314, 0.8364083640836408, 0.8310893512851897, 0.8237410071942446, 0.8358585858585859, 0.8587223587223587, 0.8207663782447466, 0.8380614657210402, 0.8522167487684729, 0.8327183271832719, 0.8349753694581281, 0.8202653799758746, 0.8337408312958435, 0.8208955223880597, 0.850546780072904, 0.8335388409371147, 0.8646341463414634, 0.8236024844720496, 0.8670731707317073, 0.8377358490566038, 0.8323207776427703, 0.823019801980198, 0.8414481897627965, 0.8203221809169765, 0.8478002378121284, 0.8322903629536921, 0.8319226118500604, 0.8424821002386634, 0.8341463414634146, 0.8353365384615384, 0.844059405940594, 0.8366093366093366, 0.8524788391777509, 0.8134328358208955, 0.8343409915356711, 0.8216867469879519, 0.8483353884093712, 0.8022113022113022, 0.8403990024937655, 0.8412698412698413, 0.8319226118500604, 0.8270858524788391, 0.821831869510665, 0.8440925700365408, 0.8335403726708075, 0.8327228327228328, 0.8290398126463701, 0.8302808302808303, 0.8374384236453202, 0.8331332533013205, 0.8385542168674699, 0.833530106257379, 0.8484107579462102, 0.8329268292682926, 0.8463414634146341, 0.8351515151515152, 0.8412698412698413, 0.8487084870848709, 0.83, 0.8335287221570926, 0.8309352517985612, 0.8566084788029925, 0.8396111786148238, 0.86125, 0.84, 0.8441890166028098, 0.8438661710037175, 0.8256658595641646, 0.8185185185185185, 0.8345771144278606, 0.8449328449328449, 0.8528301886792453, 0.8636911942098915, 0.8155339805825242, 0.8262910798122066, 0.8667481662591687, 0.8319123020706456, 0.8365384615384616, 0.829683698296837, 0.8580024067388689, 0.8349633251833741, 0.8397129186602871, 0.8465286236297198, 0.8496894409937888, 0.8370098039215687, 0.8509852216748769, 0.8381995133819952, 0.8259303721488596, 0.8366583541147132, 0.8525402726146221, 0.8322981366459627, 0.8410194174757282, 0.8318804483188045, 0.8375451263537906, 0.8600478468899522, 0.8548387096774194, 0.8606965174129353, 0.8275862068965517, 0.8583850931677018, 0.8388683886838868, 0.8481481481481481, 0.8188494492044064, 0.8343057176196033, 0.8034610630407911, 0.8212974296205631, 0.8450184501845018, 0.8313397129186603, 0.8199052132701422, 0.8303030303030303, 0.8370098039215687, 0.7995018679950187, 0.8553299492385786, 0.8493150684931506, 0.8497596153846154, 0.834319526627219, 0.8506172839506173, 0.8533007334963325, 0.82625, 0.8507462686567164, 0.8604060913705583, 0.8561064087061668, 0.8418549346016647, 0.8445532435740514, 0.8416458852867831, 0.8431845597104946, 0.8566131025957973, 0.8423645320197044, 0.8490566037735849, 0.8573170731707317, 0.8453105968331304, 0.8230088495575221, 0.8416458852867831, 0.8321428571428572, 0.8461538461538461, 0.8175182481751825, 0.8478260869565217, 0.8298906439854192, 0.8323353293413174, 0.857487922705314, 0.8587360594795539, 0.8402366863905325, 0.8387096774193549, 0.8476190476190476, 0.8488943488943489, 0.838107098381071, 0.820543093270366, 0.8482252141982864, 0.8333333333333334, 0.8339483394833949, 0.83, 0.845398773006135, 0.8431372549019608, 0.8495788206979543, 0.8626237623762376, 0.8365853658536585, 0.852760736196319, 0.8391356542617047, 0.8407960199004975, 0.8385922330097088, 0.8506024096385543, 0.8604091456077015, 0.8266033254156769, 0.8513011152416357, 0.8446969696969697, 0.8496894409937888, 0.8498168498168498, 0.8289473684210527, 0.8357843137254902, 0.8418491484184915, 0.8486682808716707, 0.8588957055214724, 0.8415366146458584, 0.8381294964028777, 0.8539458186101295, 0.8547008547008547, 0.8531139835487661, 0.8532675709001233, 0.8649318463444857, 0.8481012658227848, 0.8413284132841329, 0.8559523809523809, 0.8604651162790697, 0.8479899497487438, 0.8380024360535931, 0.8373493975903614, 0.839390386869871, 0.8606658446362515, 0.8319623971797885, 0.8521212121212122, 0.8140947752126367, 0.8461538461538461, 0.8501228501228502, 0.8418491484184915, 0.8529769137302552, 0.8152709359605911, 0.8541666666666666, 0.8321167883211679, 0.8440145102781137, 0.8316708229426434, 0.8286066584463625, 0.853416149068323, 0.8675742574257426, 0.8406862745098039, 0.8237454100367197, 0.8478260869565217, 0.8611793611793612, 0.8267813267813268, 0.844139650872818, 0.8573185731857319, 0.8635809987819733, 0.8362068965517241, 0.8583535108958837, 0.8443337484433375, 0.8431618569636136, 0.8426150121065376, 0.8374689826302729, 0.8463444857496902, 0.8392204628501827, 0.8244084682440846, 0.8374689826302729, 0.8362068965517241, 0.8440925700365408, 0.8567901234567902, 0.8381995133819952, 0.8439450686641697, 0.8580171358629131, 0.8473748473748474, 0.8313106796116505, 0.8313106796116505, 0.826302729528536, 0.8316953316953317, 0.8483009708737864, 0.854320987654321, 0.8438661710037175, 0.8353365384615384, 0.8534798534798534, 0.8350877192982457, 0.8413284132841329, 0.8569682151589242, 0.8605230386052304, 0.8685162846803377, 0.8453105968331304, 0.8494492044063647, 0.8528708133971292, 0.8327228327228328, 0.8419753086419753, 0.8305489260143198, 0.8440145102781137, 0.8560975609756097, 0.8383233532934131, 0.8461538461538461, 0.8533178114086146, 0.8248792270531401, 0.8463414634146341, 0.8508014796547472, 0.8327272727272728, 0.8292682926829268, 0.8497536945812808, 0.85625, 0.8394607843137255, 0.853566958698373, 0.8604651162790697, 0.847877358490566, 0.8719211822660099, 0.8564102564102564, 0.8567990373044525, 0.8384710234278668, 0.8358024691358025, 0.8256658595641646, 0.8341404358353511, 0.8617788461538461, 0.8389830508474576, 0.8327272727272728, 0.8325123152709359, 0.8069738480697385, 0.8489646772228989, 0.8207663782447466, 0.8304668304668305, 0.8329383886255924, 0.8382877526753865, 0.8392204628501827, 0.8608490566037735, 0.8441717791411043, 0.8127294981640147, 0.8393285371702638, 0.8379351740696278, 0.8457831325301205, 0.8680387409200968, 0.84688995215311, 0.8507281553398058, 0.8359659781287971, 0.8357664233576643, 0.8528347406513872, 0.8574850299401198, 0.8387909319899244, 0.8317307692307693, 0.8480861244019139, 0.825925925925926, 0.843710292249047, 0.8256658595641646, 0.8486682808716707, 0.8469015795868773, 0.8351783517835178, 0.8390941597139452, 0.8489646772228989, 0.8585247883917775, 0.8386308068459658, 0.861557478368356, 0.8347718865598027, 0.84375, 0.8273291925465839, 0.8611464968152867, 0.85125, 0.8291517323775388, 0.8267326732673267, 0.8341463414634146, 0.8319327731092437, 0.8233128834355828, 0.8245192307692307, 0.8244746600741656, 0.8389021479713604, 0.8372641509433962, 0.8425584255842559, 0.8333333333333334, 0.8347934918648311, 0.8526570048309179, 0.8362989323843416, 0.8496420047732697, 0.851089588377724, 0.8408262454434994, 0.8163265306122449, 0.8353221957040573, 0.8354114713216958, 0.8627450980392157, 0.827503015681544, 0.8459627329192546, 0.82625, 0.8312655086848635, 0.8319327731092437, 0.8427518427518428, 0.8536880290205562, 0.8227383863080685, 0.8548972188633616, 0.837772397094431, 0.8423586040914561, 0.8395657418576599, 0.8275434243176178, 0.8350125944584383, 0.8263546798029556, 0.8318804483188045, 0.8529048207663782, 0.8461538461538461, 0.8272506082725061, 0.8433292533659731, 0.8343634116192831, 0.8062953995157385, 0.8415233415233415, 0.8451053283767038, 0.8535414165666266, 0.8411330049261084, 0.8585131894484412, 0.8449519230769231, 0.8253382533825339, 0.8314465408805032, 0.8691931540342298, 0.8291457286432161, 0.8275862068965517, 0.8148584905660378, 0.8495684340320592, 0.8065296251511487, 0.8560885608856088, 0.8411330049261084, 0.8426270136307311, 0.843065693430657, 0.8337408312958435, 0.8431845597104946, 0.8479087452471483, 0.8613496932515338, 0.8574908647990256, 0.8475390156062425, 0.8451219512195122, 0.8416075650118203, 0.8465473145780051, 0.8474784747847478, 0.835820895522388, 0.8351648351648352, 0.8315018315018315, 0.8374083129584352, 0.8581907090464548, 0.8157248157248157, 0.8481927710843373, 0.8501259445843828, 0.8566084788029925, 0.8460591133004927, 0.8660049627791563, 0.8266832917705735, 0.8503740648379052, 0.8316831683168316, 0.8480861244019139, 0.8122699386503067, 0.8407407407407408, 0.8312807881773399, 0.8189448441247003, 0.8542413381123058, 0.8345679012345679, 0.8347406513872135, 0.8226779252110977, 0.8617283950617284, 0.8492159227985525, 0.8192161820480405, 0.8433014354066986, 0.8205128205128205, 0.8383084577114428, 0.8301404853128991, 0.8439024390243902, 0.835985312117503, 0.8784933171324423, 0.8435619735258725, 0.8583850931677018, 0.8405797101449275, 0.8264058679706602, 0.846441947565543, 0.8417874396135265, 0.8436018957345972, 0.8185185185185185, 0.8529411764705882, 0.8295042321644498, 0.8258706467661692, 0.8557336621454994, 0.8217338217338217, 0.8307322929171669, 0.8353960396039604, 0.853566958698373, 0.8412698412698413, 0.845223700120919, 0.8426150121065376, 0.846244131455399, 0.8523985239852399, 0.8434782608695652, 0.8448275862068966, 0.8220338983050848, 0.8492647058823529, 0.8559322033898306, 0.8481613285883749, 0.8421052631578947, 0.8353080568720379, 0.8412698412698413, 0.8618581907090465, 0.8611111111111112, 0.8271752085816448, 0.8425381903642774, 0.8432563791008505, 0.8426829268292683, 0.8488943488943489, 0.8571428571428571, 0.83270911360799, 0.8260325406758448, 0.842603550295858, 0.8376470588235294, 0.8483353884093712, 0.8592411260709915, 0.8513674197384067, 0.8589420654911839, 0.8443908323281062, 0.8313397129186603, 0.8523153942428036, 0.8578431372549019, 0.8384332925336597, 0.8253382533825339, 0.8404522613065326, 0.84625, 0.8443037974683544, 0.8663366336633663, 0.8226600985221675, 0.8564417177914111, 0.8304878048780487, 0.8304297328687572, 0.86, 0.8463444857496902, 0.8252184769038702, 0.8554360812425329, 0.833743842364532, 0.8438256658595642, 0.8405797101449275, 0.8366834170854272, 0.8535353535353535, 0.8378378378378378, 0.8530510585305106, 0.8350125944584383, 0.8414634146341463, 0.8154613466334164, 0.8141263940520446, 0.8295739348370927, 0.8751500600240096, 0.8411552346570397, 0.8478802992518704, 0.8481166464155528, 0.8575, 0.8098765432098766, 0.8365853658536585, 0.8353510895883777, 0.8368098159509203, 0.8405797101449275, 0.8088235294117647, 0.8484136310223267, 0.8329207920792079, 0.832541567695962, 0.845679012345679, 0.8347509113001215, 0.8395061728395061, 0.8213851761846902, 0.8516284680337757, 0.8590686274509803, 0.8372670807453416, 0.8260869565217391, 0.8480392156862745, 0.8514150943396226, 0.8585365853658536, 0.8409090909090909, 0.8770186335403727, 0.8610086100861009, 0.8547418967587035, 0.8376068376068376, 0.8366394399066511, 0.8286066584463625, 0.859375, 0.8529048207663782, 0.8427370948379351, 0.8709677419354839, 0.850546780072904, 0.8154981549815498, 0.8179581795817958, 0.8566131025957973, 0.8355828220858895, 0.8298906439854192, 0.8386699507389163, 0.8220024721878862, 0.8310727496917386, 0.8508557457212714, 0.8277571251548946, 0.8377358490566038, 0.8536880290205562, 0.8198529411764706, 0.8253382533825339, 0.8529048207663782, 0.8518072289156626, 0.8327137546468402, 0.8383458646616542, 0.8439024390243902, 0.8514492753623188, 0.8393077873918418, 0.8237454100367197, 0.8513189448441247, 0.866751269035533, 0.8337408312958435]\n",
            "sd_acc: 0.01055898892005772\n",
            "sd_f1: 0.008565264362993317\n",
            "sd_mcc: 0.022062130583262132\n",
            "sd_sn: 0.013143095000683934\n",
            "sd_sp: 0.013143095000683934\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.69      0.87      0.77       331\n",
            "           1       0.94      0.84      0.89       818\n",
            "\n",
            "    accuracy                           0.85      1149\n",
            "   macro avg       0.82      0.86      0.83      1149\n",
            "weighted avg       0.87      0.85      0.85      1149\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.01055898892005772, 0.022062130583262132, 0.008565264362993317)"
            ]
          },
          "execution_count": 138,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "error_rate(testing_labels, predicted_mlp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "fxWBC6cMacP-",
        "outputId": "8f2e5766-bff3-4b36-a8a8-bb8274f76324"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-139-1cf47a78de25>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfpr_MLP\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtpr_MLP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresholds_MLP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_mlp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mauc_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_mlp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpr_MLP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr_MLP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m\"MLP ({:.2f})\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauc_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36mroc_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight, drop_intermediate)\u001b[0m\n\u001b[1;32m    990\u001b[0m     \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1.8\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m0.8\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m0.4\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m0.35\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m     \"\"\"\n\u001b[0;32m--> 992\u001b[0;31m     fps, tps, thresholds = _binary_clf_curve(\n\u001b[0m\u001b[1;32m    993\u001b[0m         \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpos_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    994\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36m_binary_clf_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[0my_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y_true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"binary\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mpos_label\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 749\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{0} format is not supported\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: continuous-multioutput format is not supported"
          ]
        }
      ],
      "source": [
        "fpr_MLP , tpr_MLP, thresholds_MLP = roc_curve(test_dataset, predicted_mlp)\n",
        "auc_score = roc_auc_score(test_dataset, predicted_mlp)\n",
        "plt.plot(fpr_MLP, tpr_MLP, label= \"MLP ({:.2f})\".format(auc_score))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJMUq8Jl7PWl"
      },
      "outputs": [],
      "source": [
        "df =pd.DataFrame(train_dataset)\n",
        "df.to_excel('/content/mlp_train_dataset.xlsx')\n",
        "df =pd.DataFrame(training_labels)\n",
        "df.to_excel('/content/mlp_training_labels.xlsx')\n",
        "df =pd.DataFrame(predicted_training_labels)\n",
        "df.to_excel('/content/mlp_pred_train_labels.xlsx')\n",
        "\n",
        "df =pd.DataFrame(test_dataset)\n",
        "df.to_excel('/content/mlp_test_dataset.xlsx')\n",
        "df =pd.DataFrame(testing_labels)\n",
        "df.to_excel('/content/mlp_testing_labels.xlsx')\n",
        "df =pd.DataFrame(predicted_mlp)\n",
        "df.to_excel('/content/mlp_pred_test_labels.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PeG8NDvGBeYM"
      },
      "outputs": [],
      "source": [
        "from pandas import DataFrame\n",
        "cv_results = DataFrame(classifiers.cv_results_)\n",
        "\n",
        "#cv_results[['param_hidden_layer_sizes','split0_test_score', 'split1_test_score', 'split2_test_score']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ViDDElvGBnf6"
      },
      "outputs": [],
      "source": [
        "# Further metrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Data visualization\n",
        "import plotly.express as px"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aqKqqPfoBvZ7"
      },
      "outputs": [],
      "source": [
        "classes = np.unique(testing_labels)\n",
        "\n",
        "confusion_matrix_data = confusion_matrix(testing_labels, predicted_mlp, labels=classes)\n",
        "conf_matrix(confusion_matrix_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "azx1iNma1Lyc"
      },
      "outputs": [],
      "source": [
        "TP = confusion_matrix_data[1,1]\n",
        "TN = confusion_matrix_data[0,0]\n",
        "FP = confusion_matrix_data[0,1]\n",
        "FN = confusion_matrix_data[1,0]\n",
        "\n",
        "print(TP,TN, FP, FN)\n",
        "\n",
        "sn = TP / float(TP + FN)\n",
        "print(sn)\n",
        "sp = TN / float(TN + FP)\n",
        "print(sp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UC6sZ0D_kgZj"
      },
      "source": [
        "### CNN with GridSearch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "8S7uiM5ykoYz",
        "outputId": "427893ae-76cb-45d1-f7ca-cde94dc6460e"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-23ca8a21cc41>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mActivation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensorBoard\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCSVLogger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrappers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscikit_learn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKerasClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras.wrappers'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Input, Dense, Dropout, Activation\n",
        "from keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping, CSVLogger\n",
        "from keras.wrappers.scikit_learn import KerasClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GycVI1SWkoY0"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Conv1D\n",
        "from keras.layers import MaxPooling1D\n",
        "from keras.layers import Embedding\n",
        "from keras.preprocessing import sequence\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.utils import to_categorical\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from time import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTsuS35AXYJX"
      },
      "outputs": [],
      "source": [
        "from numpy.random import seed\n",
        "seed(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p8l8VZsGXYJY"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import set_random_seed\n",
        "set_random_seed(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g2wBaT28koY0"
      },
      "outputs": [],
      "source": [
        "X_train, X_test= train_dataset, test_dataset\n",
        "y_train, y_test = training_labels, testing_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJcrkuiokoY0"
      },
      "outputs": [],
      "source": [
        "Y_train = np.reshape(y_train,(len(y_train),1)).astype(int)\n",
        "Y_test = np.reshape(y_test,(len(y_test),1)).astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jHoLG0YFkoY0"
      },
      "outputs": [],
      "source": [
        "n_timesteps, n_features, n_outputs =train_dataset.shape[0], train_dataset.shape[1], Y_train.shape[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cpfyojPuYKtg"
      },
      "outputs": [],
      "source": [
        "n_epochs = 30 # 30\n",
        "n_epochs_cv = 10 # 10  # reduce number of epochs for cross validation for performance reason\n",
        "\n",
        "n_cv = 3\n",
        "validation_ratio = 0.10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lavUwJEZPC9T"
      },
      "outputs": [],
      "source": [
        "def create_cnn_model(pool_type='max', conv_activation='sigmoid', dropout_rate=0.10, kernel=3):\n",
        "    # create model\n",
        "    model = Sequential()\n",
        "\n",
        "    # first layer: convolution\n",
        "    #model.add(Conv2D(16, kernel_size=(5, 5), activation='relu', input_shape=(28, 28, 1)))\n",
        "    model.add(Conv1D(filters=128, kernel_size=kernel, activation='relu',input_shape=(n_features,1)))\n",
        "    # second series of layers: convolution, pooling, and dropout\n",
        "    model.add(Conv1D(32, kernel_size=kernel, activation=conv_activation))\n",
        "    if pool_type == 'max':\n",
        "        model.add(MaxPooling1D(pool_size=2))\n",
        "    if pool_type == 'average':\n",
        "        model.add(AveragePooling1D(pool_size=2))\n",
        "    if dropout_rate != 0:\n",
        "        model.add(Dropout(rate=dropout_rate))\n",
        "\n",
        "    # third series of layers: convolution, pooling, and dropout\n",
        "    model.add(Conv1D(64, kernel_size=kernel, activation=conv_activation))   # 32\n",
        "    if pool_type == 'max':\n",
        "        model.add(MaxPooling1D(pool_size=2))\n",
        "    if pool_type == 'average':\n",
        "        model.add(AveragePooling1D(pool_size=2))\n",
        "    if dropout_rate != 0:\n",
        "        model.add(Dropout(rate=dropout_rate))\n",
        "\n",
        "    # fourth series\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(64, activation='sigmoid')) # 64\n",
        "    # add a dropout layer if rate is not null\n",
        "    if dropout_rate != 0:\n",
        "        model.add(Dropout(rate=dropout_rate))\n",
        "\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy'],\n",
        "        )\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VLA3JilNT1UC"
      },
      "outputs": [],
      "source": [
        "cnn = create_cnn_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ufu-Fy93T57n"
      },
      "outputs": [],
      "source": [
        "cnn.compile(\n",
        "  optimizer='adam',\n",
        "  loss='binary_crossentropy',\n",
        "  metrics=['accuracy'],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gGjva7x6T7mx",
        "outputId": "b7d6af66-9ed6-4388-b834-733df3014ab2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d (Conv1D)             (None, 2558, 128)         512       \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, 2556, 32)          12320     \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1D  (None, 1278, 32)         0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 1278, 32)          0         \n",
            "                                                                 \n",
            " conv1d_2 (Conv1D)           (None, 1276, 64)          6208      \n",
            "                                                                 \n",
            " max_pooling1d_1 (MaxPooling  (None, 638, 64)          0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 638, 64)           0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 40832)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 64)                2613312   \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,632,417\n",
            "Trainable params: 2,632,417\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "cnn.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ByEiiFAnO6v5",
        "outputId": "00a6fd39-d262-4e77-89a6-cc9e654fa6a2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-54-d1b57facbbf5>:5: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
            "  model = KerasClassifier(build_fn=create_cnn_model, verbose=1)\n"
          ]
        }
      ],
      "source": [
        "# optimize model\n",
        "start = time()\n",
        "\n",
        "# create model\n",
        "model = KerasClassifier(build_fn=create_cnn_model, verbose=1)\n",
        "# define parameters and values for grid search\n",
        "param_grid = {\n",
        "    'pool_type': ['max', 'average'],\n",
        "    'conv_activation': ['relu', 'tanh'],\n",
        "    'epochs': [10,30],\n",
        "    'kernel':[1,2,3,4,5],\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOb1l3JIclmQ"
      },
      "outputs": [],
      "source": [
        "metrics = {'accuracy':make_scorer(accuracy_score,greater_is_better=True),'f1':make_scorer(f1_score,greater_is_better=True),'mcc':make_scorer(matthews_corrcoef,greater_is_better=True)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4AjzaBaZDkC",
        "outputId": "c46638cc-816b-4d5d-dd8e-8a69361a77ba"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
            "200 fits failed out of a total of 400.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "200 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/wrappers/scikit_learn.py\", line 248, in fit\n",
            "    return super().fit(x, y, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/wrappers/scikit_learn.py\", line 164, in fit\n",
            "    self.model = self.build_fn(**self.filter_sk_params(self.build_fn))\n",
            "  File \"<ipython-input-50-d254d4d44777>\", line 13, in create_cnn_model\n",
            "NameError: name 'AveragePooling1D' is not defined\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [0.79743204        nan 0.8383717         nan 0.82618878        nan\n",
            " 0.84121625        nan 0.82094771        nan 0.79633845        nan\n",
            " 0.78305437        nan 0.814201          nan 0.79196505        nan\n",
            " 0.79525575        nan 0.80025481        nan 0.79764185        nan\n",
            " 0.81398645        nan 0.80830397        nan 0.82682628        nan\n",
            " 0.7906858         nan 0.7832604         nan 0.79678365        nan\n",
            " 0.80069148        nan 0.81571943        nan]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [0.72765936        nan 0.74486443        nan 0.72480506        nan\n",
            " 0.7357453         nan 0.73300388        nan 0.72730871        nan\n",
            " 0.72649756        nan 0.72836179        nan 0.72931804        nan\n",
            " 0.72392252        nan 0.72514484        nan 0.73004127        nan\n",
            " 0.73366989        nan 0.73682787        nan 0.73597376        nan\n",
            " 0.7249994         nan 0.72573191        nan 0.72207464        nan\n",
            " 0.73200529        nan 0.73411716        nan]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [0.14865445        nan 0.15897596        nan 0.13465269        nan\n",
            " 0.15674601        nan 0.14482761        nan 0.14309738        nan\n",
            " 0.14191058        nan 0.14698086        nan 0.1413327         nan\n",
            " 0.140173          nan 0.13590024        nan 0.13680196        nan\n",
            " 0.14924407        nan 0.15604946        nan 0.15379385        nan\n",
            " 0.14246891        nan 0.13534621        nan 0.13899633        nan\n",
            " 0.14486459        nan 0.15208172        nan]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "144/144 [==============================] - 23s 150ms/step - loss: 0.3868 - accuracy: 0.8276\n",
            "Epoch 2/10\n",
            "144/144 [==============================] - 21s 145ms/step - loss: 0.2926 - accuracy: 0.8796\n",
            "Epoch 3/10\n",
            "144/144 [==============================] - 21s 146ms/step - loss: 0.2583 - accuracy: 0.8979\n",
            "Epoch 4/10\n",
            "144/144 [==============================] - 21s 146ms/step - loss: 0.2362 - accuracy: 0.9068\n",
            "Epoch 5/10\n",
            "144/144 [==============================] - 21s 144ms/step - loss: 0.2170 - accuracy: 0.9162\n",
            "Epoch 6/10\n",
            "144/144 [==============================] - 21s 145ms/step - loss: 0.1923 - accuracy: 0.9234\n",
            "Epoch 7/10\n",
            "144/144 [==============================] - 21s 145ms/step - loss: 0.1689 - accuracy: 0.9336\n",
            "Epoch 8/10\n",
            "144/144 [==============================] - 21s 148ms/step - loss: 0.1523 - accuracy: 0.9438\n",
            "Epoch 9/10\n",
            "144/144 [==============================] - 21s 143ms/step - loss: 0.1276 - accuracy: 0.9545\n",
            "Epoch 10/10\n",
            "144/144 [==============================] - 23s 159ms/step - loss: 0.0996 - accuracy: 0.9639\n"
          ]
        }
      ],
      "source": [
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=10,  scoring=metrics, refit='mcc')\n",
        "grid_result = grid.fit(train_dataset, Y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GvU3yNNhgdFO"
      },
      "outputs": [],
      "source": [
        "def display_cv_results(search_results):\n",
        "    print('Best score = {:.4f} using {}'.format(search_results.best_score_, search_results.best_params_))\n",
        "    means = search_results.cv_results_['mean_test_score']\n",
        "    stds = search_results.cv_results_['std_test_score']\n",
        "    params = search_results.cv_results_['params']\n",
        "    for mean, stdev, param in zip(means, stds, params):\n",
        "        print('mean test accuracy +/- std = {:.4f} +/- {:.4f} with: {}'.format(mean, stdev, param))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdc9syuZgPDy"
      },
      "outputs": [],
      "source": [
        "# summarize results\n",
        "#print('time for grid search = {:.0f} sec'.format(time()-start))\n",
        "#display_cv_results(grid_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqp_rfA9dLy2"
      },
      "outputs": [],
      "source": [
        "#DataFrame(grid.cv_results_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tn8hczYMdLy5"
      },
      "outputs": [],
      "source": [
        "df =pd.DataFrame(grid.cv_results_)\n",
        "#new_path = '/content/test.xls'\n",
        "#writer = pd.ExcelWriter(new_path, engine='xlsxwriter')\n",
        "df.to_excel('/content/drive/MyDrive/Acidophilic/ESM23BCNN.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5UoXcPndi1td",
        "outputId": "731fe977-e694-45f1-acc1-2a409270f8f7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'verbose': 1,\n",
              " 'conv_activation': 'relu',\n",
              " 'epochs': 10,\n",
              " 'kernel': 2,\n",
              " 'pool_type': 'max',\n",
              " 'build_fn': <function __main__.create_cnn_model(pool_type='max', conv_activation='sigmoid', dropout_rate=0.1, kernel=3)>}"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "classifier = grid.best_estimator_\n",
        "\n",
        "params = classifier.get_params()\n",
        "params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PU5zpAmPEspN",
        "outputId": "f3af3ae7-11f5-4606-fbc0-cc4f58be0019"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "144/144 [==============================] - 4s 29ms/step\n",
            "acc: 0.978880905726105\n",
            "f1: 0.9850975572284528\n",
            "mcc: 0.9489925121690131\n",
            "sn: [0.977639751552795, 0.9795348837209302, 0.9777710109622412, 0.9799878714372346, 0.9765315452605913, 0.9783602560195063, 0.9802102659245516, 0.9841027208804647, 0.9845440494590417, 0.9761401389308366, 0.9846708746618575, 0.979147500766636, 0.9778720824492271, 0.9781941031941032, 0.9830508474576272, 0.9834558823529411, 0.9825248568846038, 0.9778519417475728, 0.9816905706438815, 0.9789890377588307, 0.9768946395563771, 0.9795857099969979, 0.9789669038045159, 0.9786128933699969, 0.9806060606060606, 0.9792682926829268, 0.980840543881335, 0.9813365442504516, 0.98033093974399, 0.9781454886044333, 0.9815950920245399, 0.9791666666666666, 0.9825581395348837, 0.9807103490508267, 0.9789634146341464, 0.9803076923076923, 0.9781287970838396, 0.9817850637522769, 0.9835816357555488, 0.9806001818732949, 0.9800796812749004, 0.9807033924680983, 0.9795918367346939, 0.9825109306683323, 0.98, 0.9862628785513581, 0.98171109733416, 0.977947932618683, 0.9803379416282642, 0.9796476306196841, 0.9836621454993835, 0.9813196746007834, 0.9773075743636921, 0.9768160741885626, 0.9792239535594256, 0.9805234327449787, 0.9791282995702886, 0.9785043899485316, 0.9781336618417, 0.9800181653042689, 0.981874039938556, 0.9769514443761524, 0.9840745192307693, 0.9800857843137255, 0.9758261933904528, 0.9779749158764148, 0.9826325411334552, 0.9785498489425982, 0.9831804281345565, 0.9785539215686274, 0.9743510506798516, 0.9749320446994866, 0.978494623655914, 0.9791985316610584, 0.9803016312711603, 0.9786466165413534, 0.9818851704022106, 0.9858635525507068, 0.9795294836541399, 0.9802791262135923, 0.9820655534941249, 0.9845173041894353, 0.9782608695652174, 0.982148353339489, 0.9812291855888586, 0.9769017554665845, 0.9773838630806846, 0.9814173228346457, 0.9790962188748847, 0.979223125564589, 0.9777710109622412, 0.9849277145493694, 0.9752245277175596, 0.9771039603960396, 0.9759556103575833, 0.9811028500619579, 0.9769627159745378, 0.977363107984093, 0.9832317073170732, 0.9800184445127574, 0.9812235009085403, 0.9814929056138186, 0.9794456555590159, 0.9834457388105456, 0.9793438639125152, 0.9770079705702023, 0.9846484494933988, 0.9789283564118001, 0.9855421686746988, 0.9779722389861195, 0.982131854590265, 0.9805855161787366, 0.9815682599187754, 0.9842662632375189, 0.9814646004254026, 0.9789734075448361, 0.975811390079608, 0.9828431372549019, 0.9801796221740476, 0.9748234571691741, 0.9813981398139814, 0.9833999385182908, 0.9824720459353279, 0.9789055334760012, 0.9754751686082158, 0.9750692520775623, 0.9782016348773842, 0.9810223446587083, 0.9822706065318818, 0.9794918885827977, 0.9799444615859303, 0.9820012202562538, 0.97821482602118, 0.9802408150663785, 0.9807985370313929, 0.9790464621925296, 0.9806332616046726, 0.9822074788902292, 0.9778048038917604, 0.9757371007371007, 0.9819571865443425, 0.9824402058734484, 0.9796923076923076, 0.9798798798798799, 0.9806689168456582, 0.9839357429718876, 0.9811893203883495, 0.9810028028651511, 0.9833538840937115, 0.9819902319902319, 0.9796538111144852, 0.9802530083307621, 0.9829164124466138, 0.9814024390243903, 0.9718693284936479, 0.9790845710821461, 0.9832661915091416, 0.9798657718120806, 0.9770462096043492, 0.9849785407725322, 0.9821484497337927, 0.9807457212713936, 0.9850609756097561, 0.9819406183042547, 0.9825260576333538, 0.9816098884534218, 0.9810264385692068, 0.9841560024375381, 0.9796969696969697, 0.9762989972652689, 0.9810165339865279, 0.9819059107358263, 0.9783013019218847, 0.9824614454188086, 0.9798449612403101, 0.9818461538461538, 0.9833987322668276, 0.976730129948625, 0.9818851704022106, 0.9814194334450198, 0.978756157635468, 0.9795792079207921, 0.9791794243723209, 0.9809897404948702, 0.9776348039215687, 0.9812745394140743, 0.9835616438356164, 0.9790782292298362, 0.9796414463688848, 0.9804275820535984, 0.9813490829965806, 0.9835505896958411, 0.9811659192825112, 0.9800613496932515, 0.9816120134845234, 0.9761092150170648, 0.9787363304981774, 0.9823439878234399, 0.9828903146959975, 0.9834203254528707, 0.9789055334760012, 0.984092994799633, 0.9796421961752005, 0.9810676598386096, 0.9828880216151306, 0.9767299448867116, 0.9842041312272175, 0.9822194972409565, 0.9798350137488543, 0.9812768569674647, 0.9811492854971116, 0.9810165339865279, 0.9766441303011678, 0.9783668494820231, 0.9805648344974187, 0.9853166105842766, 0.9787621359223301, 0.9819113656918903, 0.9819931698230363, 0.974907063197026, 0.9830354438049076, 0.9824236817761333, 0.9751289050652108, 0.98322659347362, 0.9773151025481666, 0.9796104686548995, 0.9785314250155569, 0.9818291944276196, 0.9785144260282382, 0.9843269821757836, 0.9794981640146879, 0.98145285935085, 0.9800735744941753, 0.9749158764148057, 0.9809437386569873, 0.9783415841584159, 0.9797297297297297, 0.9802970597150652, 0.9838463882962512, 0.9824826336454243, 0.9778177458033573, 0.9807457212713936, 0.9820614168440256, 0.9798699287705172, 0.9796290665855883, 0.9790145985401459, 0.98145285935085, 0.9817393995666976, 0.9863098942128189, 0.9806646525679759, 0.9820987654320987, 0.9807344972907887, 0.9767085504137297, 0.9787556904400607, 0.9849906191369606, 0.9797359533312865, 0.9774045801526717, 0.9814068794546018, 0.9753770390889505, 0.9772587584511371, 0.9835811648079306, 0.9826571694022918, 0.9803506650544136, 0.9784718010915706, 0.977859778597786, 0.9803741183685986, 0.9846011703110563, 0.9819406183042547, 0.9787234042553191, 0.9805555555555555, 0.9818125770653514, 0.9776758409785933, 0.9833383823083914, 0.9787365177195686, 0.9798596277082697, 0.978401727861771, 0.9786144578313253, 0.9831223628691983, 0.9764237599510104, 0.9771816219549799, 0.9783536585365854, 0.9808452417148069, 0.9832009773976786, 0.9795119011750527, 0.9802955665024631, 0.983030303030303, 0.9809406701506301, 0.9780420860018298, 0.9847607436757086, 0.9801009801009801, 0.9815821256038647, 0.9831132944427388, 0.9815554872425454, 0.9766116941529236, 0.9788278613071494, 0.9785407725321889, 0.9799568300955905, 0.980067463968108, 0.980639213275968, 0.9816320385426076, 0.9825301204819277, 0.9807692307692307, 0.9799010513296228, 0.9777167277167277, 0.9796672828096118, 0.9751966122202057, 0.9779546846295162, 0.9732965009208103, 0.9802773497688752, 0.9818349753694581, 0.978997899789979, 0.9775246305418719, 0.9828873677660237, 0.9837672281776416, 0.9777020749458036, 0.9789864029666254, 0.9785473490652774, 0.9780054233202772, 0.9811145510835914, 0.9809553543552919, 0.9785341919656547, 0.977859778597786, 0.9805293580772741, 0.9827639273622654, 0.9798411728772144, 0.9818071558520315, 0.980816077953715, 0.9766536964980544, 0.9813140446051839, 0.9800796812749004, 0.9793273680962666, 0.9839950754078178, 0.9817788758492897, 0.9816849816849816, 0.9773139745916516, 0.9786389990845286, 0.9839555692687442, 0.976913730255164, 0.9832184596943362, 0.9850700792199878, 0.9762195121951219, 0.9787757613042141, 0.9852080123266564, 0.9722730042656916, 0.9721886336154776, 0.9799691833590138, 0.977286869943995, 0.9804639804639804, 0.9815157116451017, 0.9769067152841082, 0.9792441140024783, 0.9853345554537122, 0.9806689168456582, 0.9811205846528623, 0.9819350887936313, 0.9824029126213593, 0.9797047970479705, 0.9774114774114774, 0.9849693251533742, 0.9813569682151589, 0.9824182603331277, 0.981080256332011, 0.979236641221374, 0.9801611903285803, 0.9795792079207921, 0.9823439878234399, 0.9831081081081081, 0.9826536822884967, 0.9802491643877241, 0.9830665024630542, 0.9798473282442748, 0.9770253929866989, 0.9778393351800554, 0.9765287214329833, 0.9764309764309764, 0.9814420444174019, 0.9784549092028316, 0.9779749158764148, 0.9769938650306749, 0.9817795323413301, 0.9799878714372346, 0.9801242236024845, 0.98145285935085, 0.9787365177195686, 0.9820544554455446, 0.9809033040315247, 0.9792762140426848, 0.9826086956521739, 0.9837005734983398, 0.9836858006042296, 0.9800302571860817, 0.9813981398139814, 0.9795728876508821, 0.9783147459727385, 0.9795855242808537, 0.9773006134969325, 0.982542113323124, 0.9768732654949122, 0.9777235276167227, 0.9792682926829268, 0.9798657718120806, 0.9775798525798526, 0.9836413208118752, 0.9830306638880619, 0.979933110367893, 0.9812423124231242, 0.9794981640146879, 0.9770462096043492, 0.9802130898021308, 0.9800979791794244, 0.9808877928483354, 0.9833987322668276, 0.9782542113323124, 0.9849230769230769, 0.9798136645962733, 0.982874617737003, 0.9828468251579898, 0.9817901234567902, 0.9796476306196841, 0.9769975786924939, 0.9789283564118001, 0.981549815498155, 0.9786112833230006, 0.9811320754716981, 0.9787104622871047, 0.9788861689106487, 0.9790833589664718, 0.9767862526379258, 0.9785012285012284, 0.9788213627992634, 0.9818181818181818, 0.98531211750306, 0.9808801213960546, 0.9830148619957537, 0.9798512089274644, 0.9819406183042547, 0.9796784956020624, 0.9781134401972873, 0.9846011703110563, 0.9844132029339854, 0.980428134556575, 0.9807574832009774, 0.9775144333029474, 0.9831578947368421, 0.9805234327449787, 0.9787234042553191, 0.9786063569682152, 0.9816737935247404, 0.9769067152841082, 0.9776758409785933, 0.9738622386223862, 0.9780714929408231, 0.9772657450076805, 0.9796860572483841, 0.9857446163178647, 0.9822929171668667, 0.9822249463683727, 0.9799382716049383, 0.9806094182825484, 0.9817788758492897, 0.9787430683918669, 0.9820067093626106, 0.9829476248477467, 0.9817128924108504, 0.9807170834588732, 0.9796672828096118, 0.9806926141587496, 0.9837473167739957, 0.9822412737293326, 0.976915974145891, 0.9818628957885029, 0.9829631883176149, 0.9787430683918669, 0.9779189352692075, 0.9786259541984733, 0.9794478527607362, 0.9776328052190121, 0.9810687022900764, 0.9780621572212066, 0.9825474586650337, 0.979951397326853, 0.9739292364990689, 0.980319803198032, 0.9795044356072193, 0.9811148339932988, 0.9813645927261797, 0.9822738386308069, 0.9813309244203553, 0.98145285935085, 0.9817506959480359, 0.9841319228375855, 0.9824723247232472, 0.982732038236201, 0.9811090798293723, 0.9815724815724816, 0.9793018226753166, 0.9808701018204258, 0.9786194257788637, 0.9820600061862048, 0.9778332812987824, 0.9791099000908265, 0.9752747252747253, 0.9839852171234986, 0.9807926829268293, 0.9814986123959297, 0.9811721834193744, 0.9797979797979798, 0.9776974080771549, 0.9763349514563107, 0.9801405438435686, 0.9733736762481089, 0.983940704138357, 0.9810240963855422, 0.9791985316610584, 0.9802773497688752, 0.9818628957885029, 0.9828641370869033, 0.979757085020243, 0.9784522003034901, 0.983601579107197, 0.9839491217443973, 0.976183111660996, 0.9811033221578787, 0.9821263482280431, 0.979788838612368, 0.9804161566707467, 0.9783155248271528, 0.9783549783549783, 0.9756764974156279, 0.9811786485652576, 0.9794313369630974, 0.9836772405297197, 0.981155390793945, 0.9807692307692307, 0.983019450447669, 0.980410447761194, 0.9820067093626106, 0.9778665846910544, 0.9747259439707674, 0.9774733637747336, 0.9743748070392096, 0.9801980198019802, 0.9802611600364409, 0.9795981452859351, 0.9821042887997532, 0.9769805680119581, 0.9811612106238419, 0.9781420765027322, 0.9812768569674647, 0.9813626642224259, 0.9782541161851507, 0.9815567364801501, 0.9835416031697654, 0.9831907090464548, 0.9802590993214065, 0.9835365853658536, 0.9830089589125733, 0.9801344743276283, 0.9798350137488543, 0.9793846153846154, 0.9811893203883495, 0.9815668202764977, 0.9805825242718447, 0.9802070645554202, 0.9808219178082191, 0.9811379097093382, 0.9811435523114356, 0.9765315452605913, 0.9793124429571037, 0.9808975136446331, 0.9832009773976786, 0.98242956679794, 0.9813645927261797, 0.9798288508557457, 0.9802070645554202, 0.980036855036855, 0.9816289038579302, 0.9785407725321889, 0.9789698262724779, 0.9770150168556543, 0.9785867237687366, 0.9852851011649295, 0.9788754271512892, 0.9799624295554165, 0.9800302571860817, 0.9790962188748847, 0.9822792545065689, 0.9786650411459921, 0.9801526717557252, 0.9816232771822359, 0.9860521528198909, 0.9790231684408266, 0.9801742264944427, 0.9819406183042547, 0.9811148339932988, 0.982412835544585, 0.9850335980452046, 0.9767090139140956, 0.9763295419612665, 0.9795731707317074, 0.9831958447907119, 0.9793289394847214, 0.9813025864755376, 0.9742489270386266, 0.9777577397054403, 0.981549815498155, 0.977217496962333, 0.9826431181485993, 0.9802551640340219, 0.98318557016203, 0.9823474759987613, 0.9788902291917974, 0.9814194334450198, 0.9739156809220504, 0.9807162534435262, 0.984249536751081, 0.9786377708978328, 0.9820655534941249, 0.9811263318112633, 0.9824082499241735, 0.9834305001534213, 0.9784522003034901, 0.9777031154551008, 0.9760024301336574, 0.9842883548983364, 0.9792682926829268, 0.9786910197869102, 0.9833897262380806, 0.9771341463414634, 0.9808801213960546, 0.9826571694022918, 0.9808394160583942, 0.9818851704022106, 0.9808102345415778, 0.9793227449805214, 0.9787742899850523, 0.9824189148226735, 0.9821373575608254, 0.9841722063944286, 0.9767868051313379, 0.9836462749848577, 0.980639213275968, 0.9776951672862454, 0.9848391089108911, 0.9769372693726938, 0.9817128924108504, 0.9831029185867896, 0.9799635701275046, 0.9808701018204258, 0.9795357361026268, 0.9815969130305728, 0.9813854134879463, 0.9795918367346939, 0.9750148720999405, 0.9763277693474962, 0.9833692639359408, 0.9784522003034901, 0.9806354009077156, 0.9778120184899846, 0.9814929056138186, 0.9799696509863429, 0.9780999383096853, 0.9835315645013724, 0.9785238959467635, 0.9834152334152334, 0.9811884144520753, 0.9846295727021211, 0.9804937519049071, 0.9868098159509202, 0.9825954198473282, 0.9822900763358778, 0.9835811648079306, 0.9783206106870229, 0.9812909260991581, 0.9797338173018754, 0.980439362022269, 0.9841704718417047, 0.9748234571691741, 0.9816289038579302, 0.977621091354997, 0.9817629179331308, 0.9776004909481436, 0.980422147445702, 0.9805411979325023, 0.9785932721712538, 0.9811836115326252, 0.9810617820552623, 0.9823815309842041, 0.9810240963855422, 0.9852625115136628, 0.9772727272727273, 0.9818461538461538, 0.9797460701330108, 0.9804275820535984, 0.9784681636419563, 0.9823278488726387, 0.9846719803801349, 0.9776839565741857, 0.9795602353669867, 0.9804580152671756, 0.9779883827575665, 0.9800918836140888, 0.98, 0.979064039408867, 0.9782082324455206, 0.9829631883176149, 0.9819737244118546, 0.979211914365498, 0.9813341493268054, 0.983944259315359, 0.9831546707503829, 0.9799938442597722, 0.979064039408867, 0.9822792545065689, 0.9838954725007596, 0.9863180297962907, 0.9791218913110225, 0.9810571341277116, 0.9770755885997522, 0.9777845404747413, 0.9790273556231003, 0.9746300211416491, 0.9821538461538462, 0.9835658914728682, 0.982015503875969, 0.98171109733416, 0.9773783700030989, 0.9812914906457453, 0.9822140447715425, 0.9822609741431149, 0.9795918367346939, 0.9756616975965926, 0.9781863508881271, 0.9829993928354585, 0.9818181818181818, 0.9786650411459921, 0.983787090853472, 0.9790704832256079, 0.9814194334450198, 0.9812178127840049, 0.9823654606263302, 0.980561555075594, 0.9814589665653496, 0.9781818181818182, 0.9777571825764597, 0.9808877928483354, 0.9844748858447488, 0.9790511398644486, 0.9809379727685326, 0.9767085504137297, 0.9820766378244746, 0.9824399260628466, 0.9774870702768482, 0.9785498489425982, 0.9800307219662059, 0.9816063764561619, 0.9793909566287297, 0.9828009828009828, 0.9825353809093647, 0.9801950030469226, 0.9862047823421214, 0.9836369249768447, 0.9786975045648204, 0.9860335195530726, 0.9759407772979642, 0.9829320329167937, 0.9841075794621027, 0.9800123001230012, 0.9787169352386744, 0.980386147716825, 0.9837153196622437, 0.9774343122102009, 0.9802671523983, 0.9814081072843646, 0.9798780487804878, 0.9802970597150652, 0.9763948497854077, 0.9788861689106487, 0.9797338173018754, 0.977845220030349, 0.9796547472256474, 0.9831391784181484, 0.9753799392097264, 0.980275516593613, 0.9803741183685986, 0.9783616692426584, 0.9743034055727554, 0.978448275862069, 0.9810583283223091, 0.9794478527607362, 0.9765793528505393, 0.9823224626638221, 0.9785078292907584, 0.9820067093626106, 0.9813740458015268, 0.979951397326853, 0.9810917962793535, 0.9824507389162561, 0.9785932721712538, 0.980763358778626, 0.9809264305177112, 0.9806511056511057, 0.978807145019679, 0.9803921568627451, 0.9831752829611502, 0.980440097799511, 0.9787953288260602, 0.9800060587700696, 0.980049109883364, 0.9796547472256474, 0.9815894446149126, 0.9811145510835914, 0.977545370655183, 0.9806689168456582, 0.9789156626506024, 0.978108847674065, 0.9837622549019608, 0.9834457388105456, 0.9813854134879463, 0.9797705314009661, 0.9819032761310452, 0.9803801348865726, 0.9797608095676172, 0.9771015366074118, 0.98, 0.9835013748854262, 0.9780253791395853, 0.9790897908979089, 0.981195025781013, 0.9804408568767463, 0.9799209005171889, 0.9819370912488321, 0.9811612106238419, 0.9756394640682094, 0.9822085889570552, 0.9811721834193744, 0.9828431372549019, 0.9792619701128393, 0.9819548872180451, 0.986244019138756, 0.9834101382488479, 0.9815151515151516, 0.9811437403400309, 0.9815157116451017, 0.979739945570003, 0.9829164124466138, 0.9802671523983, 0.9804511278195489, 0.9806511056511057, 0.9831804281345565, 0.9784063260340633, 0.9795057263411694, 0.980852378011118, 0.9821593355890496, 0.9812018489984592, 0.9819406183042547, 0.9832141746969226, 0.9830403946962689, 0.9833333333333333, 0.9784681636419563, 0.9831172746457643, 0.9799506477483035, 0.9801344743276283, 0.983243566726511, 0.9776279497395035, 0.9794604537093807, 0.9787888103289272, 0.9777020749458036, 0.9792302993280391, 0.9787365177195686, 0.9803089972735535, 0.9779141104294479, 0.9744379427163535, 0.981733746130031, 0.982858891949801, 0.9811320754716981, 0.9812348668280871, 0.9795348837209302, 0.9783602560195063, 0.9824290998766955, 0.9866057838660578, 0.9810513447432763, 0.9795158286778398, 0.9804460739382829, 0.9810617820552623, 0.9808510638297873, 0.9812653562653563, 0.980517503805175, 0.9757060431217734, 0.9824938574938575, 0.98516228748068, 0.9775315481686673, 0.9826060421116876, 0.982412835544585, 0.9836907278767744, 0.9806763285024155, 0.9807926829268293, 0.9826536822884967, 0.9796040791841631, 0.9783817171093268, 0.979064039408867, 0.9830097087378641, 0.9811087023846392, 0.9795348837209302, 0.9779816513761468, 0.980110159118727, 0.9795482295482295, 0.9776674937965261, 0.9813309244203553, 0.9814533292794162, 0.9790897908979089, 0.981280193236715, 0.9802773497688752, 0.9818566676746295, 0.9819737244118546, 0.9762412427657631, 0.9790446841294299, 0.9810911345319281, 0.9806451612903225, 0.9783668494820231, 0.9776894865525673, 0.980428134556575, 0.9806704922983993, 0.9773619076365831, 0.9809172052939366, 0.9757371007371007, 0.9796661608497724, 0.9789377289377289, 0.9804041641151255, 0.9824290998766955, 0.9807162534435262, 0.9786530366806976, 0.9792583280955374, 0.9785998165698563, 0.9774114774114774, 0.9793751895662723, 0.9821703043344605, 0.9834963325183375, 0.9783007334963325, 0.9816485225505444, 0.9828622970535177, 0.9768715763846622, 0.980883735506111, 0.9796414463688848, 0.9769067152841082, 0.9784217016029593, 0.9787045252883763, 0.9744707347447074, 0.9810975609756097, 0.9810860280658938, 0.9783602560195063, 0.9838660578386605, 0.9796476306196841, 0.9776073619631902, 0.9777365491651205, 0.9819902319902319, 0.9794933655006032, 0.9814356435643564, 0.9815724815724816, 0.9819406183042547, 0.9820176775373362, 0.9814927184466019, 0.9808452417148069, 0.9824189148226735, 0.981651376146789, 0.9794478527607362, 0.9782141761276465, 0.9853300733496333, 0.9840637450199203, 0.9793689320388349, 0.9772589448150394, 0.9777175549533274, 0.9800429843414185, 0.9766737352317479, 0.9763779527559056, 0.9847684177805409, 0.9824992324224747, 0.9782541161851507, 0.9831907090464548, 0.9810571341277116, 0.9802955665024631, 0.9813606710158435, 0.9800809212573919, 0.981155390793945, 0.9760460885385082, 0.9812178127840049, 0.9775075987841946, 0.9785238959467635, 0.9787234042553191, 0.9814194334450198, 0.9813316739265713, 0.9795792079207921, 0.9791538933169834, 0.9860394537177541, 0.9804101622283441, 0.983030303030303, 0.9831495098039216, 0.9818851704022106, 0.9828641370869033, 0.9840686274509803]\n",
            "sp: [0.977639751552795, 0.9795348837209302, 0.9777710109622412, 0.9799878714372346, 0.9765315452605913, 0.9783602560195063, 0.9802102659245516, 0.9841027208804647, 0.9845440494590417, 0.9761401389308366, 0.9846708746618575, 0.979147500766636, 0.9778720824492271, 0.9781941031941032, 0.9830508474576272, 0.9834558823529411, 0.9825248568846038, 0.9778519417475728, 0.9816905706438815, 0.9789890377588307, 0.9768946395563771, 0.9795857099969979, 0.9789669038045159, 0.9786128933699969, 0.9806060606060606, 0.9792682926829268, 0.980840543881335, 0.9813365442504516, 0.98033093974399, 0.9781454886044333, 0.9815950920245399, 0.9791666666666666, 0.9825581395348837, 0.9807103490508267, 0.9789634146341464, 0.9803076923076923, 0.9781287970838396, 0.9817850637522769, 0.9835816357555488, 0.9806001818732949, 0.9800796812749004, 0.9807033924680983, 0.9795918367346939, 0.9825109306683323, 0.98, 0.9862628785513581, 0.98171109733416, 0.977947932618683, 0.9803379416282642, 0.9796476306196841, 0.9836621454993835, 0.9813196746007834, 0.9773075743636921, 0.9768160741885626, 0.9792239535594256, 0.9805234327449787, 0.9791282995702886, 0.9785043899485316, 0.9781336618417, 0.9800181653042689, 0.981874039938556, 0.9769514443761524, 0.9840745192307693, 0.9800857843137255, 0.9758261933904528, 0.9779749158764148, 0.9826325411334552, 0.9785498489425982, 0.9831804281345565, 0.9785539215686274, 0.9743510506798516, 0.9749320446994866, 0.978494623655914, 0.9791985316610584, 0.9803016312711603, 0.9786466165413534, 0.9818851704022106, 0.9858635525507068, 0.9795294836541399, 0.9802791262135923, 0.9820655534941249, 0.9845173041894353, 0.9782608695652174, 0.982148353339489, 0.9812291855888586, 0.9769017554665845, 0.9773838630806846, 0.9814173228346457, 0.9790962188748847, 0.979223125564589, 0.9777710109622412, 0.9849277145493694, 0.9752245277175596, 0.9771039603960396, 0.9759556103575833, 0.9811028500619579, 0.9769627159745378, 0.977363107984093, 0.9832317073170732, 0.9800184445127574, 0.9812235009085403, 0.9814929056138186, 0.9794456555590159, 0.9834457388105456, 0.9793438639125152, 0.9770079705702023, 0.9846484494933988, 0.9789283564118001, 0.9855421686746988, 0.9779722389861195, 0.982131854590265, 0.9805855161787366, 0.9815682599187754, 0.9842662632375189, 0.9814646004254026, 0.9789734075448361, 0.975811390079608, 0.9828431372549019, 0.9801796221740476, 0.9748234571691741, 0.9813981398139814, 0.9833999385182908, 0.9824720459353279, 0.9789055334760012, 0.9754751686082158, 0.9750692520775623, 0.9782016348773842, 0.9810223446587083, 0.9822706065318818, 0.9794918885827977, 0.9799444615859303, 0.9820012202562538, 0.97821482602118, 0.9802408150663785, 0.9807985370313929, 0.9790464621925296, 0.9806332616046726, 0.9822074788902292, 0.9778048038917604, 0.9757371007371007, 0.9819571865443425, 0.9824402058734484, 0.9796923076923076, 0.9798798798798799, 0.9806689168456582, 0.9839357429718876, 0.9811893203883495, 0.9810028028651511, 0.9833538840937115, 0.9819902319902319, 0.9796538111144852, 0.9802530083307621, 0.9829164124466138, 0.9814024390243903, 0.9718693284936479, 0.9790845710821461, 0.9832661915091416, 0.9798657718120806, 0.9770462096043492, 0.9849785407725322, 0.9821484497337927, 0.9807457212713936, 0.9850609756097561, 0.9819406183042547, 0.9825260576333538, 0.9816098884534218, 0.9810264385692068, 0.9841560024375381, 0.9796969696969697, 0.9762989972652689, 0.9810165339865279, 0.9819059107358263, 0.9783013019218847, 0.9824614454188086, 0.9798449612403101, 0.9818461538461538, 0.9833987322668276, 0.976730129948625, 0.9818851704022106, 0.9814194334450198, 0.978756157635468, 0.9795792079207921, 0.9791794243723209, 0.9809897404948702, 0.9776348039215687, 0.9812745394140743, 0.9835616438356164, 0.9790782292298362, 0.9796414463688848, 0.9804275820535984, 0.9813490829965806, 0.9835505896958411, 0.9811659192825112, 0.9800613496932515, 0.9816120134845234, 0.9761092150170648, 0.9787363304981774, 0.9823439878234399, 0.9828903146959975, 0.9834203254528707, 0.9789055334760012, 0.984092994799633, 0.9796421961752005, 0.9810676598386096, 0.9828880216151306, 0.9767299448867116, 0.9842041312272175, 0.9822194972409565, 0.9798350137488543, 0.9812768569674647, 0.9811492854971116, 0.9810165339865279, 0.9766441303011678, 0.9783668494820231, 0.9805648344974187, 0.9853166105842766, 0.9787621359223301, 0.9819113656918903, 0.9819931698230363, 0.974907063197026, 0.9830354438049076, 0.9824236817761333, 0.9751289050652108, 0.98322659347362, 0.9773151025481666, 0.9796104686548995, 0.9785314250155569, 0.9818291944276196, 0.9785144260282382, 0.9843269821757836, 0.9794981640146879, 0.98145285935085, 0.9800735744941753, 0.9749158764148057, 0.9809437386569873, 0.9783415841584159, 0.9797297297297297, 0.9802970597150652, 0.9838463882962512, 0.9824826336454243, 0.9778177458033573, 0.9807457212713936, 0.9820614168440256, 0.9798699287705172, 0.9796290665855883, 0.9790145985401459, 0.98145285935085, 0.9817393995666976, 0.9863098942128189, 0.9806646525679759, 0.9820987654320987, 0.9807344972907887, 0.9767085504137297, 0.9787556904400607, 0.9849906191369606, 0.9797359533312865, 0.9774045801526717, 0.9814068794546018, 0.9753770390889505, 0.9772587584511371, 0.9835811648079306, 0.9826571694022918, 0.9803506650544136, 0.9784718010915706, 0.977859778597786, 0.9803741183685986, 0.9846011703110563, 0.9819406183042547, 0.9787234042553191, 0.9805555555555555, 0.9818125770653514, 0.9776758409785933, 0.9833383823083914, 0.9787365177195686, 0.9798596277082697, 0.978401727861771, 0.9786144578313253, 0.9831223628691983, 0.9764237599510104, 0.9771816219549799, 0.9783536585365854, 0.9808452417148069, 0.9832009773976786, 0.9795119011750527, 0.9802955665024631, 0.983030303030303, 0.9809406701506301, 0.9780420860018298, 0.9847607436757086, 0.9801009801009801, 0.9815821256038647, 0.9831132944427388, 0.9815554872425454, 0.9766116941529236, 0.9788278613071494, 0.9785407725321889, 0.9799568300955905, 0.980067463968108, 0.980639213275968, 0.9816320385426076, 0.9825301204819277, 0.9807692307692307, 0.9799010513296228, 0.9777167277167277, 0.9796672828096118, 0.9751966122202057, 0.9779546846295162, 0.9732965009208103, 0.9802773497688752, 0.9818349753694581, 0.978997899789979, 0.9775246305418719, 0.9828873677660237, 0.9837672281776416, 0.9777020749458036, 0.9789864029666254, 0.9785473490652774, 0.9780054233202772, 0.9811145510835914, 0.9809553543552919, 0.9785341919656547, 0.977859778597786, 0.9805293580772741, 0.9827639273622654, 0.9798411728772144, 0.9818071558520315, 0.980816077953715, 0.9766536964980544, 0.9813140446051839, 0.9800796812749004, 0.9793273680962666, 0.9839950754078178, 0.9817788758492897, 0.9816849816849816, 0.9773139745916516, 0.9786389990845286, 0.9839555692687442, 0.976913730255164, 0.9832184596943362, 0.9850700792199878, 0.9762195121951219, 0.9787757613042141, 0.9852080123266564, 0.9722730042656916, 0.9721886336154776, 0.9799691833590138, 0.977286869943995, 0.9804639804639804, 0.9815157116451017, 0.9769067152841082, 0.9792441140024783, 0.9853345554537122, 0.9806689168456582, 0.9811205846528623, 0.9819350887936313, 0.9824029126213593, 0.9797047970479705, 0.9774114774114774, 0.9849693251533742, 0.9813569682151589, 0.9824182603331277, 0.981080256332011, 0.979236641221374, 0.9801611903285803, 0.9795792079207921, 0.9823439878234399, 0.9831081081081081, 0.9826536822884967, 0.9802491643877241, 0.9830665024630542, 0.9798473282442748, 0.9770253929866989, 0.9778393351800554, 0.9765287214329833, 0.9764309764309764, 0.9814420444174019, 0.9784549092028316, 0.9779749158764148, 0.9769938650306749, 0.9817795323413301, 0.9799878714372346, 0.9801242236024845, 0.98145285935085, 0.9787365177195686, 0.9820544554455446, 0.9809033040315247, 0.9792762140426848, 0.9826086956521739, 0.9837005734983398, 0.9836858006042296, 0.9800302571860817, 0.9813981398139814, 0.9795728876508821, 0.9783147459727385, 0.9795855242808537, 0.9773006134969325, 0.982542113323124, 0.9768732654949122, 0.9777235276167227, 0.9792682926829268, 0.9798657718120806, 0.9775798525798526, 0.9836413208118752, 0.9830306638880619, 0.979933110367893, 0.9812423124231242, 0.9794981640146879, 0.9770462096043492, 0.9802130898021308, 0.9800979791794244, 0.9808877928483354, 0.9833987322668276, 0.9782542113323124, 0.9849230769230769, 0.9798136645962733, 0.982874617737003, 0.9828468251579898, 0.9817901234567902, 0.9796476306196841, 0.9769975786924939, 0.9789283564118001, 0.981549815498155, 0.9786112833230006, 0.9811320754716981, 0.9787104622871047, 0.9788861689106487, 0.9790833589664718, 0.9767862526379258, 0.9785012285012284, 0.9788213627992634, 0.9818181818181818, 0.98531211750306, 0.9808801213960546, 0.9830148619957537, 0.9798512089274644, 0.9819406183042547, 0.9796784956020624, 0.9781134401972873, 0.9846011703110563, 0.9844132029339854, 0.980428134556575, 0.9807574832009774, 0.9775144333029474, 0.9831578947368421, 0.9805234327449787, 0.9787234042553191, 0.9786063569682152, 0.9816737935247404, 0.9769067152841082, 0.9776758409785933, 0.9738622386223862, 0.9780714929408231, 0.9772657450076805, 0.9796860572483841, 0.9857446163178647, 0.9822929171668667, 0.9822249463683727, 0.9799382716049383, 0.9806094182825484, 0.9817788758492897, 0.9787430683918669, 0.9820067093626106, 0.9829476248477467, 0.9817128924108504, 0.9807170834588732, 0.9796672828096118, 0.9806926141587496, 0.9837473167739957, 0.9822412737293326, 0.976915974145891, 0.9818628957885029, 0.9829631883176149, 0.9787430683918669, 0.9779189352692075, 0.9786259541984733, 0.9794478527607362, 0.9776328052190121, 0.9810687022900764, 0.9780621572212066, 0.9825474586650337, 0.979951397326853, 0.9739292364990689, 0.980319803198032, 0.9795044356072193, 0.9811148339932988, 0.9813645927261797, 0.9822738386308069, 0.9813309244203553, 0.98145285935085, 0.9817506959480359, 0.9841319228375855, 0.9824723247232472, 0.982732038236201, 0.9811090798293723, 0.9815724815724816, 0.9793018226753166, 0.9808701018204258, 0.9786194257788637, 0.9820600061862048, 0.9778332812987824, 0.9791099000908265, 0.9752747252747253, 0.9839852171234986, 0.9807926829268293, 0.9814986123959297, 0.9811721834193744, 0.9797979797979798, 0.9776974080771549, 0.9763349514563107, 0.9801405438435686, 0.9733736762481089, 0.983940704138357, 0.9810240963855422, 0.9791985316610584, 0.9802773497688752, 0.9818628957885029, 0.9828641370869033, 0.979757085020243, 0.9784522003034901, 0.983601579107197, 0.9839491217443973, 0.976183111660996, 0.9811033221578787, 0.9821263482280431, 0.979788838612368, 0.9804161566707467, 0.9783155248271528, 0.9783549783549783, 0.9756764974156279, 0.9811786485652576, 0.9794313369630974, 0.9836772405297197, 0.981155390793945, 0.9807692307692307, 0.983019450447669, 0.980410447761194, 0.9820067093626106, 0.9778665846910544, 0.9747259439707674, 0.9774733637747336, 0.9743748070392096, 0.9801980198019802, 0.9802611600364409, 0.9795981452859351, 0.9821042887997532, 0.9769805680119581, 0.9811612106238419, 0.9781420765027322, 0.9812768569674647, 0.9813626642224259, 0.9782541161851507, 0.9815567364801501, 0.9835416031697654, 0.9831907090464548, 0.9802590993214065, 0.9835365853658536, 0.9830089589125733, 0.9801344743276283, 0.9798350137488543, 0.9793846153846154, 0.9811893203883495, 0.9815668202764977, 0.9805825242718447, 0.9802070645554202, 0.9808219178082191, 0.9811379097093382, 0.9811435523114356, 0.9765315452605913, 0.9793124429571037, 0.9808975136446331, 0.9832009773976786, 0.98242956679794, 0.9813645927261797, 0.9798288508557457, 0.9802070645554202, 0.980036855036855, 0.9816289038579302, 0.9785407725321889, 0.9789698262724779, 0.9770150168556543, 0.9785867237687366, 0.9852851011649295, 0.9788754271512892, 0.9799624295554165, 0.9800302571860817, 0.9790962188748847, 0.9822792545065689, 0.9786650411459921, 0.9801526717557252, 0.9816232771822359, 0.9860521528198909, 0.9790231684408266, 0.9801742264944427, 0.9819406183042547, 0.9811148339932988, 0.982412835544585, 0.9850335980452046, 0.9767090139140956, 0.9763295419612665, 0.9795731707317074, 0.9831958447907119, 0.9793289394847214, 0.9813025864755376, 0.9742489270386266, 0.9777577397054403, 0.981549815498155, 0.977217496962333, 0.9826431181485993, 0.9802551640340219, 0.98318557016203, 0.9823474759987613, 0.9788902291917974, 0.9814194334450198, 0.9739156809220504, 0.9807162534435262, 0.984249536751081, 0.9786377708978328, 0.9820655534941249, 0.9811263318112633, 0.9824082499241735, 0.9834305001534213, 0.9784522003034901, 0.9777031154551008, 0.9760024301336574, 0.9842883548983364, 0.9792682926829268, 0.9786910197869102, 0.9833897262380806, 0.9771341463414634, 0.9808801213960546, 0.9826571694022918, 0.9808394160583942, 0.9818851704022106, 0.9808102345415778, 0.9793227449805214, 0.9787742899850523, 0.9824189148226735, 0.9821373575608254, 0.9841722063944286, 0.9767868051313379, 0.9836462749848577, 0.980639213275968, 0.9776951672862454, 0.9848391089108911, 0.9769372693726938, 0.9817128924108504, 0.9831029185867896, 0.9799635701275046, 0.9808701018204258, 0.9795357361026268, 0.9815969130305728, 0.9813854134879463, 0.9795918367346939, 0.9750148720999405, 0.9763277693474962, 0.9833692639359408, 0.9784522003034901, 0.9806354009077156, 0.9778120184899846, 0.9814929056138186, 0.9799696509863429, 0.9780999383096853, 0.9835315645013724, 0.9785238959467635, 0.9834152334152334, 0.9811884144520753, 0.9846295727021211, 0.9804937519049071, 0.9868098159509202, 0.9825954198473282, 0.9822900763358778, 0.9835811648079306, 0.9783206106870229, 0.9812909260991581, 0.9797338173018754, 0.980439362022269, 0.9841704718417047, 0.9748234571691741, 0.9816289038579302, 0.977621091354997, 0.9817629179331308, 0.9776004909481436, 0.980422147445702, 0.9805411979325023, 0.9785932721712538, 0.9811836115326252, 0.9810617820552623, 0.9823815309842041, 0.9810240963855422, 0.9852625115136628, 0.9772727272727273, 0.9818461538461538, 0.9797460701330108, 0.9804275820535984, 0.9784681636419563, 0.9823278488726387, 0.9846719803801349, 0.9776839565741857, 0.9795602353669867, 0.9804580152671756, 0.9779883827575665, 0.9800918836140888, 0.98, 0.979064039408867, 0.9782082324455206, 0.9829631883176149, 0.9819737244118546, 0.979211914365498, 0.9813341493268054, 0.983944259315359, 0.9831546707503829, 0.9799938442597722, 0.979064039408867, 0.9822792545065689, 0.9838954725007596, 0.9863180297962907, 0.9791218913110225, 0.9810571341277116, 0.9770755885997522, 0.9777845404747413, 0.9790273556231003, 0.9746300211416491, 0.9821538461538462, 0.9835658914728682, 0.982015503875969, 0.98171109733416, 0.9773783700030989, 0.9812914906457453, 0.9822140447715425, 0.9822609741431149, 0.9795918367346939, 0.9756616975965926, 0.9781863508881271, 0.9829993928354585, 0.9818181818181818, 0.9786650411459921, 0.983787090853472, 0.9790704832256079, 0.9814194334450198, 0.9812178127840049, 0.9823654606263302, 0.980561555075594, 0.9814589665653496, 0.9781818181818182, 0.9777571825764597, 0.9808877928483354, 0.9844748858447488, 0.9790511398644486, 0.9809379727685326, 0.9767085504137297, 0.9820766378244746, 0.9824399260628466, 0.9774870702768482, 0.9785498489425982, 0.9800307219662059, 0.9816063764561619, 0.9793909566287297, 0.9828009828009828, 0.9825353809093647, 0.9801950030469226, 0.9862047823421214, 0.9836369249768447, 0.9786975045648204, 0.9860335195530726, 0.9759407772979642, 0.9829320329167937, 0.9841075794621027, 0.9800123001230012, 0.9787169352386744, 0.980386147716825, 0.9837153196622437, 0.9774343122102009, 0.9802671523983, 0.9814081072843646, 0.9798780487804878, 0.9802970597150652, 0.9763948497854077, 0.9788861689106487, 0.9797338173018754, 0.977845220030349, 0.9796547472256474, 0.9831391784181484, 0.9753799392097264, 0.980275516593613, 0.9803741183685986, 0.9783616692426584, 0.9743034055727554, 0.978448275862069, 0.9810583283223091, 0.9794478527607362, 0.9765793528505393, 0.9823224626638221, 0.9785078292907584, 0.9820067093626106, 0.9813740458015268, 0.979951397326853, 0.9810917962793535, 0.9824507389162561, 0.9785932721712538, 0.980763358778626, 0.9809264305177112, 0.9806511056511057, 0.978807145019679, 0.9803921568627451, 0.9831752829611502, 0.980440097799511, 0.9787953288260602, 0.9800060587700696, 0.980049109883364, 0.9796547472256474, 0.9815894446149126, 0.9811145510835914, 0.977545370655183, 0.9806689168456582, 0.9789156626506024, 0.978108847674065, 0.9837622549019608, 0.9834457388105456, 0.9813854134879463, 0.9797705314009661, 0.9819032761310452, 0.9803801348865726, 0.9797608095676172, 0.9771015366074118, 0.98, 0.9835013748854262, 0.9780253791395853, 0.9790897908979089, 0.981195025781013, 0.9804408568767463, 0.9799209005171889, 0.9819370912488321, 0.9811612106238419, 0.9756394640682094, 0.9822085889570552, 0.9811721834193744, 0.9828431372549019, 0.9792619701128393, 0.9819548872180451, 0.986244019138756, 0.9834101382488479, 0.9815151515151516, 0.9811437403400309, 0.9815157116451017, 0.979739945570003, 0.9829164124466138, 0.9802671523983, 0.9804511278195489, 0.9806511056511057, 0.9831804281345565, 0.9784063260340633, 0.9795057263411694, 0.980852378011118, 0.9821593355890496, 0.9812018489984592, 0.9819406183042547, 0.9832141746969226, 0.9830403946962689, 0.9833333333333333, 0.9784681636419563, 0.9831172746457643, 0.9799506477483035, 0.9801344743276283, 0.983243566726511, 0.9776279497395035, 0.9794604537093807, 0.9787888103289272, 0.9777020749458036, 0.9792302993280391, 0.9787365177195686, 0.9803089972735535, 0.9779141104294479, 0.9744379427163535, 0.981733746130031, 0.982858891949801, 0.9811320754716981, 0.9812348668280871, 0.9795348837209302, 0.9783602560195063, 0.9824290998766955, 0.9866057838660578, 0.9810513447432763, 0.9795158286778398, 0.9804460739382829, 0.9810617820552623, 0.9808510638297873, 0.9812653562653563, 0.980517503805175, 0.9757060431217734, 0.9824938574938575, 0.98516228748068, 0.9775315481686673, 0.9826060421116876, 0.982412835544585, 0.9836907278767744, 0.9806763285024155, 0.9807926829268293, 0.9826536822884967, 0.9796040791841631, 0.9783817171093268, 0.979064039408867, 0.9830097087378641, 0.9811087023846392, 0.9795348837209302, 0.9779816513761468, 0.980110159118727, 0.9795482295482295, 0.9776674937965261, 0.9813309244203553, 0.9814533292794162, 0.9790897908979089, 0.981280193236715, 0.9802773497688752, 0.9818566676746295, 0.9819737244118546, 0.9762412427657631, 0.9790446841294299, 0.9810911345319281, 0.9806451612903225, 0.9783668494820231, 0.9776894865525673, 0.980428134556575, 0.9806704922983993, 0.9773619076365831, 0.9809172052939366, 0.9757371007371007, 0.9796661608497724, 0.9789377289377289, 0.9804041641151255, 0.9824290998766955, 0.9807162534435262, 0.9786530366806976, 0.9792583280955374, 0.9785998165698563, 0.9774114774114774, 0.9793751895662723, 0.9821703043344605, 0.9834963325183375, 0.9783007334963325, 0.9816485225505444, 0.9828622970535177, 0.9768715763846622, 0.980883735506111, 0.9796414463688848, 0.9769067152841082, 0.9784217016029593, 0.9787045252883763, 0.9744707347447074, 0.9810975609756097, 0.9810860280658938, 0.9783602560195063, 0.9838660578386605, 0.9796476306196841, 0.9776073619631902, 0.9777365491651205, 0.9819902319902319, 0.9794933655006032, 0.9814356435643564, 0.9815724815724816, 0.9819406183042547, 0.9820176775373362, 0.9814927184466019, 0.9808452417148069, 0.9824189148226735, 0.981651376146789, 0.9794478527607362, 0.9782141761276465, 0.9853300733496333, 0.9840637450199203, 0.9793689320388349, 0.9772589448150394, 0.9777175549533274, 0.9800429843414185, 0.9766737352317479, 0.9763779527559056, 0.9847684177805409, 0.9824992324224747, 0.9782541161851507, 0.9831907090464548, 0.9810571341277116, 0.9802955665024631, 0.9813606710158435, 0.9800809212573919, 0.981155390793945, 0.9760460885385082, 0.9812178127840049, 0.9775075987841946, 0.9785238959467635, 0.9787234042553191, 0.9814194334450198, 0.9813316739265713, 0.9795792079207921, 0.9791538933169834, 0.9860394537177541, 0.9804101622283441, 0.983030303030303, 0.9831495098039216, 0.9818851704022106, 0.9828641370869033, 0.9840686274509803]\n",
            "sd_acc: 0.0020753019562981022\n",
            "sd_f1: 0.001479106402750087\n",
            "sd_mcc: 0.005008733855549987\n",
            "sd_sn: 0.0024214045603024085\n",
            "sd_sp: 0.0024214045603024085\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.98      0.96      1323\n",
            "           1       0.99      0.98      0.99      3270\n",
            "\n",
            "    accuracy                           0.98      4593\n",
            "   macro avg       0.97      0.98      0.97      4593\n",
            "weighted avg       0.98      0.98      0.98      4593\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.0020753019562981022, 0.005008733855549987, 0.001479106402750087)"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predicted_training_labels = classifier.predict(train_dataset)\n",
        "error_rate(training_labels, predicted_training_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-It8jLsCj9dy",
        "outputId": "abbf1ca5-9234-4b41-8d07-e5cab4902e05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "36/36 [==============================] - 1s 28ms/step\n",
            "Our model has an accuracy of 0.88\n"
          ]
        }
      ],
      "source": [
        "# Basic Protocol 3 — Step 10\n",
        "\n",
        "predicted_cnn = classifier.predict(test_dataset)\n",
        "accuracy = accuracy_score(testing_labels, predicted_cnn)\n",
        "\n",
        "print(f\"Our model has an accuracy of {accuracy:.2}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45PkomX_j9dy",
        "outputId": "49f858a1-0468-45c9-b610-937e78bf1065"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "acc: 0.8833768494342907\n",
            "f1: 0.917283950617284\n",
            "mcc: 0.7201125653725319\n",
            "sn: [0.9123020706455542, 0.8907563025210085, 0.9176904176904177, 0.8915375446960667, 0.9185905224787363, 0.928921568627451, 0.907563025210084, 0.8989037758830695, 0.9124087591240876, 0.9203860072376358, 0.9079601990049752, 0.9257425742574258, 0.9286592865928659, 0.9055489964580874, 0.9148936170212766, 0.9216867469879518, 0.9051724137931034, 0.91625, 0.9098660170523751, 0.9036144578313253, 0.8928571428571429, 0.9145299145299145, 0.9228855721393034, 0.9109263657957245, 0.8969325153374234, 0.9056122448979592, 0.8952020202020202, 0.9206145966709347, 0.9077669902912622, 0.8996235884567126, 0.9085365853658537, 0.8924205378973105, 0.9044740024183797, 0.8989169675090253, 0.9097651421508035, 0.90920245398773, 0.9208899876390606, 0.9285714285714286, 0.9266503667481663, 0.9118727050183598, 0.9124537607891492, 0.9090909090909091, 0.9172661870503597, 0.8985680190930787, 0.9161528976572133, 0.9115479115479116, 0.8852258852258852, 0.9083129584352079, 0.9101941747572816, 0.8914354644149578, 0.9132530120481928, 0.9143206854345165, 0.9339152119700748, 0.8929016189290162, 0.9039615846338536, 0.890840652446675, 0.9120095124851367, 0.9027283511269276, 0.9089805825242718, 0.9048780487804878, 0.9205776173285198, 0.9131985731272295, 0.9042944785276074, 0.8994011976047904, 0.9135654261704682, 0.9331713244228432, 0.899880810488677, 0.9184177997527813, 0.9021739130434783, 0.9062119366626066, 0.9227941176470589, 0.9040097205346294, 0.9010025062656641, 0.9073405535499398, 0.9174876847290641, 0.9164677804295943, 0.8974668275030157, 0.9133016627078385, 0.909541511771995, 0.8807453416149068, 0.9157372986369269, 0.90375, 0.926829268292683, 0.9020310633213859, 0.9085365853658537, 0.9236363636363636, 0.9069767441860465, 0.902948402948403, 0.9125615763546798, 0.9084249084249084, 0.8950617283950617, 0.8990147783251231, 0.9006289308176101, 0.9176904176904177, 0.9077490774907749, 0.91875, 0.9090909090909091, 0.9203860072376358, 0.8991793669402111, 0.9058402860548271, 0.9077844311377246, 0.9301745635910225, 0.902557856272838, 0.9129383313180169, 0.9221105527638191, 0.9057527539779682, 0.915576694411415, 0.901840490797546, 0.9136253041362531, 0.9072532699167658, 0.9082007343941249, 0.9073405535499398, 0.8973429951690821, 0.8880688806888068, 0.9101654846335697, 0.9121706398996235, 0.9037758830694276, 0.9038929440389294, 0.9229813664596274, 0.9037758830694276, 0.8987654320987655, 0.902676399026764, 0.9003601440576231, 0.9078455790784558, 0.9196891191709845, 0.8946716232961586, 0.9085290482076638, 0.8917274939172749, 0.8822815533980582, 0.9004914004914005, 0.9195849546044098, 0.9089820359281438, 0.9100850546780073, 0.9102091020910209, 0.8992628992628993, 0.9152542372881356, 0.9192399049881235, 0.9074519230769231, 0.9134146341463415, 0.8988621997471555, 0.9248554913294798, 0.9127272727272727, 0.9152334152334153, 0.90125, 0.8964677222898904, 0.9146039603960396, 0.9174528301886793, 0.9038701622971286, 0.8948655256723717, 0.9089805825242718, 0.9080745341614906, 0.9149198520345253, 0.9124700239808153, 0.8990147783251231, 0.9096612296110415, 0.8991172761664565, 0.9021065675340768, 0.9032648125755743, 0.9, 0.9030226700251889, 0.9201430274135876, 0.9118727050183598, 0.9172749391727494, 0.9106280193236715, 0.9232643118148599, 0.8970588235294118, 0.8957575757575758, 0.9038929440389294, 0.8996282527881041, 0.9065656565656566, 0.9044740024183797, 0.9002403846153846, 0.8842874543239951, 0.9160493827160494, 0.926860025220681, 0.9215922798552473, 0.8913308913308914, 0.9303721488595438, 0.898428053204353, 0.9154589371980676, 0.9072681704260651, 0.926829268292683, 0.8805031446540881, 0.9160493827160494, 0.9245742092457421, 0.9129901960784313, 0.9069478908188585, 0.9027093596059114, 0.911504424778761, 0.8857493857493858, 0.9009661835748792, 0.9200968523002422, 0.919496855345912, 0.9199057714958775, 0.905521472392638, 0.9068923821039904, 0.9112426035502958, 0.9280575539568345, 0.9030674846625767, 0.9002433090024331, 0.9053398058252428, 0.9114832535885168, 0.901840490797546, 0.9234449760765551, 0.9144578313253012, 0.9106487148102815, 0.9111111111111111, 0.9153567110036276, 0.9029585798816568, 0.9131985731272295, 0.8981818181818182, 0.9095477386934674, 0.9234567901234568, 0.9321212121212121, 0.8965936739659367, 0.9117276166456494, 0.8958333333333334, 0.9034564958283671, 0.9058971141781681, 0.920099875156055, 0.9100850546780073, 0.9128787878787878, 0.9157509157509157, 0.9089775561097256, 0.9076549210206561, 0.8939213349225268, 0.9071518193224593, 0.8919567827130852, 0.9113475177304965, 0.9071170084439083, 0.9161372299872935, 0.928395061728395, 0.916058394160584, 0.9111111111111111, 0.9174876847290641, 0.9273399014778325, 0.9093137254901961, 0.9146039603960396, 0.8941469489414695, 0.9066147859922179, 0.8955773955773956, 0.9, 0.9155609167671894, 0.8934324659231723, 0.896969696969697, 0.9119804400977995, 0.9115890083632019, 0.9088639200998752, 0.9008464328899637, 0.9153175591531756, 0.9034157832744405, 0.9087546239210851, 0.9085439229843562, 0.9073170731707317, 0.9113300492610837, 0.9175, 0.9117997616209773, 0.8997555012224939, 0.9050480769230769, 0.9172749391727494, 0.9007263922518159, 0.9089805825242718, 0.9338235294117647, 0.8929889298892989, 0.8926342072409488, 0.899876390605686, 0.8969325153374234, 0.9070321811680572, 0.9004796163069544, 0.905055487053021, 0.9138349514563107, 0.9141475211608222, 0.9106280193236715, 0.9136690647482014, 0.9218559218559218, 0.9126691266912669, 0.9051620648259304, 0.9271844660194175, 0.9137115839243499, 0.9274292742927429, 0.9016189290161893, 0.8992718446601942, 0.9171741778319124, 0.9148681055155875, 0.9159456118665018, 0.9150246305418719, 0.9157769869513642, 0.9071246819338422, 0.896969696969697, 0.9044740024183797, 0.9228944246737841, 0.8933823529411765, 0.9082125603864735, 0.9078014184397163, 0.9214026602176542, 0.9079903147699758, 0.9248210023866349, 0.915, 0.890625, 0.9006134969325154, 0.9067796610169492, 0.9057071960297767, 0.9118727050183598, 0.9074299634591961, 0.8948655256723717, 0.895808383233533, 0.9016587677725119, 0.9081761006289308, 0.8894289185905225, 0.9129886506935687, 0.9027777777777778, 0.9147095179233622, 0.8939024390243903, 0.9253012048192771, 0.9107981220657277, 0.8911392405063291, 0.9020807833537332, 0.9081272084805654, 0.9011406844106464, 0.9076549210206561, 0.900523560209424, 0.9087546239210851, 0.9051008303677343, 0.897908979089791, 0.9026442307692307, 0.9211886304909561, 0.9053892215568863, 0.9054545454545454, 0.9023199023199023, 0.9048192771084337, 0.9112709832134293, 0.9065533980582524, 0.9197080291970803, 0.9004854368932039, 0.9021479713603818, 0.9294554455445545, 0.9298892988929889, 0.9244604316546763, 0.9110012360939431, 0.9069767441860465, 0.8919902912621359, 0.917550058892815, 0.8852657004830918, 0.9093137254901961, 0.8971291866028708, 0.9153936545240893, 0.907292954264524, 0.9013563501849569, 0.9034981905910736, 0.9190897597977244, 0.9140811455847255, 0.9020807833537332, 0.9034229828850856, 0.9168704156479217, 0.902264600715137, 0.9227941176470589, 0.911377245508982, 0.9011124845488258, 0.9050480769230769, 0.9161603888213852, 0.8942992874109263, 0.9060975609756098, 0.9125151883353585, 0.906801007556675, 0.9075425790754258, 0.9082007343941249, 0.9066666666666666, 0.9140164899882215, 0.9068100358422939, 0.9022842639593909, 0.909310761789601, 0.9054373522458629, 0.9052890528905289, 0.9253164556962026, 0.8984088127294981, 0.905, 0.9207100591715977, 0.8963795255930087, 0.9153567110036276, 0.9129353233830846, 0.9134734239802225, 0.9006134969325154, 0.9, 0.9169764560099133, 0.9177377892030848, 0.9249706916764361, 0.9064102564102564, 0.9202453987730062, 0.9087635054021609, 0.9046454767726161, 0.8990147783251231, 0.8831658291457286, 0.903822441430333, 0.894927536231884, 0.9101941747572816, 0.9095354523227384, 0.9146634615384616, 0.9023485784919654, 0.9136868064118372, 0.9074778200253485, 0.9096313912009513, 0.9053398058252428, 0.9066666666666666, 0.9050480769230769, 0.9209905660377359, 0.9074299634591961, 0.9143192488262911, 0.919496855345912, 0.9141475211608222, 0.9115479115479116, 0.9326556543837357, 0.8997524752475248, 0.8890290037831021, 0.9114713216957606, 0.9079601990049752, 0.9188861985472155, 0.8923444976076556, 0.8970944309927361, 0.9197080291970803, 0.9112426035502958, 0.9200477326968973, 0.9176755447941889, 0.918854415274463, 0.9185905224787363, 0.9129411764705883, 0.9071170084439083, 0.9063625450180072, 0.9221411192214112, 0.9014423076923077, 0.8886168910648715, 0.9094247246022031, 0.9076549210206561, 0.9109756097560976, 0.9110576923076923, 0.8923267326732673, 0.89119804400978, 0.9089790897908979, 0.9007263922518159, 0.9071170084439083, 0.9126691266912669, 0.89618138424821, 0.9023199023199023, 0.9039408866995073, 0.9299754299754299, 0.9127272727272727, 0.9113300492610837, 0.9044025157232705, 0.9293544457978076, 0.9181141439205955, 0.9176029962546817, 0.9100591715976332, 0.9139922978177151, 0.9105793450881612, 0.9006060606060606, 0.904881101376721, 0.9127272727272727, 0.9160493827160494, 0.9147659063625451, 0.9089775561097256, 0.9033047735618115, 0.9141475211608222, 0.9159561510353228, 0.9061728395061729, 0.9148418491484185, 0.9063231850117096, 0.8903775883069428, 0.9197994987468672, 0.9017412935323383, 0.8872180451127819, 0.8917274939172749, 0.8926342072409488, 0.8847539015606243, 0.9134734239802225, 0.9116564417177914, 0.9100985221674877, 0.9127764127764127, 0.903822441430333, 0.9166666666666666, 0.8944900351699883, 0.9138576779026217, 0.9004854368932039, 0.9025893958076449, 0.9117997616209773, 0.910394265232975, 0.8982035928143712, 0.9052132701421801, 0.9111111111111111, 0.913730255164034, 0.9187192118226601, 0.918546365914787, 0.9059107358262968, 0.9233668341708543, 0.9002433090024331, 0.90875, 0.9010025062656641, 0.8993788819875776, 0.8882282996432818, 0.913730255164034, 0.9073170731707317, 0.9236276849642004, 0.9053254437869822, 0.9090909090909091, 0.8829915560916767, 0.9035409035409036, 0.9180722891566265, 0.9138134592680047, 0.9117291414752116, 0.9090909090909091, 0.8954489544895449, 0.9089820359281438, 0.9047619047619048, 0.899746192893401, 0.9053398058252428, 0.9218559218559218, 0.9168674698795181, 0.8960880195599022, 0.9105392156862745, 0.885143570536829, 0.9121287128712872, 0.9035846724351051, 0.9169811320754717, 0.9292682926829269, 0.9074519230769231, 0.9210206561360875, 0.9027611044417767, 0.8913857677902621, 0.9176755447941889, 0.9098660170523751, 0.9130434782608695, 0.9065656565656566, 0.9015795868772782, 0.9236453201970444, 0.9108669108669109, 0.9214463840399002, 0.9111389236545682, 0.8978829389788294, 0.9191270860077022, 0.9133663366336634, 0.9226932668329177, 0.908433734939759, 0.9170731707317074, 0.9161676646706587, 0.9119804400977995, 0.9144654088050315, 0.923739237392374, 0.9075425790754258, 0.9142512077294686, 0.9166666666666666, 0.9057527539779682, 0.9045287637698899, 0.9053398058252428, 0.9048192771084337, 0.9115815691158157, 0.9007352941176471, 0.8883495145631068, 0.8979833926453143, 0.9142512077294686, 0.9129916567342073, 0.9105011933174224, 0.9071518193224593, 0.920253164556962, 0.9003516998827668, 0.8924205378973105, 0.899641577060932, 0.89125, 0.9063260340632603, 0.907673860911271, 0.9024691358024691, 0.8950617283950617, 0.9163679808841099, 0.8987804878048781, 0.9208972845336482, 0.9096385542168675, 0.8987804878048781, 0.9290953545232273, 0.9085365853658537, 0.9150246305418719, 0.9042424242424243, 0.8875453446191052, 0.9115151515151515, 0.918546365914787, 0.9172749391727494, 0.9051008303677343, 0.90125, 0.9120879120879121, 0.9028642590286425, 0.8964677222898904, 0.9165644171779141, 0.9074074074074074, 0.9268585131894485, 0.8983253588516746, 0.8931116389548693, 0.9054054054054054, 0.9224242424242424, 0.9104477611940298, 0.9067164179104478, 0.9267990074441688, 0.9183168316831684, 0.9069767441860465, 0.9159456118665018, 0.9042944785276074, 0.9214463840399002, 0.8996282527881041, 0.899749373433584, 0.8974970202622169, 0.9066985645933014, 0.9003601440576231, 0.9090909090909091, 0.9142156862745098, 0.9099756690997567, 0.8911483253588517, 0.8957055214723927, 0.9158536585365854, 0.9045848822800495, 0.9020807833537332, 0.9063670411985019, 0.9170792079207921, 0.902676399026764, 0.8879618593563766, 0.9080882352941176, 0.8982630272952854, 0.8911138923654568, 0.915129151291513, 0.9158536585365854, 0.9051620648259304, 0.9022277227722773, 0.894478527607362, 0.9121706398996235, 0.9049338146811071, 0.9071782178217822, 0.912621359223301, 0.9181929181929182, 0.8884848484848484, 0.910950661853189, 0.9037758830694276, 0.8818297331639136, 0.9014251781472684, 0.9081015719467956, 0.9098873591989988, 0.9003601440576231, 0.9048192771084337, 0.9096534653465347, 0.89125, 0.8988095238095238, 0.8993939393939394, 0.9139240506329114, 0.8952380952380953, 0.9079754601226994, 0.89875, 0.9063981042654028, 0.9308176100628931, 0.9077117572692794, 0.9030303030303031, 0.9139150943396226, 0.9118727050183598, 0.9078282828282829, 0.8985330073349633, 0.9018181818181819, 0.8949367088607595, 0.9215922798552473, 0.8864197530864197, 0.9155446756425949, 0.9200492004920049, 0.9039408866995073, 0.8978829389788294, 0.927710843373494, 0.902200488997555, 0.9031862745098039, 0.905224787363305, 0.9033816425120773, 0.8865598027127004, 0.914249684741488, 0.9089790897908979, 0.9153567110036276, 0.9070904645476773, 0.914792899408284, 0.9034653465346535, 0.9186893203883495, 0.9235511713933415, 0.9110070257611241, 0.9087403598971723, 0.9055900621118013, 0.903822441430333, 0.9151732377538829, 0.9180124223602485, 0.9045848822800495, 0.9298669891172914, 0.8871359223300971, 0.8933823529411765, 0.9258793969849246, 0.9007263922518159, 0.9083023543990086, 0.8996282527881041, 0.8956310679611651, 0.905952380952381, 0.9004914004914005, 0.9021739130434783, 0.9002433090024331, 0.8918590522478737, 0.9102244389027432, 0.9164648910411622, 0.8971141781681304, 0.9085365853658537, 0.8953922789539228, 0.9180327868852459, 0.9100850546780073, 0.9041769041769042, 0.9233532934131736, 0.9107806691449815, 0.8974668275030157, 0.9002433090024331, 0.9102870813397129, 0.889294403892944, 0.8928571428571429, 0.91375, 0.9142512077294686, 0.9122807017543859, 0.9055690072639225, 0.8879093198992444, 0.9157509157509157, 0.9256505576208178, 0.8913043478260869, 0.9311639549436797, 0.8992805755395683, 0.8950617283950617, 0.9152542372881356, 0.9100850546780073, 0.9037758830694276, 0.9096534653465347, 0.8864795918367347, 0.9109026963657679, 0.9084249084249084, 0.8974668275030157, 0.9090909090909091, 0.9158536585365854, 0.8903940886699507, 0.9034564958283671, 0.8853658536585366, 0.9018912529550828, 0.9154228855721394, 0.9040462427745665, 0.8987341772151899, 0.9168734491315137, 0.9154228855721394, 0.9187192118226601, 0.9030732860520094, 0.9059107358262968, 0.9247448979591837, 0.9262394195888755, 0.9157232704402516, 0.8951507208387942, 0.9036144578313253, 0.9108669108669109, 0.9031862745098039, 0.9108910891089109, 0.8923821039903265, 0.8942891859052248, 0.913730255164034, 0.888755980861244, 0.9155446756425949, 0.9169675090252708, 0.9127272727272727, 0.918648310387985, 0.9033816425120773, 0.9176755447941889, 0.9044205495818399, 0.9175757575757576, 0.9139393939393939, 0.9263803680981595, 0.9186893203883495, 0.9019607843137255, 0.909310761789601, 0.9190184049079755, 0.8890243902439025, 0.9036585365853659, 0.9069767441860465, 0.9231738035264484, 0.9106487148102815, 0.9072039072039072, 0.91875, 0.9081885856079405, 0.9117647058823529, 0.9153374233128835, 0.9201430274135876, 0.8917589175891759, 0.9200968523002422, 0.9124700239808153, 0.9098660170523751, 0.9147381242387332, 0.9086229086229086, 0.8966789667896679, 0.9021065675340768, 0.9196102314250914, 0.9057714958775029, 0.9118387909319899, 0.9126328217237308, 0.9028642590286425, 0.9166666666666666, 0.9154228855721394, 0.8937728937728938, 0.9046454767726161, 0.9181929181929182, 0.9097560975609756, 0.9018867924528302, 0.910950661853189, 0.9147095179233622, 0.9262394195888755, 0.9025893958076449, 0.9243379571248423, 0.9049338146811071, 0.925, 0.9144578313253012, 0.9094247246022031, 0.9067484662576687, 0.9111922141119222, 0.9202988792029888, 0.8953626634958383, 0.9133663366336634, 0.916256157635468, 0.9079754601226994, 0.9051724137931034, 0.8948626045400239, 0.8951911220715166, 0.9032258064516129, 0.9023779724655819, 0.9210206561360875, 0.9157232704402516, 0.9055900621118013, 0.9106487148102815, 0.8828502415458938, 0.9120879120879121, 0.8964241676942046, 0.8955042527339003, 0.8876678876678876, 0.9049338146811071, 0.9067796610169492, 0.9126092384519351, 0.9176904176904177, 0.9073405535499398, 0.8940886699507389, 0.9039900249376559, 0.9116222760290557, 0.9078624078624079, 0.9143546441495778, 0.8914141414141414, 0.9175757575757576, 0.9092039800995025, 0.9097560975609756, 0.9213075060532687, 0.927536231884058, 0.912621359223301, 0.9179734620024126, 0.9119420989143546, 0.8937728937728938, 0.9226044226044227, 0.914179104477612, 0.9161451814768461, 0.9164588528678305, 0.9030674846625767, 0.9120198265179678, 0.9148936170212766, 0.9211822660098522, 0.9029940119760479, 0.9031862745098039, 0.9105011933174224, 0.9136868064118372, 0.9126691266912669, 0.9072039072039072, 0.9146634615384616, 0.8950617283950617, 0.9180124223602485, 0.8962962962962963, 0.890625, 0.9142857142857143, 0.9031476997578692, 0.905982905982906, 0.9030303030303031, 0.912826899128269, 0.8949320148331273, 0.8906633906633906, 0.908641975308642, 0.9124700239808153, 0.9028290282902829, 0.9001203369434416, 0.9089805825242718, 0.9093137254901961, 0.9141104294478528, 0.8897243107769424, 0.8997613365155132, 0.9309090909090909, 0.91320293398533, 0.9198520345252774, 0.9210206561360875, 0.9106699751861043, 0.9134146341463415, 0.9038929440389294, 0.9188191881918819, 0.9008363201911589, 0.9058679706601467, 0.8971848225214198, 0.9043583535108959, 0.922509225092251, 0.8902900378310215, 0.8926829268292683, 0.9053398058252428, 0.9035409035409036, 0.9135338345864662, 0.9234507897934386, 0.909541511771995, 0.9137931034482759, 0.8928571428571429, 0.9236363636363636, 0.9064748201438849, 0.9096313912009513, 0.8960096735187424, 0.9246913580246914, 0.9033047735618115, 0.900990099009901, 0.8910411622276029, 0.8977407847800237, 0.925748502994012, 0.9065868263473054, 0.8987951807228916, 0.9191176470588235, 0.9, 0.9050480769230769, 0.89728453364817, 0.8995098039215687, 0.9171817058096415, 0.9175757575757576, 0.9164648910411622, 0.9155446756425949, 0.9107806691449815, 0.9027431421446384, 0.9046483909415971, 0.9178921568627451, 0.9137931034482759, 0.9084249084249084, 0.9119804400977995, 0.9184914841849149, 0.8970588235294118, 0.9098660170523751, 0.9042944785276074, 0.8927294398092968, 0.9034653465346535, 0.9056603773584906, 0.9141104294478528, 0.9205219454329775, 0.9088729016786571, 0.9161528976572133, 0.9156626506024096, 0.8986568986568987, 0.9166666666666666, 0.9173859432799013, 0.9306071871127634, 0.8905109489051095, 0.9201520912547528, 0.9187817258883249, 0.921146953405018, 0.9065868263473054, 0.9047619047619048, 0.9166666666666666, 0.9151515151515152, 0.9009661835748792, 0.9108669108669109, 0.9007352941176471, 0.9321212121212121, 0.9079903147699758, 0.8898305084745762, 0.9201474201474201, 0.9188846641318125, 0.8939580764488286, 0.9140722291407223, 0.9105392156862745, 0.9136690647482014, 0.8759305210918115, 0.9053398058252428, 0.912883435582822, 0.8948004836759371, 0.9219512195121952, 0.9039900249376559, 0.9085510688836105, 0.9036585365853659, 0.8991596638655462, 0.9024096385542169, 0.8959608323133414, 0.893719806763285, 0.8948004836759371, 0.9223300970873787, 0.9124700239808153, 0.919496855345912, 0.9147869674185464, 0.9008464328899637, 0.9114391143911439, 0.9190184049079755, 0.9178743961352657, 0.9227985524728589, 0.9029940119760479, 0.9122807017543859, 0.92336217552534]\n",
            "sp: [0.9123020706455542, 0.8907563025210085, 0.9176904176904177, 0.8915375446960667, 0.9185905224787363, 0.928921568627451, 0.907563025210084, 0.8989037758830695, 0.9124087591240876, 0.9203860072376358, 0.9079601990049752, 0.9257425742574258, 0.9286592865928659, 0.9055489964580874, 0.9148936170212766, 0.9216867469879518, 0.9051724137931034, 0.91625, 0.9098660170523751, 0.9036144578313253, 0.8928571428571429, 0.9145299145299145, 0.9228855721393034, 0.9109263657957245, 0.8969325153374234, 0.9056122448979592, 0.8952020202020202, 0.9206145966709347, 0.9077669902912622, 0.8996235884567126, 0.9085365853658537, 0.8924205378973105, 0.9044740024183797, 0.8989169675090253, 0.9097651421508035, 0.90920245398773, 0.9208899876390606, 0.9285714285714286, 0.9266503667481663, 0.9118727050183598, 0.9124537607891492, 0.9090909090909091, 0.9172661870503597, 0.8985680190930787, 0.9161528976572133, 0.9115479115479116, 0.8852258852258852, 0.9083129584352079, 0.9101941747572816, 0.8914354644149578, 0.9132530120481928, 0.9143206854345165, 0.9339152119700748, 0.8929016189290162, 0.9039615846338536, 0.890840652446675, 0.9120095124851367, 0.9027283511269276, 0.9089805825242718, 0.9048780487804878, 0.9205776173285198, 0.9131985731272295, 0.9042944785276074, 0.8994011976047904, 0.9135654261704682, 0.9331713244228432, 0.899880810488677, 0.9184177997527813, 0.9021739130434783, 0.9062119366626066, 0.9227941176470589, 0.9040097205346294, 0.9010025062656641, 0.9073405535499398, 0.9174876847290641, 0.9164677804295943, 0.8974668275030157, 0.9133016627078385, 0.909541511771995, 0.8807453416149068, 0.9157372986369269, 0.90375, 0.926829268292683, 0.9020310633213859, 0.9085365853658537, 0.9236363636363636, 0.9069767441860465, 0.902948402948403, 0.9125615763546798, 0.9084249084249084, 0.8950617283950617, 0.8990147783251231, 0.9006289308176101, 0.9176904176904177, 0.9077490774907749, 0.91875, 0.9090909090909091, 0.9203860072376358, 0.8991793669402111, 0.9058402860548271, 0.9077844311377246, 0.9301745635910225, 0.902557856272838, 0.9129383313180169, 0.9221105527638191, 0.9057527539779682, 0.915576694411415, 0.901840490797546, 0.9136253041362531, 0.9072532699167658, 0.9082007343941249, 0.9073405535499398, 0.8973429951690821, 0.8880688806888068, 0.9101654846335697, 0.9121706398996235, 0.9037758830694276, 0.9038929440389294, 0.9229813664596274, 0.9037758830694276, 0.8987654320987655, 0.902676399026764, 0.9003601440576231, 0.9078455790784558, 0.9196891191709845, 0.8946716232961586, 0.9085290482076638, 0.8917274939172749, 0.8822815533980582, 0.9004914004914005, 0.9195849546044098, 0.9089820359281438, 0.9100850546780073, 0.9102091020910209, 0.8992628992628993, 0.9152542372881356, 0.9192399049881235, 0.9074519230769231, 0.9134146341463415, 0.8988621997471555, 0.9248554913294798, 0.9127272727272727, 0.9152334152334153, 0.90125, 0.8964677222898904, 0.9146039603960396, 0.9174528301886793, 0.9038701622971286, 0.8948655256723717, 0.9089805825242718, 0.9080745341614906, 0.9149198520345253, 0.9124700239808153, 0.8990147783251231, 0.9096612296110415, 0.8991172761664565, 0.9021065675340768, 0.9032648125755743, 0.9, 0.9030226700251889, 0.9201430274135876, 0.9118727050183598, 0.9172749391727494, 0.9106280193236715, 0.9232643118148599, 0.8970588235294118, 0.8957575757575758, 0.9038929440389294, 0.8996282527881041, 0.9065656565656566, 0.9044740024183797, 0.9002403846153846, 0.8842874543239951, 0.9160493827160494, 0.926860025220681, 0.9215922798552473, 0.8913308913308914, 0.9303721488595438, 0.898428053204353, 0.9154589371980676, 0.9072681704260651, 0.926829268292683, 0.8805031446540881, 0.9160493827160494, 0.9245742092457421, 0.9129901960784313, 0.9069478908188585, 0.9027093596059114, 0.911504424778761, 0.8857493857493858, 0.9009661835748792, 0.9200968523002422, 0.919496855345912, 0.9199057714958775, 0.905521472392638, 0.9068923821039904, 0.9112426035502958, 0.9280575539568345, 0.9030674846625767, 0.9002433090024331, 0.9053398058252428, 0.9114832535885168, 0.901840490797546, 0.9234449760765551, 0.9144578313253012, 0.9106487148102815, 0.9111111111111111, 0.9153567110036276, 0.9029585798816568, 0.9131985731272295, 0.8981818181818182, 0.9095477386934674, 0.9234567901234568, 0.9321212121212121, 0.8965936739659367, 0.9117276166456494, 0.8958333333333334, 0.9034564958283671, 0.9058971141781681, 0.920099875156055, 0.9100850546780073, 0.9128787878787878, 0.9157509157509157, 0.9089775561097256, 0.9076549210206561, 0.8939213349225268, 0.9071518193224593, 0.8919567827130852, 0.9113475177304965, 0.9071170084439083, 0.9161372299872935, 0.928395061728395, 0.916058394160584, 0.9111111111111111, 0.9174876847290641, 0.9273399014778325, 0.9093137254901961, 0.9146039603960396, 0.8941469489414695, 0.9066147859922179, 0.8955773955773956, 0.9, 0.9155609167671894, 0.8934324659231723, 0.896969696969697, 0.9119804400977995, 0.9115890083632019, 0.9088639200998752, 0.9008464328899637, 0.9153175591531756, 0.9034157832744405, 0.9087546239210851, 0.9085439229843562, 0.9073170731707317, 0.9113300492610837, 0.9175, 0.9117997616209773, 0.8997555012224939, 0.9050480769230769, 0.9172749391727494, 0.9007263922518159, 0.9089805825242718, 0.9338235294117647, 0.8929889298892989, 0.8926342072409488, 0.899876390605686, 0.8969325153374234, 0.9070321811680572, 0.9004796163069544, 0.905055487053021, 0.9138349514563107, 0.9141475211608222, 0.9106280193236715, 0.9136690647482014, 0.9218559218559218, 0.9126691266912669, 0.9051620648259304, 0.9271844660194175, 0.9137115839243499, 0.9274292742927429, 0.9016189290161893, 0.8992718446601942, 0.9171741778319124, 0.9148681055155875, 0.9159456118665018, 0.9150246305418719, 0.9157769869513642, 0.9071246819338422, 0.896969696969697, 0.9044740024183797, 0.9228944246737841, 0.8933823529411765, 0.9082125603864735, 0.9078014184397163, 0.9214026602176542, 0.9079903147699758, 0.9248210023866349, 0.915, 0.890625, 0.9006134969325154, 0.9067796610169492, 0.9057071960297767, 0.9118727050183598, 0.9074299634591961, 0.8948655256723717, 0.895808383233533, 0.9016587677725119, 0.9081761006289308, 0.8894289185905225, 0.9129886506935687, 0.9027777777777778, 0.9147095179233622, 0.8939024390243903, 0.9253012048192771, 0.9107981220657277, 0.8911392405063291, 0.9020807833537332, 0.9081272084805654, 0.9011406844106464, 0.9076549210206561, 0.900523560209424, 0.9087546239210851, 0.9051008303677343, 0.897908979089791, 0.9026442307692307, 0.9211886304909561, 0.9053892215568863, 0.9054545454545454, 0.9023199023199023, 0.9048192771084337, 0.9112709832134293, 0.9065533980582524, 0.9197080291970803, 0.9004854368932039, 0.9021479713603818, 0.9294554455445545, 0.9298892988929889, 0.9244604316546763, 0.9110012360939431, 0.9069767441860465, 0.8919902912621359, 0.917550058892815, 0.8852657004830918, 0.9093137254901961, 0.8971291866028708, 0.9153936545240893, 0.907292954264524, 0.9013563501849569, 0.9034981905910736, 0.9190897597977244, 0.9140811455847255, 0.9020807833537332, 0.9034229828850856, 0.9168704156479217, 0.902264600715137, 0.9227941176470589, 0.911377245508982, 0.9011124845488258, 0.9050480769230769, 0.9161603888213852, 0.8942992874109263, 0.9060975609756098, 0.9125151883353585, 0.906801007556675, 0.9075425790754258, 0.9082007343941249, 0.9066666666666666, 0.9140164899882215, 0.9068100358422939, 0.9022842639593909, 0.909310761789601, 0.9054373522458629, 0.9052890528905289, 0.9253164556962026, 0.8984088127294981, 0.905, 0.9207100591715977, 0.8963795255930087, 0.9153567110036276, 0.9129353233830846, 0.9134734239802225, 0.9006134969325154, 0.9, 0.9169764560099133, 0.9177377892030848, 0.9249706916764361, 0.9064102564102564, 0.9202453987730062, 0.9087635054021609, 0.9046454767726161, 0.8990147783251231, 0.8831658291457286, 0.903822441430333, 0.894927536231884, 0.9101941747572816, 0.9095354523227384, 0.9146634615384616, 0.9023485784919654, 0.9136868064118372, 0.9074778200253485, 0.9096313912009513, 0.9053398058252428, 0.9066666666666666, 0.9050480769230769, 0.9209905660377359, 0.9074299634591961, 0.9143192488262911, 0.919496855345912, 0.9141475211608222, 0.9115479115479116, 0.9326556543837357, 0.8997524752475248, 0.8890290037831021, 0.9114713216957606, 0.9079601990049752, 0.9188861985472155, 0.8923444976076556, 0.8970944309927361, 0.9197080291970803, 0.9112426035502958, 0.9200477326968973, 0.9176755447941889, 0.918854415274463, 0.9185905224787363, 0.9129411764705883, 0.9071170084439083, 0.9063625450180072, 0.9221411192214112, 0.9014423076923077, 0.8886168910648715, 0.9094247246022031, 0.9076549210206561, 0.9109756097560976, 0.9110576923076923, 0.8923267326732673, 0.89119804400978, 0.9089790897908979, 0.9007263922518159, 0.9071170084439083, 0.9126691266912669, 0.89618138424821, 0.9023199023199023, 0.9039408866995073, 0.9299754299754299, 0.9127272727272727, 0.9113300492610837, 0.9044025157232705, 0.9293544457978076, 0.9181141439205955, 0.9176029962546817, 0.9100591715976332, 0.9139922978177151, 0.9105793450881612, 0.9006060606060606, 0.904881101376721, 0.9127272727272727, 0.9160493827160494, 0.9147659063625451, 0.9089775561097256, 0.9033047735618115, 0.9141475211608222, 0.9159561510353228, 0.9061728395061729, 0.9148418491484185, 0.9063231850117096, 0.8903775883069428, 0.9197994987468672, 0.9017412935323383, 0.8872180451127819, 0.8917274939172749, 0.8926342072409488, 0.8847539015606243, 0.9134734239802225, 0.9116564417177914, 0.9100985221674877, 0.9127764127764127, 0.903822441430333, 0.9166666666666666, 0.8944900351699883, 0.9138576779026217, 0.9004854368932039, 0.9025893958076449, 0.9117997616209773, 0.910394265232975, 0.8982035928143712, 0.9052132701421801, 0.9111111111111111, 0.913730255164034, 0.9187192118226601, 0.918546365914787, 0.9059107358262968, 0.9233668341708543, 0.9002433090024331, 0.90875, 0.9010025062656641, 0.8993788819875776, 0.8882282996432818, 0.913730255164034, 0.9073170731707317, 0.9236276849642004, 0.9053254437869822, 0.9090909090909091, 0.8829915560916767, 0.9035409035409036, 0.9180722891566265, 0.9138134592680047, 0.9117291414752116, 0.9090909090909091, 0.8954489544895449, 0.9089820359281438, 0.9047619047619048, 0.899746192893401, 0.9053398058252428, 0.9218559218559218, 0.9168674698795181, 0.8960880195599022, 0.9105392156862745, 0.885143570536829, 0.9121287128712872, 0.9035846724351051, 0.9169811320754717, 0.9292682926829269, 0.9074519230769231, 0.9210206561360875, 0.9027611044417767, 0.8913857677902621, 0.9176755447941889, 0.9098660170523751, 0.9130434782608695, 0.9065656565656566, 0.9015795868772782, 0.9236453201970444, 0.9108669108669109, 0.9214463840399002, 0.9111389236545682, 0.8978829389788294, 0.9191270860077022, 0.9133663366336634, 0.9226932668329177, 0.908433734939759, 0.9170731707317074, 0.9161676646706587, 0.9119804400977995, 0.9144654088050315, 0.923739237392374, 0.9075425790754258, 0.9142512077294686, 0.9166666666666666, 0.9057527539779682, 0.9045287637698899, 0.9053398058252428, 0.9048192771084337, 0.9115815691158157, 0.9007352941176471, 0.8883495145631068, 0.8979833926453143, 0.9142512077294686, 0.9129916567342073, 0.9105011933174224, 0.9071518193224593, 0.920253164556962, 0.9003516998827668, 0.8924205378973105, 0.899641577060932, 0.89125, 0.9063260340632603, 0.907673860911271, 0.9024691358024691, 0.8950617283950617, 0.9163679808841099, 0.8987804878048781, 0.9208972845336482, 0.9096385542168675, 0.8987804878048781, 0.9290953545232273, 0.9085365853658537, 0.9150246305418719, 0.9042424242424243, 0.8875453446191052, 0.9115151515151515, 0.918546365914787, 0.9172749391727494, 0.9051008303677343, 0.90125, 0.9120879120879121, 0.9028642590286425, 0.8964677222898904, 0.9165644171779141, 0.9074074074074074, 0.9268585131894485, 0.8983253588516746, 0.8931116389548693, 0.9054054054054054, 0.9224242424242424, 0.9104477611940298, 0.9067164179104478, 0.9267990074441688, 0.9183168316831684, 0.9069767441860465, 0.9159456118665018, 0.9042944785276074, 0.9214463840399002, 0.8996282527881041, 0.899749373433584, 0.8974970202622169, 0.9066985645933014, 0.9003601440576231, 0.9090909090909091, 0.9142156862745098, 0.9099756690997567, 0.8911483253588517, 0.8957055214723927, 0.9158536585365854, 0.9045848822800495, 0.9020807833537332, 0.9063670411985019, 0.9170792079207921, 0.902676399026764, 0.8879618593563766, 0.9080882352941176, 0.8982630272952854, 0.8911138923654568, 0.915129151291513, 0.9158536585365854, 0.9051620648259304, 0.9022277227722773, 0.894478527607362, 0.9121706398996235, 0.9049338146811071, 0.9071782178217822, 0.912621359223301, 0.9181929181929182, 0.8884848484848484, 0.910950661853189, 0.9037758830694276, 0.8818297331639136, 0.9014251781472684, 0.9081015719467956, 0.9098873591989988, 0.9003601440576231, 0.9048192771084337, 0.9096534653465347, 0.89125, 0.8988095238095238, 0.8993939393939394, 0.9139240506329114, 0.8952380952380953, 0.9079754601226994, 0.89875, 0.9063981042654028, 0.9308176100628931, 0.9077117572692794, 0.9030303030303031, 0.9139150943396226, 0.9118727050183598, 0.9078282828282829, 0.8985330073349633, 0.9018181818181819, 0.8949367088607595, 0.9215922798552473, 0.8864197530864197, 0.9155446756425949, 0.9200492004920049, 0.9039408866995073, 0.8978829389788294, 0.927710843373494, 0.902200488997555, 0.9031862745098039, 0.905224787363305, 0.9033816425120773, 0.8865598027127004, 0.914249684741488, 0.9089790897908979, 0.9153567110036276, 0.9070904645476773, 0.914792899408284, 0.9034653465346535, 0.9186893203883495, 0.9235511713933415, 0.9110070257611241, 0.9087403598971723, 0.9055900621118013, 0.903822441430333, 0.9151732377538829, 0.9180124223602485, 0.9045848822800495, 0.9298669891172914, 0.8871359223300971, 0.8933823529411765, 0.9258793969849246, 0.9007263922518159, 0.9083023543990086, 0.8996282527881041, 0.8956310679611651, 0.905952380952381, 0.9004914004914005, 0.9021739130434783, 0.9002433090024331, 0.8918590522478737, 0.9102244389027432, 0.9164648910411622, 0.8971141781681304, 0.9085365853658537, 0.8953922789539228, 0.9180327868852459, 0.9100850546780073, 0.9041769041769042, 0.9233532934131736, 0.9107806691449815, 0.8974668275030157, 0.9002433090024331, 0.9102870813397129, 0.889294403892944, 0.8928571428571429, 0.91375, 0.9142512077294686, 0.9122807017543859, 0.9055690072639225, 0.8879093198992444, 0.9157509157509157, 0.9256505576208178, 0.8913043478260869, 0.9311639549436797, 0.8992805755395683, 0.8950617283950617, 0.9152542372881356, 0.9100850546780073, 0.9037758830694276, 0.9096534653465347, 0.8864795918367347, 0.9109026963657679, 0.9084249084249084, 0.8974668275030157, 0.9090909090909091, 0.9158536585365854, 0.8903940886699507, 0.9034564958283671, 0.8853658536585366, 0.9018912529550828, 0.9154228855721394, 0.9040462427745665, 0.8987341772151899, 0.9168734491315137, 0.9154228855721394, 0.9187192118226601, 0.9030732860520094, 0.9059107358262968, 0.9247448979591837, 0.9262394195888755, 0.9157232704402516, 0.8951507208387942, 0.9036144578313253, 0.9108669108669109, 0.9031862745098039, 0.9108910891089109, 0.8923821039903265, 0.8942891859052248, 0.913730255164034, 0.888755980861244, 0.9155446756425949, 0.9169675090252708, 0.9127272727272727, 0.918648310387985, 0.9033816425120773, 0.9176755447941889, 0.9044205495818399, 0.9175757575757576, 0.9139393939393939, 0.9263803680981595, 0.9186893203883495, 0.9019607843137255, 0.909310761789601, 0.9190184049079755, 0.8890243902439025, 0.9036585365853659, 0.9069767441860465, 0.9231738035264484, 0.9106487148102815, 0.9072039072039072, 0.91875, 0.9081885856079405, 0.9117647058823529, 0.9153374233128835, 0.9201430274135876, 0.8917589175891759, 0.9200968523002422, 0.9124700239808153, 0.9098660170523751, 0.9147381242387332, 0.9086229086229086, 0.8966789667896679, 0.9021065675340768, 0.9196102314250914, 0.9057714958775029, 0.9118387909319899, 0.9126328217237308, 0.9028642590286425, 0.9166666666666666, 0.9154228855721394, 0.8937728937728938, 0.9046454767726161, 0.9181929181929182, 0.9097560975609756, 0.9018867924528302, 0.910950661853189, 0.9147095179233622, 0.9262394195888755, 0.9025893958076449, 0.9243379571248423, 0.9049338146811071, 0.925, 0.9144578313253012, 0.9094247246022031, 0.9067484662576687, 0.9111922141119222, 0.9202988792029888, 0.8953626634958383, 0.9133663366336634, 0.916256157635468, 0.9079754601226994, 0.9051724137931034, 0.8948626045400239, 0.8951911220715166, 0.9032258064516129, 0.9023779724655819, 0.9210206561360875, 0.9157232704402516, 0.9055900621118013, 0.9106487148102815, 0.8828502415458938, 0.9120879120879121, 0.8964241676942046, 0.8955042527339003, 0.8876678876678876, 0.9049338146811071, 0.9067796610169492, 0.9126092384519351, 0.9176904176904177, 0.9073405535499398, 0.8940886699507389, 0.9039900249376559, 0.9116222760290557, 0.9078624078624079, 0.9143546441495778, 0.8914141414141414, 0.9175757575757576, 0.9092039800995025, 0.9097560975609756, 0.9213075060532687, 0.927536231884058, 0.912621359223301, 0.9179734620024126, 0.9119420989143546, 0.8937728937728938, 0.9226044226044227, 0.914179104477612, 0.9161451814768461, 0.9164588528678305, 0.9030674846625767, 0.9120198265179678, 0.9148936170212766, 0.9211822660098522, 0.9029940119760479, 0.9031862745098039, 0.9105011933174224, 0.9136868064118372, 0.9126691266912669, 0.9072039072039072, 0.9146634615384616, 0.8950617283950617, 0.9180124223602485, 0.8962962962962963, 0.890625, 0.9142857142857143, 0.9031476997578692, 0.905982905982906, 0.9030303030303031, 0.912826899128269, 0.8949320148331273, 0.8906633906633906, 0.908641975308642, 0.9124700239808153, 0.9028290282902829, 0.9001203369434416, 0.9089805825242718, 0.9093137254901961, 0.9141104294478528, 0.8897243107769424, 0.8997613365155132, 0.9309090909090909, 0.91320293398533, 0.9198520345252774, 0.9210206561360875, 0.9106699751861043, 0.9134146341463415, 0.9038929440389294, 0.9188191881918819, 0.9008363201911589, 0.9058679706601467, 0.8971848225214198, 0.9043583535108959, 0.922509225092251, 0.8902900378310215, 0.8926829268292683, 0.9053398058252428, 0.9035409035409036, 0.9135338345864662, 0.9234507897934386, 0.909541511771995, 0.9137931034482759, 0.8928571428571429, 0.9236363636363636, 0.9064748201438849, 0.9096313912009513, 0.8960096735187424, 0.9246913580246914, 0.9033047735618115, 0.900990099009901, 0.8910411622276029, 0.8977407847800237, 0.925748502994012, 0.9065868263473054, 0.8987951807228916, 0.9191176470588235, 0.9, 0.9050480769230769, 0.89728453364817, 0.8995098039215687, 0.9171817058096415, 0.9175757575757576, 0.9164648910411622, 0.9155446756425949, 0.9107806691449815, 0.9027431421446384, 0.9046483909415971, 0.9178921568627451, 0.9137931034482759, 0.9084249084249084, 0.9119804400977995, 0.9184914841849149, 0.8970588235294118, 0.9098660170523751, 0.9042944785276074, 0.8927294398092968, 0.9034653465346535, 0.9056603773584906, 0.9141104294478528, 0.9205219454329775, 0.9088729016786571, 0.9161528976572133, 0.9156626506024096, 0.8986568986568987, 0.9166666666666666, 0.9173859432799013, 0.9306071871127634, 0.8905109489051095, 0.9201520912547528, 0.9187817258883249, 0.921146953405018, 0.9065868263473054, 0.9047619047619048, 0.9166666666666666, 0.9151515151515152, 0.9009661835748792, 0.9108669108669109, 0.9007352941176471, 0.9321212121212121, 0.9079903147699758, 0.8898305084745762, 0.9201474201474201, 0.9188846641318125, 0.8939580764488286, 0.9140722291407223, 0.9105392156862745, 0.9136690647482014, 0.8759305210918115, 0.9053398058252428, 0.912883435582822, 0.8948004836759371, 0.9219512195121952, 0.9039900249376559, 0.9085510688836105, 0.9036585365853659, 0.8991596638655462, 0.9024096385542169, 0.8959608323133414, 0.893719806763285, 0.8948004836759371, 0.9223300970873787, 0.9124700239808153, 0.919496855345912, 0.9147869674185464, 0.9008464328899637, 0.9114391143911439, 0.9190184049079755, 0.9178743961352657, 0.9227985524728589, 0.9029940119760479, 0.9122807017543859, 0.92336217552534]\n",
            "sd_acc: 0.009339585968328006\n",
            "sd_f1: 0.006956202006441634\n",
            "sd_mcc: 0.022659684509569865\n",
            "sd_sn: 0.009963699216082532\n",
            "sd_sp: 0.009963699216082532\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.82      0.80       331\n",
            "           1       0.93      0.91      0.92       818\n",
            "\n",
            "    accuracy                           0.88      1149\n",
            "   macro avg       0.86      0.87      0.86      1149\n",
            "weighted avg       0.89      0.88      0.88      1149\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.009339585968328006, 0.022659684509569865, 0.006956202006441634)"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "error_rate(testing_labels, predicted_cnn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "id": "8Z3tNZwLbaGc",
        "outputId": "9da4905e-d537-47e7-9da5-0774dfb2c2fa"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-7907fe73dc39>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfpr_CNN\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtpr_CNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresholds_CNN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_cnn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mauc_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_cnn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpr_CNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr_CNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m\"CNN ({:.2f})\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauc_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'roc_curve' is not defined"
          ]
        }
      ],
      "source": [
        "fpr_CNN , tpr_CNN, thresholds_CNN = roc_curve(Y_test, predicted_cnn)\n",
        "auc_score = roc_auc_score(Y_test, predicted_cnn)\n",
        "plt.plot(fpr_CNN, tpr_CNN, label= \"CNN ({:.2f})\".format(auc_score))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "fZaD6LSW7YGr",
        "outputId": "ee5fe165-c408-4f3d-aa56-6b6795a8cc65"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-f2ce7bb2f4f1>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/cnn_train_dataset.xlsx'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/cnn_training_labels.xlsx'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_training_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ],
      "source": [
        "df =pd.DataFrame(train_dataset)\n",
        "df.to_excel('/content/cnn_train_dataset.xlsx')\n",
        "df =pd.DataFrame(training_labels)\n",
        "df.to_excel('/content/cnn_training_labels.xlsx')\n",
        "df =pd.DataFrame(predicted_training_labels)\n",
        "df.to_excel('/content/cnn_pred_train_labels.xlsx')\n",
        "\n",
        "df =pd.DataFrame(test_dataset)\n",
        "df.to_excel('/content/cnn_test_dataset.xlsx')\n",
        "df =pd.DataFrame(testing_labels)\n",
        "df.to_excel('/content/cnn_testing_labels.xlsx')\n",
        "df =pd.DataFrame(predicted_cnn)\n",
        "df.to_excel('/content/cnn_pred_test_labels.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2e7ei0hoj9dy"
      },
      "outputs": [],
      "source": [
        "from pandas import DataFrame\n",
        "cv_results = DataFrame(grid.cv_results_)\n",
        "\n",
        "#cv_results[['param_conv_activation','split0_test_score', 'split1_test_score', 'split2_test_score']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R1Ut3Y2ej9dy"
      },
      "outputs": [],
      "source": [
        "# Further metrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Data visualization\n",
        "import plotly.express as px"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EvD7KRaBj9dy"
      },
      "outputs": [],
      "source": [
        "classes = np.unique(testing_labels)\n",
        "\n",
        "confusion_matrix_data = confusion_matrix(testing_labels, predicted_cnn, labels=classes)\n",
        "conf_matrix(confusion_matrix_data)\n",
        "confusion_matrix_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "guHyXf_vj9dy"
      },
      "outputs": [],
      "source": [
        "TP = confusion_matrix_data[1,1]\n",
        "TN = confusion_matrix_data[0,0]\n",
        "FP = confusion_matrix_data[0,1]\n",
        "FN = confusion_matrix_data[1,0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FpqxYh-7j9dy"
      },
      "outputs": [],
      "source": [
        "sn = TP / float(TP + FN)\n",
        "print(sn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NPGTyFCcj9dy"
      },
      "outputs": [],
      "source": [
        "sp = TN / float(TN + FP)\n",
        "print(sp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tgM9DaT4iGC"
      },
      "source": [
        "### RNN with GridSearch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cK16axrl4iGE"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import SimpleRNN, Bidirectional, Input, Embedding, LSTM, Dropout, Dense, InputLayer, GRU\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from keras import regularizers\n",
        "from keras.layers import Embedding, Bidirectional\n",
        "from keras.regularizers import l2\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from time import time\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "buqYUfvFmjgR"
      },
      "outputs": [],
      "source": [
        "from numpy.random import seed\n",
        "seed(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sOeNrEYSmjgr"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import set_random_seed\n",
        "set_random_seed(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xE7qx0Ef4iGF"
      },
      "outputs": [],
      "source": [
        "X_train, X_test= train_dataset, test_dataset\n",
        "y_train, y_test = training_labels, testing_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "omdigfpk4iGF"
      },
      "outputs": [],
      "source": [
        "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1],1))\n",
        "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1],1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1V-40i72fiJ"
      },
      "outputs": [],
      "source": [
        "input_shape = X_train[1,:].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3L0Zne-xO1r"
      },
      "outputs": [],
      "source": [
        "Y_train = np.reshape(y_train,(len(y_train),1)).astype(int)\n",
        "Y_test = np.reshape(y_test,(len(y_test),1)).astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wUc3tuPW4iGE"
      },
      "outputs": [],
      "source": [
        "max_length = 1280"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vibk8C8t4iGG"
      },
      "outputs": [],
      "source": [
        "def create_rnn_model(dropout_rate=0.0, units=50, learning_rate_init=0.001, regularizer=0.0, solver='adam'):\n",
        "    x_input = layers.Input(input_shape)\n",
        "    #emb = Embedding(21, units, input_length=max_length)(x_input)\n",
        "    rnn = SimpleRNN(units, activity_regularizer=l2(regularizer), return_sequences = True)(x_input) #kernel_regularizer=l2(0.01), recurrent_regularizer=l2(0.01), bias_regularizer=l2(0.01)\n",
        "    x = Dropout(dropout_rate)(rnn)\n",
        "    rnn2 =SimpleRNN(units, activity_regularizer=l2(dropout_rate))(x) #kernel_regularizer=l2(0.01), recurrent_regularizer=l2(0.01), bias_regularizer=l2(0.01)\n",
        "    x2 = Dropout(dropout_rate)(rnn2)\n",
        "    rnn3 =SimpleRNN(units, activity_regularizer=l2(dropout_rate))(x) #kernel_regularizer=l2(0.01), recurrent_regularizer=l2(0.01), bias_regularizer=l2(0.01)\n",
        "    x3 = Dropout(dropout_rate)(rnn3)\n",
        "    # softmax classifier\n",
        "    x_output = Dense(1, activation='sigmoid')(x3)\n",
        "\n",
        "    model = Model(inputs=x_input, outputs=x_output)\n",
        "    # Compile model\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate_init),#solver,#\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy'],\n",
        "        )\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Fx-WbXR4iGG",
        "outputId": "c0f83cb2-f13e-4bf4-89a9-b33433adc6ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 2560, 1)]         0         \n",
            "                                                                 \n",
            " simple_rnn (SimpleRNN)      (None, 2560, 50)          2600      \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 2560, 50)          0         \n",
            "                                                                 \n",
            " simple_rnn_2 (SimpleRNN)    (None, 50)                5050      \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 50)                0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 51        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 7,701\n",
            "Trainable params: 7,701\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "rnn_model = create_rnn_model()\n",
        "rnn_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZ8aG71e4iGH",
        "outputId": "bcd53985-3e17-4266-c799-c9b24873517b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-60-f12cdc226b33>:4: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
            "  model = KerasClassifier(build_fn=create_rnn_model, verbose=1, epochs=5, batch_size=256)\n"
          ]
        }
      ],
      "source": [
        "start = time()\n",
        "\n",
        "# create model\n",
        "model = KerasClassifier(build_fn=create_rnn_model, verbose=1, epochs=5, batch_size=256)\n",
        "# define parameters and values for grid search\n",
        "\n",
        "parameters = {\n",
        "    'units':[50], # 1024\n",
        "    'learning_rate_init': [0.01],\n",
        "    #'solver':['adam'],\n",
        "    'epochs':[3,5,10,15],\n",
        "    'dropout_rate':[0.05], #0.05\n",
        "    'regularizer':[0.0],\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K0Us0Bmf4iGH"
      },
      "outputs": [],
      "source": [
        "metrics = {'accuracy':make_scorer(accuracy_score,greater_is_better=True),'f1':make_scorer(f1_score,greater_is_better=True),'mcc':make_scorer(matthews_corrcoef,greater_is_better=True)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VId_Vtu64iGI",
        "outputId": "a5a76024-2997-44b2-ea48-3430106b85bb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "18/18 [==============================] - 58s 3s/step - loss: 1.2504 - accuracy: 0.5998\n",
            "Epoch 2/5\n",
            "18/18 [==============================] - 54s 3s/step - loss: 1.2574 - accuracy: 0.6314\n",
            "Epoch 3/5\n",
            "18/18 [==============================] - 54s 3s/step - loss: 1.0723 - accuracy: 0.6971\n",
            "Epoch 4/5\n",
            "18/18 [==============================] - 53s 3s/step - loss: 0.8840 - accuracy: 0.7072\n",
            "Epoch 5/5\n",
            "18/18 [==============================] - 57s 3s/step - loss: 0.7577 - accuracy: 0.7104\n"
          ]
        }
      ],
      "source": [
        "grid = GridSearchCV(estimator=model, param_grid=parameters, n_jobs=-1, cv=10, scoring=metrics, refit='mcc')\n",
        "grid_result = grid.fit(X_train, Y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n6V4U-0E4iGI"
      },
      "outputs": [],
      "source": [
        "classifier = grid.best_estimator_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxZTjoZX4iGJ",
        "outputId": "61968bb9-f9ee-47c5-abda-917520844690"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'verbose': 1,\n",
              " 'epochs': 5,\n",
              " 'batch_size': 256,\n",
              " 'dropout_rate': 0.05,\n",
              " 'learning_rate_init': 0.01,\n",
              " 'regularizer': 0.0,\n",
              " 'units': 50,\n",
              " 'build_fn': <function __main__.create_rnn_model(dropout_rate=0.0, units=50, learning_rate_init=0.001, regularizer=0.0, solver='adam')>}"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "params = classifier.get_params()\n",
        "params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eU7pc4Fz4iGJ"
      },
      "outputs": [],
      "source": [
        "#cv_results = DataFrame(grid.cv_results_)\n",
        "\n",
        "#cv_results[['param_units','param_dropout_rate','param_regularizer','split0_test_score', 'split1_test_score', 'split2_test_score']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OGc8apIR4iGJ"
      },
      "outputs": [],
      "source": [
        "df =pd.DataFrame(grid.cv_results_)\n",
        "new_path = '/content/test.xls'\n",
        "writer = pd.ExcelWriter(new_path, engine='xlsxwriter')\n",
        "df.to_excel('/content/drive/MyDrive/Acidophilic/ESM23BRNN.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtA2NV_z4iGK",
        "outputId": "34de9f02-0c47-4ee4-f5c1-f473e938887a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "144/144 [==============================] - 39s 266ms/step\n",
            "acc: 0.7119529719137818\n",
            "f1: 0.8317436093094238\n",
            "mcc: 0.0\n",
            "sn: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "sp: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "sd_acc: 0.006851777839605985\n",
            "sd_f1: 0.004676051386765148\n",
            "sd_mcc: 0.0\n",
            "sd_sn: 0.0\n",
            "sd_sp: 0.0\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00      1323\n",
            "           1       0.71      1.00      0.83      3270\n",
            "\n",
            "    accuracy                           0.71      4593\n",
            "   macro avg       0.36      0.50      0.42      4593\n",
            "weighted avg       0.51      0.71      0.59      4593\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.006851777839605985, 0.0, 0.004676051386765148)"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predicted_training_labels = classifier.predict(X_train)\n",
        "predicted_training_labels = np.where(predicted_training_labels > 0.5, 1, 0)\n",
        "predicted_training_labels = np.reshape(predicted_training_labels,(len(predicted_training_labels),)).astype(int)\n",
        "error_rate(Y_train, predicted_training_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZNsQYdJ4iGL",
        "outputId": "331364a4-a718-4db8-cdd8-881a6431d7a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "36/36 [==============================] - 10s 275ms/step\n",
            "acc: 0.7119234116623151\n",
            "f1: 0.8317234367056432\n",
            "mcc: 0.0\n",
            "sn: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "sp: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "sd_acc: 0.013122935523096362\n",
            "sd_f1: 0.008967481264584083\n",
            "sd_mcc: 0.0\n",
            "sd_sn: 0.0\n",
            "sd_sp: 0.0\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       331\n",
            "           1       0.71      1.00      0.83       818\n",
            "\n",
            "    accuracy                           0.71      1149\n",
            "   macro avg       0.36      0.50      0.42      1149\n",
            "weighted avg       0.51      0.71      0.59      1149\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.013122935523096362, 0.0, 0.008967481264584083)"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predicted_rnn = classifier.predict(X_test)\n",
        "predicted_rnn = np.where(predicted_rnn > 0.5, 1, 0)\n",
        "predicted_rnn = np.reshape(predicted_rnn,(len(predicted_rnn),)).astype(int)\n",
        "error_rate(Y_test, predicted_rnn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "23XHv_0Hm6R-",
        "outputId": "b9df881d-339b-4575-e42c-4d0b7e13b5c8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7b73f026c8e0>]"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "fpr_RNN , tpr_RNN, thresholds_RNN = roc_curve(Y_test, predicted_rnn)\n",
        "auc_score = roc_auc_score(Y_test, predicted_rnn)\n",
        "plt.plot(fpr_RNN , tpr_RNN, label= \"RNN ({:.2f})\".format(auc_score))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUmKQxzxA1Db"
      },
      "source": [
        "### GRU with GridSearch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KK2NSwkbA1Dc"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import SimpleRNN, Bidirectional, Input, Embedding, LSTM, Dropout, Dense, InputLayer, GRU\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from keras import regularizers\n",
        "from keras.layers import Embedding, Bidirectional\n",
        "from keras.regularizers import l2\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from time import time\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "740UNHeXmkmt"
      },
      "outputs": [],
      "source": [
        "from numpy.random import seed\n",
        "seed(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RtsqGegkmkmu"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import set_random_seed\n",
        "set_random_seed(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hN7nCbmTcKg2"
      },
      "outputs": [],
      "source": [
        "X_train, X_test= train_dataset, test_dataset\n",
        "y_train, y_test = training_labels, testing_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gdRdiuWWcKg3"
      },
      "outputs": [],
      "source": [
        "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1],1))\n",
        "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1],1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZo1uuvLv7jK"
      },
      "outputs": [],
      "source": [
        "input_shape = X_train[1,:].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7889QoQ7v7jN"
      },
      "outputs": [],
      "source": [
        "Y_train = np.reshape(y_train,(len(y_train),1)).astype(int)\n",
        "Y_test = np.reshape(y_test,(len(y_test),1)).astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4yBJvwV3A1De"
      },
      "outputs": [],
      "source": [
        "max_length = 1280"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yDwfKhy9A1Dl"
      },
      "outputs": [],
      "source": [
        "def create_gru_model(dropout_rate=0.0, units=50, learning_rate_init=0.001, regularizer=0.0, solver='adam'):\n",
        "    x_input = layers.Input(input_shape)\n",
        "    #emb = Embedding(21, units, input_length=max_length)(x_input)\n",
        "    gru = GRU(units, activity_regularizer=l2(regularizer), return_sequences = True)(x_input) #kernel_regularizer=l2(0.01), recurrent_regularizer=l2(0.01), bias_regularizer=l2(0.01)\n",
        "    x = Dropout(dropout_rate)(gru)\n",
        "    gru2 =GRU(units, activity_regularizer=l2(dropout_rate), return_sequences = True)(x) #kernel_regularizer=l2(0.01), recurrent_regularizer=l2(0.01), bias_regularizer=l2(0.01)\n",
        "    x2 = Dropout(dropout_rate)(gru2)\n",
        "    gru3 =GRU(units, activity_regularizer=l2(dropout_rate))(x2) #kernel_regularizer=l2(0.01), recurrent_regularizer=l2(0.01), bias_regularizer=l2(0.01)\n",
        "    x3 = Dropout(dropout_rate)(gru3)\n",
        "    # softmax classifier\n",
        "    x_output = Dense(1, activation='sigmoid')(x3)\n",
        "\n",
        "    model = Model(inputs=x_input, outputs=x_output)\n",
        "    # Compile model\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate_init),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy'],\n",
        "        )\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vhouw-7FA1Dm",
        "outputId": "6f139695-56cf-44d1-e360-d8ae682ad335"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_8 (InputLayer)        [(None, 2560, 1)]         0         \n",
            "                                                                 \n",
            " gru_6 (GRU)                 (None, 2560, 50)          7950      \n",
            "                                                                 \n",
            " dropout_15 (Dropout)        (None, 2560, 50)          0         \n",
            "                                                                 \n",
            " gru_7 (GRU)                 (None, 2560, 50)          15300     \n",
            "                                                                 \n",
            " dropout_16 (Dropout)        (None, 2560, 50)          0         \n",
            "                                                                 \n",
            " gru_8 (GRU)                 (None, 50)                15300     \n",
            "                                                                 \n",
            " dropout_17 (Dropout)        (None, 50)                0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 1)                 51        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 38,601\n",
            "Trainable params: 38,601\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "gru_model = create_gru_model()\n",
        "gru_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-w417EQA1Do",
        "outputId": "c0f915f9-76e8-4007-a7db-e4725beae6f8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-127-a4768240d228>:4: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
            "  model = KerasClassifier(build_fn=create_gru_model, verbose=1, epochs=3, batch_size=128)\n"
          ]
        }
      ],
      "source": [
        "start = time()\n",
        "\n",
        "# create model\n",
        "model = KerasClassifier(build_fn=create_gru_model, verbose=1, epochs=3, batch_size=128)\n",
        "# define parameters and values for grid search\n",
        "\n",
        "parameters = {\n",
        "    'units':[50], # 1024\n",
        "    'learning_rate_init': [0.0001],\n",
        "    #'solver':['adam','sgd'],\n",
        "    'epochs':[5],\n",
        "    'dropout_rate':[0.0], #0.05\n",
        "    'regularizer':[0.0],\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rkOj6mr-A1Dp"
      },
      "outputs": [],
      "source": [
        "metrics = {'accuracy':make_scorer(accuracy_score,greater_is_better=True),'f1':make_scorer(f1_score,greater_is_better=True),'mcc':make_scorer(matthews_corrcoef,greater_is_better=True)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yigwkBtWA1Dq",
        "outputId": "895c942f-b86f-4dba-a716-ca88da0e4f05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "36/36 [==============================] - 251s 7s/step - loss: 0.6782 - accuracy: 0.7083\n",
            "Epoch 2/5\n",
            "36/36 [==============================] - 238s 7s/step - loss: 0.6344 - accuracy: 0.7120\n",
            "Epoch 3/5\n",
            "36/36 [==============================] - 242s 7s/step - loss: 0.5998 - accuracy: 0.7120\n",
            "Epoch 4/5\n",
            "36/36 [==============================] - 238s 7s/step - loss: 0.5997 - accuracy: 0.7120\n",
            "Epoch 5/5\n",
            "36/36 [==============================] - 237s 7s/step - loss: 0.5995 - accuracy: 0.7120\n"
          ]
        }
      ],
      "source": [
        "grid = GridSearchCV(estimator=model, param_grid=parameters, n_jobs=-1, cv=10, scoring=metrics, refit='mcc')\n",
        "grid_result = grid.fit(X_train, Y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_9fa6hpA1Dr"
      },
      "outputs": [],
      "source": [
        "classifier = grid.best_estimator_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFBt4GSMA1Ds",
        "outputId": "f3a3e7a7-a6b0-4f6e-aaae-d455e53b1175"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'verbose': 1,\n",
              " 'epochs': 5,\n",
              " 'batch_size': 128,\n",
              " 'dropout_rate': 0.0,\n",
              " 'learning_rate_init': 0.0001,\n",
              " 'regularizer': 0.0,\n",
              " 'units': 50,\n",
              " 'build_fn': <function __main__.create_gru_model(dropout_rate=0.0, units=50, learning_rate_init=0.001, regularizer=0.0, solver='adam')>}"
            ]
          },
          "execution_count": 130,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "params = classifier.get_params()\n",
        "params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IyGS6TWyA1Du"
      },
      "outputs": [],
      "source": [
        "#cv_results = DataFrame(grid.cv_results_)\n",
        "\n",
        "#cv_results[['param_units','param_dropout_rate','param_regularizer','split0_test_score', 'split1_test_score', 'split2_test_score']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4QVS3LvMA1Dv"
      },
      "outputs": [],
      "source": [
        "df =pd.DataFrame(grid.cv_results_)\n",
        "new_path = '/content/test.xls'\n",
        "writer = pd.ExcelWriter(new_path, engine='xlsxwriter')\n",
        "df.to_excel('/content/drive/MyDrive/Acidophilic/ESM23BGRU.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhqrzzESA1Dw",
        "outputId": "097e7062-0fd6-427f-ba8d-c274649532d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "144/144 [==============================] - 112s 767ms/step\n",
            "acc: 0.7119529719137818\n",
            "f1: 0.8317436093094238\n",
            "mcc: 0.0\n",
            "sn: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "sp: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "sd_acc: 0.006851777839605985\n",
            "sd_f1: 0.004676051386765148\n",
            "sd_mcc: 0.0\n",
            "sd_sn: 0.0\n",
            "sd_sp: 0.0\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00      1323\n",
            "           1       0.71      1.00      0.83      3270\n",
            "\n",
            "    accuracy                           0.71      4593\n",
            "   macro avg       0.36      0.50      0.42      4593\n",
            "weighted avg       0.51      0.71      0.59      4593\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.006851777839605985, 0.0, 0.004676051386765148)"
            ]
          },
          "execution_count": 132,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predicted_training_labels = classifier.predict(X_train)\n",
        "predicted_training_labels = np.where(predicted_training_labels > 0.5, 1, 0)\n",
        "predicted_training_labels = np.reshape(predicted_training_labels,(len(predicted_training_labels),)).astype(int)\n",
        "error_rate(Y_train, predicted_training_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4Tz8-duA1Dx",
        "outputId": "5c60098d-065d-4e63-9c8a-66f92cc02660"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "36/36 [==============================] - 27s 742ms/step\n",
            "acc: 0.7119234116623151\n",
            "f1: 0.8317234367056432\n",
            "mcc: 0.0\n",
            "sn: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "sp: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "sd_acc: 0.013122935523096362\n",
            "sd_f1: 0.008967481264584083\n",
            "sd_mcc: 0.0\n",
            "sd_sn: 0.0\n",
            "sd_sp: 0.0\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       331\n",
            "           1       0.71      1.00      0.83       818\n",
            "\n",
            "    accuracy                           0.71      1149\n",
            "   macro avg       0.36      0.50      0.42      1149\n",
            "weighted avg       0.51      0.71      0.59      1149\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.013122935523096362, 0.0, 0.008967481264584083)"
            ]
          },
          "execution_count": 133,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predicted_gru = classifier.predict(X_test)\n",
        "predicted_gru = np.where(predicted_gru > 0.5, 1, 0)\n",
        "predicted_gru = np.reshape(predicted_gru,(len(predicted_gru),)).astype(int)\n",
        "error_rate(Y_test, predicted_gru)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "-xj9Hkk_udoP",
        "outputId": "dc1019fe-ac08-4f71-c281-c2d8792bc138"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x794b3c3946a0>]"
            ]
          },
          "execution_count": 134,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "fpr_GRU , tpr_GRU, thresholds_GRU = roc_curve(Y_test, predicted_gru)\n",
        "auc_score = roc_auc_score(Y_test, predicted_gru)\n",
        "plt.plot(fpr_GRU, tpr_GRU, label= \"GRU ({:.2f})\".format(auc_score))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_fkn3Mq1SP9"
      },
      "source": [
        "### LSTM with GridSearch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nL1rhCTLOm3i"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import SimpleRNN, Bidirectional, Input, Embedding, LSTM, Dropout, Dense, InputLayer\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from keras import regularizers\n",
        "from keras.layers import Embedding, Bidirectional\n",
        "from keras.regularizers import l2\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from time import time\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HV-qrcrllpUl"
      },
      "outputs": [],
      "source": [
        "from numpy.random import seed\n",
        "seed(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ChaSNTnXlpUn"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import set_random_seed\n",
        "set_random_seed(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4H-YJ54cLXv"
      },
      "outputs": [],
      "source": [
        "X_train, X_test= train_dataset, test_dataset\n",
        "y_train, y_test = training_labels, testing_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oD-sMZnncLXv"
      },
      "outputs": [],
      "source": [
        "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1],1))\n",
        "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1],1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ab12qLmwv9CR"
      },
      "outputs": [],
      "source": [
        "input_shape = X_train[1,:].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g0jIxbZ6v9CS"
      },
      "outputs": [],
      "source": [
        "Y_train = np.reshape(y_train,(len(y_train),1)).astype(int)\n",
        "Y_test = np.reshape(y_test,(len(y_test),1)).astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zeXpOYNQTnFZ"
      },
      "outputs": [],
      "source": [
        "def create_lstm_model(activation='tanh', dropout_rate=0.05, units=1024, learning_rate_init= 0.001):\n",
        "    x_input = layers.Input(input_shape)\n",
        "    #emb = Embedding(21, units, input_length=max_length)(x_input)\n",
        "    lstm = LSTM(units, activity_regularizer=l2(dropout_rate))(x_input) #kernel_regularizer=l2(0.01), recurrent_regularizer=l2(0.01), bias_regularizer=l2(0.01)\n",
        "    #x = Dropout(dropout_rate)(lstm)\n",
        "\n",
        "    # softmax classifier\n",
        "    x_output = Dense(1, activation='sigmoid')(lstm)\n",
        "\n",
        "    model = Model(inputs=x_input, outputs=x_output)\n",
        "    # Compile model\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate_init),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy'],\n",
        "        )\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILlQCWCe1SP9",
        "outputId": "987192ef-6435-4e51-cc4d-fda6f2084fc2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, 2560, 1)]         0         \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 1024)              4202496   \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 1025      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,203,521\n",
            "Trainable params: 4,203,521\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "lstm_model = create_lstm_model()\n",
        "lstm_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OoWHbW1IUa9s",
        "outputId": "99f64bbc-5900-46df-edd8-e39066699930"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-76-f5ea6ecf6c08>:4: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
            "  model = KerasClassifier(build_fn=create_lstm_model, verbose=1, epochs=3, batch_size=128)\n"
          ]
        }
      ],
      "source": [
        "start = time()\n",
        "\n",
        "# create model\n",
        "model = KerasClassifier(build_fn=create_lstm_model, verbose=1, epochs=3, batch_size=128)\n",
        "# define parameters and values for grid search\n",
        "\n",
        "parameters = {\n",
        "    'units':[50, 125], # 1024\n",
        "    'learning_rate_init': [0.0001],\n",
        "    #'solver':['adam','sgd'],\n",
        "    'epochs':[5],\n",
        "    'dropout_rate':[0.0], #0.05\n",
        "    #'regularizer':[0.0],\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3KhcgctvdfqW"
      },
      "outputs": [],
      "source": [
        "metrics = {'accuracy':make_scorer(accuracy_score,greater_is_better=True),'f1':make_scorer(f1_score,greater_is_better=True),'mcc':make_scorer(matthews_corrcoef,greater_is_better=True)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Numsz-XU06h",
        "outputId": "f573c6aa-e248-49ad-953f-a1261f3e92f0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "36/36 [==============================] - 70s 2s/step - loss: 0.6880 - accuracy: 0.7076\n",
            "Epoch 2/5\n",
            "36/36 [==============================] - 68s 2s/step - loss: 0.6736 - accuracy: 0.7120\n",
            "Epoch 3/5\n",
            "36/36 [==============================] - 69s 2s/step - loss: 0.6212 - accuracy: 0.7120\n",
            "Epoch 4/5\n",
            "36/36 [==============================] - 69s 2s/step - loss: 0.6014 - accuracy: 0.7120\n",
            "Epoch 5/5\n",
            "36/36 [==============================] - 71s 2s/step - loss: 0.6010 - accuracy: 0.7120\n"
          ]
        }
      ],
      "source": [
        "grid = GridSearchCV(estimator=model, param_grid=parameters, n_jobs=-1, cv=10, scoring=metrics, refit='mcc')\n",
        "grid_result = grid.fit(X_train, Y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qttmg9ayVGeL"
      },
      "outputs": [],
      "source": [
        "classifier = grid.best_estimator_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zvaGtn0VGeL",
        "outputId": "c93f3ca3-6380-41c4-84be-739651eacfe3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'verbose': 1,\n",
              " 'epochs': 5,\n",
              " 'batch_size': 128,\n",
              " 'dropout_rate': 0.0,\n",
              " 'learning_rate_init': 0.0001,\n",
              " 'units': 50,\n",
              " 'build_fn': <function __main__.create_lstm_model(activation='tanh', dropout_rate=0.05, units=1024, learning_rate_init=0.001)>}"
            ]
          },
          "execution_count": 80,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "params = classifier.get_params()\n",
        "params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_kGV3S6VGeL"
      },
      "outputs": [],
      "source": [
        "#cv_results = DataFrame(grid.cv_results_)\n",
        "\n",
        "#cv_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOW8GSano6QO"
      },
      "outputs": [],
      "source": [
        "df =pd.DataFrame(grid.cv_results_)\n",
        "new_path = '/content/test.xls'\n",
        "writer = pd.ExcelWriter(new_path, engine='xlsxwriter')\n",
        "df.to_excel('/content/drive/MyDrive/Acidophilic/ESM23BLSTM.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VsMHQT0K8vHE",
        "outputId": "e7c7bfcf-0390-4149-dd9b-179a99a40004"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "144/144 [==============================] - 46s 316ms/step\n",
            "acc: 0.7119529719137818\n",
            "f1: 0.8317436093094238\n",
            "mcc: 0.0\n",
            "sn: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "sp: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "sd_acc: 0.006806411142741788\n",
            "sd_f1: 0.004646042359589499\n",
            "sd_mcc: 0.0\n",
            "sd_sn: 0.0\n",
            "sd_sp: 0.0\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00      1323\n",
            "           1       0.71      1.00      0.83      3270\n",
            "\n",
            "    accuracy                           0.71      4593\n",
            "   macro avg       0.36      0.50      0.42      4593\n",
            "weighted avg       0.51      0.71      0.59      4593\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.006806411142741788, 0.0, 0.004646042359589499)"
            ]
          },
          "execution_count": 83,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predicted_training_labels = classifier.predict(X_train)\n",
        "predicted_training_labels = np.where(predicted_training_labels > 0.5, 1, 0)\n",
        "predicted_training_labels = np.reshape(predicted_training_labels,(len(predicted_training_labels),)).astype(int)\n",
        "error_rate(Y_train, predicted_training_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKqfkrJb1SP-",
        "outputId": "22ddedc4-d87d-4509-9b11-f5fa3a02417c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "36/36 [==============================] - 11s 311ms/step\n",
            "acc: 0.7119234116623151\n",
            "f1: 0.8317234367056432\n",
            "mcc: 0.0\n",
            "sn: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "sp: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "sd_acc: 0.01364973985779873\n",
            "sd_f1: 0.009312001695632717\n",
            "sd_mcc: 0.0\n",
            "sd_sn: 0.0\n",
            "sd_sp: 0.0\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       331\n",
            "           1       0.71      1.00      0.83       818\n",
            "\n",
            "    accuracy                           0.71      1149\n",
            "   macro avg       0.36      0.50      0.42      1149\n",
            "weighted avg       0.51      0.71      0.59      1149\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.01364973985779873, 0.0, 0.009312001695632717)"
            ]
          },
          "execution_count": 84,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predicted_lstm = classifier.predict(X_test)\n",
        "predicted_lstm = np.where(predicted_lstm > 0.5, 1, 0)\n",
        "predicted_lstm = np.reshape(predicted_lstm,(len(predicted_lstm),)).astype(int)\n",
        "error_rate(Y_test, predicted_lstm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "zZ4-5NunugvS",
        "outputId": "bc5c90b8-a4b9-4dcc-ef57-3d805711f3e3"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-86-da9cea2eba67>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfpr_LSTM\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtpr_LSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresholds_LSTM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_lstm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mauc_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_lstm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpr_LSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr_LSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m\"LSTM ({:.2f})\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauc_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36mroc_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight, drop_intermediate)\u001b[0m\n\u001b[1;32m    990\u001b[0m     \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1.8\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m0.8\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m0.4\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m0.35\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m     \"\"\"\n\u001b[0;32m--> 992\u001b[0;31m     fps, tps, thresholds = _binary_clf_curve(\n\u001b[0m\u001b[1;32m    993\u001b[0m         \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpos_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    994\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36m_binary_clf_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[0my_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y_true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"binary\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mpos_label\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 749\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{0} format is not supported\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: unknown format is not supported"
          ]
        }
      ],
      "source": [
        "fpr_LSTM , tpr_LSTM, thresholds_LSTM = roc_curve(Y_test, predicted_lstm)\n",
        "auc_score = roc_auc_score(Y_test, predicted_lstm)\n",
        "plt.plot(fpr_LSTM, tpr_LSTM, label= \"LSTM ({:.2f})\".format(auc_score))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gghPVbbuLbqN"
      },
      "source": [
        "### BiLSTM with GridSearch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-ZqdqeELYEp"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import SimpleRNN, Bidirectional, Input, Embedding, LSTM, Dropout, Dense, InputLayer\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from keras import regularizers\n",
        "from keras.layers import Embedding, Bidirectional\n",
        "from keras.regularizers import l2\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from time import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VykWP3LoloW1"
      },
      "outputs": [],
      "source": [
        "from numpy.random import seed\n",
        "seed(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQZ80_qDloW2"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import set_random_seed\n",
        "set_random_seed(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3LAhWKvcMUW"
      },
      "outputs": [],
      "source": [
        "X_train, X_test= train_dataset, test_dataset\n",
        "y_train, y_test = training_labels, testing_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VewBv-PHcMUX"
      },
      "outputs": [],
      "source": [
        "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1],1))\n",
        "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1],1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHTQUCxtv-DU"
      },
      "outputs": [],
      "source": [
        "input_shape = X_train[1,:].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r2usnetev-DW"
      },
      "outputs": [],
      "source": [
        "Y_train = np.reshape(y_train,(len(y_train),1)).astype(int)\n",
        "Y_test = np.reshape(y_test,(len(y_test),1)).astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JN7ZQUqrWfW8"
      },
      "outputs": [],
      "source": [
        "num_words = 22\n",
        "num_classes = 1\n",
        "n_cv = 3\n",
        "num_hiddens = 1024\n",
        "num_steps = 10\n",
        "num_layers = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8Q4lB7lDArN"
      },
      "outputs": [],
      "source": [
        "def create_blstm_model1(activation='relu', dropout_rate=0.0, units=1024, learning_rate_init=0.001):\n",
        "    x_input = layers.Input(input_shape)\n",
        "    #emb = Embedding(21, units, input_length=max_length)(x_input)\n",
        "    bi_rnn = Bidirectional(LSTM(units, activity_regularizer=l2(dropout_rate)))(x_input) #kernel_regularizer=l2(0.01), recurrent_regularizer=l2(0.01), bias_regularizer=l2(0.01)\n",
        "    x = Dropout(dropout_rate)(bi_rnn)\n",
        "\n",
        "    # softmax classifier\n",
        "    x_output = Dense(1, activation='sigmoid')(bi_rnn)\n",
        "\n",
        "    model = Model(inputs=x_input, outputs=x_output)\n",
        "    # Compile model\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate_init),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy'],\n",
        "        )\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9nPpnnK_FNC"
      },
      "outputs": [],
      "source": [
        "blstm1 = create_blstm_model1()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "boSIUOE0CVJ3",
        "outputId": "2c4eb9cb-7087-41c4-b12c-cb9fec4ffe0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_6 (InputLayer)        [(None, 2560, 1)]         0         \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirectio  (None, 2048)             8404992   \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 2049      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 8,407,041\n",
            "Trainable params: 8,407,041\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "blstm1.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMK08riVVajL",
        "outputId": "166e1313-b1f2-41dd-bc5e-4c2d45f227c3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-114-7ce50718670a>:4: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
            "  model = KerasClassifier(build_fn=create_blstm_model1, verbose=1, epochs=3, batch_size=128)\n"
          ]
        }
      ],
      "source": [
        "start = time()\n",
        "\n",
        "# create model\n",
        "model = KerasClassifier(build_fn=create_blstm_model1, verbose=1, epochs=3, batch_size=128)\n",
        "# define parameters and values for grid search\n",
        "\n",
        "parameters = {\n",
        "    'units':[50,128], # 1024\n",
        "    'learning_rate_init': [0.0001],\n",
        "    #'solver':['adam','sgd'],\n",
        "    'epochs':[5],\n",
        "    'dropout_rate':[0.0], #0.05\n",
        "    #'regularizer':[0.0],\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2E_3KXV-ds9X"
      },
      "outputs": [],
      "source": [
        "metrics = {'accuracy':make_scorer(accuracy_score,greater_is_better=True),'f1':make_scorer(f1_score,greater_is_better=True),'mcc':make_scorer(matthews_corrcoef,greater_is_better=True)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5gZrhg7XrYk",
        "outputId": "f619aad8-d0f2-459a-eccf-a3e62c48bf05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "36/36 [==============================] - 77s 2s/step - loss: 0.6863 - accuracy: 0.7085\n",
            "Epoch 2/5\n",
            "36/36 [==============================] - 70s 2s/step - loss: 0.6660 - accuracy: 0.7120\n",
            "Epoch 3/5\n",
            "36/36 [==============================] - 70s 2s/step - loss: 0.6127 - accuracy: 0.7120\n",
            "Epoch 4/5\n",
            "36/36 [==============================] - 70s 2s/step - loss: 0.6007 - accuracy: 0.7120\n",
            "Epoch 5/5\n",
            "36/36 [==============================] - 70s 2s/step - loss: 0.6005 - accuracy: 0.7120\n"
          ]
        }
      ],
      "source": [
        "grid = GridSearchCV(estimator=model, param_grid=parameters, n_jobs=-1, cv=10, scoring=metrics, refit='mcc')\n",
        "grid_result = grid.fit(X_train, Y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s34hMuo0Z2w2"
      },
      "outputs": [],
      "source": [
        "classifier = grid.best_estimator_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xmAALc4Z-hd",
        "outputId": "ded6161e-75cc-4478-f184-889bb43039fb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'verbose': 1,\n",
              " 'epochs': 5,\n",
              " 'batch_size': 128,\n",
              " 'dropout_rate': 0.0,\n",
              " 'learning_rate_init': 0.0001,\n",
              " 'units': 50,\n",
              " 'build_fn': <function __main__.create_blstm_model1(activation='relu', dropout_rate=0.0, units=1024, learning_rate_init=0.001)>}"
            ]
          },
          "execution_count": 118,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "params = classifier.get_params()\n",
        "params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "id": "RmF350wmb3Bf",
        "outputId": "9db80688-633a-40a7-8727-4c98c36011c4"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-119-0e9bec3014c5>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcv_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv_results_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'DataFrame' is not defined"
          ]
        }
      ],
      "source": [
        "#cv_results = DataFrame(grid.cv_results_)\n",
        "\n",
        "#cv_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mnu0uvNzb3C0"
      },
      "outputs": [],
      "source": [
        "df =pd.DataFrame(grid.cv_results_)\n",
        "new_path = '/content/test.xls'\n",
        "writer = pd.ExcelWriter(new_path, engine='xlsxwriter')\n",
        "df.to_excel('/content/drive/MyDrive/Acidophilic/ESM23BBLSTM.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "toJ9ceVVcEVU",
        "outputId": "b3826a18-f8cb-4a4a-bf14-df3720b214ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "144/144 [==============================] - 51s 345ms/step\n",
            "acc: 0.7119529719137818\n",
            "f1: 0.8317436093094238\n",
            "mcc: 0.0\n",
            "sn: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "sp: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "sd_acc: 0.006851777839605985\n",
            "sd_f1: 0.004676051386765148\n",
            "sd_mcc: 0.0\n",
            "sd_sn: 0.0\n",
            "sd_sp: 0.0\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00      1323\n",
            "           1       0.71      1.00      0.83      3270\n",
            "\n",
            "    accuracy                           0.71      4593\n",
            "   macro avg       0.36      0.50      0.42      4593\n",
            "weighted avg       0.51      0.71      0.59      4593\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.006851777839605985, 0.0, 0.004676051386765148)"
            ]
          },
          "execution_count": 121,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predicted_training_labels = classifier.predict(X_train)\n",
        "predicted_training_labels = np.where(predicted_training_labels > 0.5, 1, 0)\n",
        "predicted_training_labels = np.reshape(predicted_training_labels,(len(predicted_training_labels),)).astype(int)\n",
        "error_rate(Y_train, predicted_training_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRn-66gpcEWu",
        "outputId": "7bbe974b-2912-4777-cc29-788ef8737148"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "36/36 [==============================] - 12s 340ms/step\n",
            "acc: 0.7119234116623151\n",
            "f1: 0.8317234367056432\n",
            "mcc: 0.0\n",
            "sn: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "sp: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "sd_acc: 0.013122935523096362\n",
            "sd_f1: 0.008967481264584083\n",
            "sd_mcc: 0.0\n",
            "sd_sn: 0.0\n",
            "sd_sp: 0.0\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       331\n",
            "           1       0.71      1.00      0.83       818\n",
            "\n",
            "    accuracy                           0.71      1149\n",
            "   macro avg       0.36      0.50      0.42      1149\n",
            "weighted avg       0.51      0.71      0.59      1149\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.013122935523096362, 0.0, 0.008967481264584083)"
            ]
          },
          "execution_count": 122,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predicted_blstm = classifier.predict(X_test)\n",
        "predicted_blstm = np.where(predicted_blstm > 0.5, 1, 0)\n",
        "predicted_blstm = np.reshape(predicted_blstm,(len(predicted_blstm),)).astype(int)\n",
        "error_rate(Y_test, predicted_blstm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "Mq63NqNfukNI",
        "outputId": "a37dbaf0-57a7-4384-beec-b9f842c0f7a6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x793e102fb610>]"
            ]
          },
          "execution_count": 123,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "fpr_BLSTM , tpr_BLSTM, thresholds_BLSTM = roc_curve(Y_test, predicted_blstm)\n",
        "auc_score = roc_auc_score(Y_test, predicted_blstm)\n",
        "plt.plot(fpr_BLSTM, tpr_BLSTM, label= \"BLSTM ({:.2f})\".format(auc_score))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "mtyKU7eecPix",
        "outputId": "0c1f0cb1-e19f-4f9b-e4df-00249c4db1a5"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-124-e322b76455ef>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mconfusion_matrix_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_blstm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mconf_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfusion_matrix_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mconfusion_matrix_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'y_val' is not defined"
          ]
        }
      ],
      "source": [
        "classes = np.unique(y_val)\n",
        "\n",
        "confusion_matrix_data = confusion_matrix(y_val, predicted_blstm, labels=classes)\n",
        "conf_matrix(confusion_matrix_data)\n",
        "confusion_matrix_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpWiWyk_oHXb"
      },
      "source": [
        "## ROC / AUC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1MmK9F3oL3b"
      },
      "outputs": [],
      "source": [
        "plt.legend()\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "# plt.title('Receiver Operating Characteristic')\n",
        "plt.savefig('AUROC.png', dpi = 600)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWrjbaEbPL8J"
      },
      "source": [
        "### LR with GridSearch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Ur7Rs3yPKpr"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_selection import RFE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rwyF5b1wPTOH"
      },
      "outputs": [],
      "source": [
        "lr = LogisticRegression()\n",
        "lr.fit(train_dataset, training_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRe-fKacPyVH"
      },
      "outputs": [],
      "source": [
        "hyper_params = [{'n_features_to_select': list(range(1, 1024))}]\n",
        "rfe = RFE(lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FeyDjPkYPwir"
      },
      "outputs": [],
      "source": [
        "model_cv = GridSearchCV(estimator = rfe,\n",
        "                        param_grid = hyper_params,\n",
        "                        scoring= 'r2',\n",
        "                        cv = 3,\n",
        "                        verbose = 1,\n",
        "                        return_train_score=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hT54Q2SFQSQ-"
      },
      "outputs": [],
      "source": [
        "model_cv.fit(train_dataset, training_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2rVcpSHQnj6"
      },
      "outputs": [],
      "source": [
        "cv_results = pd.DataFrame(model_cv.cv_results_)\n",
        "cv_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UejYIxeXLxAO"
      },
      "outputs": [],
      "source": [
        "df =pd.DataFrame(model_cv.cv_results_)\n",
        "#new_path = '/content/test.xls'\n",
        "#writer = pd.ExcelWriter(new_path, engine='xlsxwriter')\n",
        "df.to_excel('/content/drive/MyDrive/SOM/LRESM1bMerged.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JlJMkOX4QqfX"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(16,6))\n",
        "\n",
        "plt.plot(cv_results[\"param_n_features_to_select\"], cv_results[\"mean_test_score\"])\n",
        "plt.plot(cv_results[\"param_n_features_to_select\"], cv_results[\"mean_train_score\"])\n",
        "plt.xlabel('number of features')\n",
        "plt.ylabel('r-squared')\n",
        "plt.title(\"Optimal Number of Features\")\n",
        "plt.legend(['test score', 'train score'], loc='upper left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9pOz5dr8Q3hU"
      },
      "outputs": [],
      "source": [
        "# final model\n",
        "n_features_optimal = 10\n",
        "\n",
        "lm = LinearRegression()\n",
        "lm.fit(X_train, y_train)\n",
        "\n",
        "rfe = RFE(lm, n_features_to_select=n_features_optimal)\n",
        "rfe = rfe.fit(X_train, y_train)\n",
        "\n",
        "# predict prices of X_test\n",
        "y_pred = lm.predict(X_test)\n",
        "r2 = sklearn.metrics.r2_score(y_test, y_pred)\n",
        "print(r2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4GdUnvG_-ZQ"
      },
      "source": [
        "### MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qDtBAkOi7R4i"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Input, Dense, Dropout, Activation\n",
        "from keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping, CSVLogger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uoIc1Jtfx1NF"
      },
      "outputs": [],
      "source": [
        "X_train, X_test= train_dataset, test_dataset\n",
        "y_train, y_test = training_labels, testing_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXqnJjz9exCE"
      },
      "outputs": [],
      "source": [
        "#seed_value= 100\n",
        "#tf.random.set_seed(seed_value)\n",
        "n_cols = X_train.shape[1]\n",
        "n_cols"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SdQovHxve07N"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(30,activation='relu', input_shape=(n_cols,)))\n",
        "#model.add(Dense(2, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X90_feOQGK8x"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08), loss=\"mse\", metrics=\"accuracy\", )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXD_MjdFGLLf"
      },
      "outputs": [],
      "source": [
        "checkpointer = ModelCheckpoint(\n",
        "    filepath='folder/.{epoch:03d}-{val_loss:.3f}.h5',\n",
        "    verbose=1,\n",
        "    save_best_only=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M4lZf20hsa2N"
      },
      "outputs": [],
      "source": [
        "es = EarlyStopping(monitor='val_loss', patience=150)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1nuaKoN-GIg8"
      },
      "outputs": [],
      "source": [
        "history = model.fit(X_train, y_train,\n",
        "   epochs = 1000,\n",
        "   verbose = 0,\n",
        "   validation_data=(X_test,y_test), shuffle=True, validation_split = 0.1\n",
        "  # callbacks = [es, checkpointer]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bGwoWzjh0aFH"
      },
      "outputs": [],
      "source": [
        "y_pred = model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BecSrkLj0fol"
      },
      "outputs": [],
      "source": [
        "y_pred = np.reshape(y_pred,(len(y_pred),)).astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ga12a1A00gmy"
      },
      "outputs": [],
      "source": [
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPOQ38Fu0jC2"
      },
      "outputs": [],
      "source": [
        "print(matthews_corrcoef(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vppRZcnfjcHl"
      },
      "outputs": [],
      "source": [
        "plot_history(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2Gi5c7qjf1x"
      },
      "outputs": [],
      "source": [
        "display_model_score(model,\n",
        "    [X_train, y_train],\n",
        "    [X_test, y_test],\n",
        "    [X_test, y_test],\n",
        "    256)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-Pbo7kf0_wJ"
      },
      "outputs": [],
      "source": [
        "classes = np.unique(y_test)\n",
        "\n",
        "confusion_matrix_data = confusion_matrix(y_test, y_pred, labels=classes)\n",
        "\n",
        "confusion_matrix_figure = px.imshow(\n",
        "    confusion_matrix_data,\n",
        "    labels=dict(x=\"True label\", y=\"Predicted label\", color=\"# of samples\"),\n",
        "    x=classes,\n",
        "    y=classes,\n",
        "    color_continuous_scale='Gray_r'\n",
        ")\n",
        "confusion_matrix_figure.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aR_uLGY305Sk"
      },
      "outputs": [],
      "source": [
        "TP = confusion_matrix_data[1,1]\n",
        "TN = confusion_matrix_data[0,0]\n",
        "FP = confusion_matrix_data[0,1]\n",
        "FN = confusion_matrix_data[1,0]\n",
        "\n",
        "print(TP,TN, FP, FN)\n",
        "\n",
        "sn = TP / float(TP + FN)\n",
        "print(sn)\n",
        "sp = TN / float(TN + FP)\n",
        "print(sp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0lsymQG-Lg0o"
      },
      "outputs": [],
      "source": [
        "model.save_weights('MLP.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L51kTUhGzRCT"
      },
      "source": [
        "### CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q5oIAt_tzPi2"
      },
      "outputs": [],
      "source": [
        "Y_train = np.reshape(y_train,(len(y_train),1)).astype(int)\n",
        "Y_test = np.reshape(y_test,(len(y_test),1)).astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "juF7N1OazXwC"
      },
      "outputs": [],
      "source": [
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.layers import Embedding\n",
        "from keras.preprocessing import sequence\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.regularizers import l2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHGAzu9nza0P"
      },
      "outputs": [],
      "source": [
        "n_timesteps, n_features, n_outputs =train_dataset.shape[0], train_dataset.shape[1], Y_train.shape[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-qd7WdmzdNQ"
      },
      "outputs": [],
      "source": [
        "# Building the CNN Model\n",
        "cnn_model = Sequential()      # initializing the Sequential nature for CNN model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jg1sSNozfti"
      },
      "outputs": [],
      "source": [
        "cnn_model.add(Conv1D(filters=128, kernel_size=3,\n",
        "activation='relu',input_shape=(n_features,1)))\n",
        "#cnn_model.add(Conv1D(filters=128, kernel_size=1, activation='relu'))\n",
        "#cnn_model.add(Dropout(0.5))\n",
        "cnn_model.add(MaxPooling1D(pool_size=2))\n",
        "cnn_model.add(Flatten())\n",
        "cnn_model.add(Dense(250, activation='relu'))\n",
        "cnn_model.add(Dense(n_outputs, activation='sigmoid'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGkcI2uyzh7S"
      },
      "outputs": [],
      "source": [
        "cnn_model.compile(loss='binary_crossentropy',\n",
        "                  optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "                  metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZ1n1as1zkG1"
      },
      "outputs": [],
      "source": [
        "cnn_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-4w2-F0S-mmW"
      },
      "outputs": [],
      "source": [
        "es = EarlyStopping(monitor='val_loss', patience=150, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2FexXQeWzmUn"
      },
      "outputs": [],
      "source": [
        "# fit network\n",
        "history = cnn_model.fit(train_dataset, Y_train,\n",
        "                        validation_data=(test_dataset, Y_test),\n",
        "                        callbacks=[es],\n",
        "                        epochs=500, batch_size=256, verbose=1\n",
        "                        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zV5zNmbzzpIs"
      },
      "outputs": [],
      "source": [
        "plot_history(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mSW3UaMxztcA"
      },
      "outputs": [],
      "source": [
        "display_model_score(cnn_model,\n",
        "    [train_dataset, Y_train],\n",
        "    [test_dataset, Y_test],\n",
        "    [test_dataset, Y_test],\n",
        "    256)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_YafGfGzvvK"
      },
      "outputs": [],
      "source": [
        "y_pred = cnn_model.predict(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNkmcnZgzwXv"
      },
      "outputs": [],
      "source": [
        "y_pred = np.reshape(y_pred,(len(y_pred),)).astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9mY6xOrkzy24"
      },
      "outputs": [],
      "source": [
        "print(classification_report(Y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zcssj1nu0l8X"
      },
      "outputs": [],
      "source": [
        "print(matthews_corrcoef(Y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CYYSjvKH09Ug"
      },
      "outputs": [],
      "source": [
        "classes = np.unique(Y_test)\n",
        "\n",
        "confusion_matrix_data = confusion_matrix(Y_test, y_pred, labels=classes)\n",
        "\n",
        "confusion_matrix_figure = px.imshow(\n",
        "    confusion_matrix_data,\n",
        "    labels=dict(x=\"True label\", y=\"Predicted label\", color=\"# of samples\"),\n",
        "    x=classes,\n",
        "    y=classes,\n",
        "    color_continuous_scale='Gray_r'\n",
        ")\n",
        "confusion_matrix_figure.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZaewkUk0qFP"
      },
      "outputs": [],
      "source": [
        "TP = confusion_matrix_data[1,1]\n",
        "TN = confusion_matrix_data[0,0]\n",
        "FP = confusion_matrix_data[0,1]\n",
        "FN = confusion_matrix_data[1,0]\n",
        "\n",
        "print(TP,TN, FP, FN)\n",
        "\n",
        "sn = TP / float(TP + FN)\n",
        "print(sn)\n",
        "sp = TN / float(TN + FP)\n",
        "print(sp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IBekoqvCXiJH"
      },
      "outputs": [],
      "source": [
        "cnn_model.save_weights('CNN.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5EKn4vmgSA9"
      },
      "source": [
        "## Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OGffLtH03vKz"
      },
      "outputs": [],
      "source": [
        "embeddings =list(map(lambda x: x[1], proteins))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "rLHorbm1sbOq"
      },
      "outputs": [],
      "source": [
        "p = np.array(embeddings)\n",
        "type(p)\n",
        "p.shape\n",
        "p = p.reshape(2809,1024)\n",
        "p.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "xGxoul2I8IQ0"
      },
      "outputs": [],
      "source": [
        "def compute_pca(X, n_components=2):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        X: of dimension (m,n) where each row corresponds to a word vector\n",
        "        n_components: Number of components you want to keep.\n",
        "    Output:\n",
        "        X_reduced: data transformed in 2 dims/columns + regenerated original data\n",
        "    pass in: data as 2D NumPy array\n",
        "    \"\"\"\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "    # mean center the data\n",
        "    X_demeaned = X - X.mean(axis=0)\n",
        "\n",
        "    # calculate the covariance matrix\n",
        "    covariance_matrix = np.cov(X_demeaned, rowvar=False)\n",
        "\n",
        "    # calculate eigenvectors & eigenvalues of the covariance matrix\n",
        "    eigen_vals, eigen_vecs = np.linalg.eigh(covariance_matrix)\n",
        "\n",
        "    # sort eigenvalue in increasing order (get the indices from the sort)\n",
        "    idx_sorted = np.argsort(eigen_vals)\n",
        "\n",
        "    # reverse the order so that it's from highest to lowest.\n",
        "    idx_sorted_decreasing = list(reversed(idx_sorted))\n",
        "\n",
        "    # sort the eigen values by idx_sorted_decreasing\n",
        "    eigen_vals_sorted = eigen_vals[idx_sorted_decreasing]\n",
        "\n",
        "    # sort eigenvectors using the idx_sorted_decreasing indices\n",
        "    eigen_vecs_sorted = eigen_vecs[:, idx_sorted_decreasing]\n",
        "\n",
        "    # select the first n eigenvectors (n is desired dimension\n",
        "    # of rescaled data array, or dims_rescaled_data)\n",
        "    eigen_vecs_subset = eigen_vecs_sorted[:, :n_components]\n",
        "\n",
        "    # transform the data by multiplying the transpose of the eigenvectors with the transpose of the de-meaned data\n",
        "    # Then take the transpose of that product.\n",
        "    X_reduced = np.dot(eigen_vecs_subset.T, X_demeaned.T).T\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return X_reduced"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "TiBjifl0sycb"
      },
      "outputs": [],
      "source": [
        "result = compute_pca(p, 2)\n",
        "plt.scatter(result[:, 0], result[:, 1])\n",
        "plt.figure(figsize = (20, 16))\n",
        "for i, word in enumerate(annotations.identifier):\n",
        "    plt.annotate(word, xy=(result[i, 0] - 0.0005, result[i, 1] + 0.001))\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5HxV3_R6syme"
      },
      "outputs": [],
      "source": [
        "proteins_reduced = compute_pca(p, n_components=5)\n",
        "print(proteins_reduced)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "kEihg6yR8ewz"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=5, svd_solver='full')\n",
        "pca.fit(p)\n",
        "print(pca.explained_variance_ratio_)\n",
        "print(pca.singular_values_)\n",
        "print(pca.score(p))\n",
        "print(pca.transform(p))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "fcTRLVgRs72s"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7t_5ECRss8bg"
      },
      "outputs": [],
      "source": [
        "def reduce_dim(weights, components = 3, method = 'tsne'):\n",
        "    \"\"\"Reduce dimensions of embeddings\"\"\"\n",
        "    if method == 'tsne':\n",
        "        return TSNE(components, metric = 'cosine').fit_transform(weights)\n",
        "    elif method == 'umap':\n",
        "        # Might want to try different parameters for UMAP\n",
        "        return UMAP(n_components=components, metric = 'cosine',\n",
        "                    init = 'random', n_neighbors = 5).fit_transform(weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "p5_DXp4Bs-iC"
      },
      "outputs": [],
      "source": [
        "protein_r = reduce_dim(p, components = 2, method = 'tsne')\n",
        "protein_r.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "yLOS1LoRs-lZ"
      },
      "outputs": [],
      "source": [
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "\n",
        "# Set shell to show all lines of output\n",
        "InteractiveShell.ast_node_interactivity = 'all'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LbCPkgtatFTg"
      },
      "outputs": [],
      "source": [
        "InteractiveShell.ast_node_interactivity = 'last'\n",
        "\n",
        "plt.figure(figsize = (20, 16))\n",
        "plt.plot(protein_r[:, 0], protein_r[:, 1], 'r.')\n",
        "plt.xlabel('TSNE 1'); plt.ylabel('TSNE 2'); plt.title('Protein Embeddings Visualized with TSNE');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "oqXeTzb7tOR3"
      },
      "outputs": [],
      "source": [
        "gen = ['non thermophilic', 'themophilic']\n",
        "ints = annotations.label\n",
        "\n",
        "plt.figure(figsize = (10, 8))\n",
        "\n",
        "# Plot embedding\n",
        "plt.scatter(protein_r[:, 0], protein_r[:, 1], c = ints)#, cmap = plt.cm.tab10)\n",
        "\n",
        "# Add colorbar and appropriate labels\n",
        "cbar = plt.colorbar()\n",
        "cbar.set_ticks([])\n",
        "for j, lab in enumerate(gen):\n",
        "    cbar.ax.text(1, (2 * j + 1) / ((10) * 2), lab, ha='left', va='center')\n",
        "cbar.ax.set_title('Label', loc = 'left')\n",
        "\n",
        "\n",
        "plt.xlabel('TSNE 1'); plt.ylabel('TSNE 2'); plt.title('TSNE Visualization of Protein Embeddings');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cc9lhZYp7ZVZ"
      },
      "source": [
        "### BLSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "CslowuzHMlka"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Embedding, Bidirectional, CuDNNLSTM, GlobalMaxPooling1D, LSTM\n",
        "from keras.regularizers import l2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "09WERpyscZ08"
      },
      "outputs": [],
      "source": [
        "x_input = Input(shape=(1024,))\n",
        "emb = Embedding(20, 512, input_length=1024)(x_input)\n",
        "bi_rnn = Bidirectional(CuDNNLSTM(256))(emb) #, kernel_regularizer=l2(0.0001), recurrent_regularizer=l2(0.0001), bias_regularizer=l2(0.0001)\n",
        "#bi_rnn = LSTM(512)(emb)\n",
        "#x = Dropout(0.2)(bi_rnn)\n",
        "\n",
        "# sigmoid classifier\n",
        "x_output = Dense(1, activation='sigmoid')(bi_rnn)\n",
        "\n",
        "blstm = Model(inputs=x_input, outputs=x_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "kqwrlwFWctxB"
      },
      "outputs": [],
      "source": [
        "blstm.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lvJzq--R7f_8"
      },
      "outputs": [],
      "source": [
        "# Early Stopping\n",
        "es = EarlyStopping(monitor='val_loss', patience=150, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "axvR2gX37gpo"
      },
      "outputs": [],
      "source": [
        "blstm.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy'],\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4miGD2dz7laj"
      },
      "outputs": [],
      "source": [
        "history = blstm.fit(\n",
        "     X_train, y_train,\n",
        "     epochs=100, batch_size=512,\n",
        "     validation_data=(X_test, y_test),\n",
        "     callbacks=[es]\n",
        "     )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vdd21gxV83DA"
      },
      "outputs": [],
      "source": [
        "plot_history(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WxZOrs4c8_vl"
      },
      "outputs": [],
      "source": [
        "display_model_score(blstm,\n",
        "    [X_train, y_train],\n",
        "    [X_test, y_test],\n",
        "    [X_test, y_test],\n",
        "    256)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7r-FmTxjvhed"
      },
      "outputs": [],
      "source": [
        "y_pred = blstm.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Yc-C9Omnwv4S"
      },
      "outputs": [],
      "source": [
        "y_pred = np.reshape(y_pred,(len(y_pred),)).astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nOtY5y-3vAIx"
      },
      "outputs": [],
      "source": [
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jTeIkIFEUIxa"
      },
      "outputs": [],
      "source": [
        "blstm.save_weights('BLSTM.h5')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "ydyh-KnUWTNV",
        "8eEXVSpeI8Un",
        "UC6sZ0D_kgZj",
        "-tgM9DaT4iGC",
        "WWrjbaEbPL8J",
        "x4GdUnvG_-ZQ",
        "L51kTUhGzRCT",
        "Q5EKn4vmgSA9"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}