# Acidophilic

## Dataset
The initial dataset included a substantial number of acidophilic proteins (237,109) compared to non-acidophilic ones (7,394). To ensure data quality, we excluded sequences containing ambiguous amino acid codes ("X", "B", and "Z") and protein fragments shorter than 100 amino acids. Furthermore, we employed the MMSeqs2-UniqueProt algorithm to eliminate redundancy within and between the training and test sets. This process reduced sequence similarity within the test set and between training and test sets to 20%, while maintaining full diversity within the training set. Following these steps, the final dataset comprised 4,089 acidophilic and 1,654 non-acidophilic proteins. This dataset was then split into an 80/20 ratio for training and testing purposes, respectively. An independent test set was reserved for unbiased evaluation. 

## ACE Model
ACE (Acidophilic Classification using Embeddings) leverages embeddings derived from the ProtT5 pLM through a logistic regression framework. This approach eliminates the need for manual feature engineering by utilizing natural language processing (NLP) techniques. It treats protein sequences like sentences, directly extracting informative vector representations (embeddings). To evaluate the effectiveness of this method, we compared embeddings generated by four pre-trained LLMs: ProtT5-XL-U50, ESM-1b, and two variants of ESM-2 with different parameter sizes (650M and 3B). These embeddings were then fed as input features to machine learning models for classification.
